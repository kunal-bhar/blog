{
  
    
        "post0": {
            "title": "Deep Audify- Deep Neural Networks for Audio Processing",
            "content": "Project At A Glance . Objective: Locate and count the number of Capuchinbird calls in 100 three-minute clips recorded in various regions of a rainforest. This will help us gain meaningful insights by mapping the number of calls to population density of the species throughout the area. . Data: Kaggle Dataset for Z by HP&#39;s Unlocked: Challenge 3 [Download] . Implementation: Audio EDA, Pre-Processing and Slicing, Spectrogram Visualizations, Normalization, Deep Neural Network (Conv2D, Flatten, Dense Layers) . Results: . The model computed considerable metrics when trained for a measly two epochs: (Precision: 0.9795), (Recall: 0.9728). | A labelled file was generated as output (Results.csv) with a count of Capuchin-calls for every recording instance. | The project was a part of my submission for Z by HP&#39;s Unlocked: Audio Analysis Challenge. | . Deployment: View this project on GitHub. . Dependencies . !pip install matplotlib tensorflow tensorflow-gpu tensorflow-io . Requirement already satisfied: matplotlib in c: users naman miniconda3 envs audioc lib site-packages (3.5.2) Requirement already satisfied: tensorflow in c: users naman miniconda3 envs audioc lib site-packages (2.9.0) Requirement already satisfied: tensorflow-gpu in c: users naman miniconda3 envs audioc lib site-packages (2.9.0) Requirement already satisfied: tensorflow-io in c: users naman miniconda3 envs audioc lib site-packages (0.26.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in c: users naman miniconda3 envs audioc lib site-packages (from matplotlib) (1.4.2) Requirement already satisfied: cycler&gt;=0.10 in c: users naman miniconda3 envs audioc lib site-packages (from matplotlib) (0.11.0) Requirement already satisfied: python-dateutil&gt;=2.7 in c: users naman miniconda3 envs audioc lib site-packages (from matplotlib) (2.8.2) Requirement already satisfied: packaging&gt;=20.0 in c: users naman miniconda3 envs audioc lib site-packages (from matplotlib) (21.3) Requirement already satisfied: pillow&gt;=6.2.0 in c: users naman miniconda3 envs audioc lib site-packages (from matplotlib) (9.1.1) Requirement already satisfied: fonttools&gt;=4.22.0 in c: users naman miniconda3 envs audioc lib site-packages (from matplotlib) (4.33.3) Requirement already satisfied: numpy&gt;=1.17 in c: users naman miniconda3 envs audioc lib site-packages (from matplotlib) (1.22.4) Requirement already satisfied: pyparsing&gt;=2.2.1 in c: users naman miniconda3 envs audioc lib site-packages (from matplotlib) (3.0.4) Requirement already satisfied: six&gt;=1.12.0 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (1.16.0) Requirement already satisfied: protobuf&gt;=3.9.2 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (3.20.1) Requirement already satisfied: google-pasta&gt;=0.1.1 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (0.2.0) Requirement already satisfied: termcolor&gt;=1.1.0 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (1.1.0) Requirement already satisfied: keras&lt;2.10.0,&gt;=2.9.0rc0 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (2.9.0) Requirement already satisfied: typing-extensions&gt;=3.6.6 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (4.1.1) Requirement already satisfied: setuptools in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (61.2.0) Requirement already satisfied: astunparse&gt;=1.6.0 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (1.6.3) Requirement already satisfied: gast&lt;=0.4.0,&gt;=0.2.1 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (0.4.0) Requirement already satisfied: wrapt&gt;=1.11.0 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (1.14.1) Requirement already satisfied: absl-py&gt;=1.0.0 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (1.0.0) Requirement already satisfied: h5py&gt;=2.9.0 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (3.6.0) Requirement already satisfied: flatbuffers&lt;2,&gt;=1.12 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (1.12) Requirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (1.46.3) Requirement already satisfied: keras-preprocessing&gt;=1.1.1 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (1.1.2) Requirement already satisfied: opt-einsum&gt;=2.3.2 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (3.3.0) Requirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (0.26.0) Requirement already satisfied: tensorboard&lt;2.10,&gt;=2.9 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (2.9.0) Requirement already satisfied: tensorflow-estimator&lt;2.10.0,&gt;=2.9.0rc0 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (2.9.0) Requirement already satisfied: libclang&gt;=13.0.0 in c: users naman miniconda3 envs audioc lib site-packages (from tensorflow) (14.0.1) Requirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in c: users naman miniconda3 envs audioc lib site-packages (from astunparse&gt;=1.6.0-&gt;tensorflow) (0.37.1) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in c: users naman miniconda3 envs audioc lib site-packages (from tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (1.8.1) Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in c: users naman miniconda3 envs audioc lib site-packages (from tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (2.6.6) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in c: users naman miniconda3 envs audioc lib site-packages (from tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (0.4.6) Requirement already satisfied: requests&lt;3,&gt;=2.21.0 in c: users naman miniconda3 envs audioc lib site-packages (from tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (2.27.1) Requirement already satisfied: markdown&gt;=2.6.8 in c: users naman miniconda3 envs audioc lib site-packages (from tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (3.3.7) Requirement already satisfied: werkzeug&gt;=1.0.1 in c: users naman miniconda3 envs audioc lib site-packages (from tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (2.1.2) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in c: users naman miniconda3 envs audioc lib site-packages (from tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (0.6.1) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in c: users naman miniconda3 envs audioc lib site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (0.2.8) Requirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in c: users naman miniconda3 envs audioc lib site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (5.1.0) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in c: users naman miniconda3 envs audioc lib site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (4.8) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in c: users naman miniconda3 envs audioc lib site-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (1.3.1) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in c: users naman miniconda3 envs audioc lib site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (0.4.8) Requirement already satisfied: idna&lt;4,&gt;=2.5 in c: users naman miniconda3 envs audioc lib site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (3.3) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in c: users naman miniconda3 envs audioc lib site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (1.26.9) Requirement already satisfied: certifi&gt;=2017.4.17 in c: users naman miniconda3 envs audioc lib site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (2022.5.18.1) Requirement already satisfied: charset-normalizer~=2.0.0 in c: users naman miniconda3 envs audioc lib site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (2.0.12) Requirement already satisfied: oauthlib&gt;=3.0.0 in c: users naman miniconda3 envs audioc lib site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.10,&gt;=2.9-&gt;tensorflow) (3.2.0) . . import os from matplotlib import pyplot as plt import tensorflow as tf import tensorflow_io as tfio . Data . Loading . CAPUCHIN_FILE= os.path.join(&#39;data&#39;, &#39;Parsed_Capuchinbird_Clips&#39;, &#39;XC3776-3.wav&#39;) NOT_CAPUCHIN_FILE= os.path.join(&#39;data&#39;, &#39;Parsed_Not_Capuchinbird_Clips&#39;, &#39;afternoon-birds-song-in-forest-0.wav&#39;) . file_contents= tf.io.read_file(CAPUCHIN_FILE) . Decoding . wav, sample_rate= tf.audio.decode_wav(file_contents, desired_channels= 1) . wav . &lt;tf.Tensor: shape=(132300, 1), dtype=float32, numpy= array([[-0.11117554], [-0.0378418 ], [ 0.05856323], ..., [-0.01077271], [-0.03436279], [-0.04879761]], dtype=float32)&gt; . sample_rate . &lt;tf.Tensor: shape=(), dtype=int32, numpy=44100&gt; . Primitive Pre-Processing . def load_wav_16k_mono(filename): file_contents= tf.io.read_file(filename) wav, sample_rate= tf.audio.decode_wav(file_contents, desired_channels= 1) wav= tf.squeeze(wav, axis= -1) sample_rate= tf.cast(sample_rate, dtype=tf.int64) wav= tfio.audio.resample(wav, rate_in= sample_rate, rate_out= 16000) return wav . wave= load_wav_16k_mono(CAPUCHIN_FILE) nwave= load_wav_16k_mono(NOT_CAPUCHIN_FILE) . Visualizations . plt.plot(wave) plt.show() . plt.plot(nwave) plt.show() . plt.plot(wave) plt.plot(nwave) plt.show() . . POS= os.path.join(&#39;data&#39;, &#39;Parsed_Capuchinbird_Clips&#39;) NEG= os.path.join(&#39;data&#39;, &#39;Parsed_Not_Capuchinbird_Clips&#39;) . pos= tf.data.Dataset.list_files(POS+&#39; *.wav&#39;) neg= tf.data.Dataset.list_files(NEG+&#39; *.wav&#39;) . len(pos), len(neg) . (217, 593) . tf.ones(len(pos)) . &lt;tf.Tensor: shape=(217,), dtype=float32, numpy= array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)&gt; . tf.zeros(len(neg)) . &lt;tf.Tensor: shape=(593,), dtype=float32, numpy= array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)&gt; . pos.as_numpy_iterator().next() . b&#39;data Parsed_Capuchinbird_Clips XC178168-3.wav&#39; . positives= tf.data.Dataset.zip((pos, tf.data.Dataset.from_tensor_slices(tf.ones(len(pos))))) negatives= tf.data.Dataset.zip((neg, tf.data.Dataset.from_tensor_slices(tf.zeros(len(neg))))) data= positives.concatenate(negatives) . data.shuffle(10000).as_numpy_iterator().next() . (b&#39;data Parsed_Not_Capuchinbird_Clips crickets-chirping-crickets-sound-27.wav&#39;, 0.0) . Clip Duration, Normalization and Zero-Padding . lengths= [] for file in os.listdir(os.path.join(&#39;data&#39;, &#39;Parsed_Capuchinbird_Clips&#39;)): tensor_wave= load_wav_16k_mono(os.path.join(&#39;data&#39;, &#39;Parsed_Capuchinbird_Clips&#39;, file)) lengths.append(len(tensor_wave)) . WARNING:tensorflow:5 out of the last 5 calls to &lt;function pfor.&lt;locals&gt;.f at 0x000001E6F095C040&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details. WARNING:tensorflow:6 out of the last 6 calls to &lt;function pfor.&lt;locals&gt;.f at 0x000001E6F095C4C0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details. . . lengths[:10] . [40000, 48000, 56000, 48000, 56000, 64000, 64000, 64000, 56000, 56000] . mean= tf.math.reduce_mean(lengths) min= tf.math.reduce_min(lengths) max= tf.math.reduce_max(lengths) . mean, min, max . (&lt;tf.Tensor: shape=(), dtype=int32, numpy=54156&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=32000&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=80000&gt;) . def preprocess(file_path, label): wav= load_wav_16k_mono(file_path) wav= wav[:48000] zero_padding= tf.zeros([48000]- tf.shape(wav), dtype= tf.float32) wav= tf.concat([zero_padding, wav], 0) spectrogram= tf.signal.stft(wav, frame_length= 320, frame_step= 32) spectrogram= tf.abs(spectrogram) spectrogram= tf.expand_dims(spectrogram, axis= 2) return spectrogram, label . file_path, label= positives.shuffle(buffer_size= 10000).as_numpy_iterator().next() . Spectrogram . spectrogram, label= preprocess(file_path, label) . spectrogram . &lt;tf.Tensor: shape=(1491, 257, 1), dtype=float32, numpy= array([[[0.0000000e+00], [0.0000000e+00], [0.0000000e+00], ..., [0.0000000e+00], [0.0000000e+00], [0.0000000e+00]], [[0.0000000e+00], [0.0000000e+00], [0.0000000e+00], ..., [0.0000000e+00], [0.0000000e+00], [0.0000000e+00]], [[0.0000000e+00], [0.0000000e+00], [0.0000000e+00], ..., [0.0000000e+00], [0.0000000e+00], [0.0000000e+00]], ..., [[3.4047730e-02], [3.1229634e-02], [3.8121101e-02], ..., [2.8812696e-07], [3.1764142e-07], [2.9243529e-07]], [[2.2530884e-02], [2.1241199e-02], [2.0391349e-02], ..., [4.3568735e-07], [3.9645255e-07], [1.6391277e-07]], [[1.0637306e-02], [8.4740259e-03], [5.8991811e-03], ..., [7.1185252e-07], [3.2554652e-07], [8.9406967e-08]]], dtype=float32)&gt; . label . 1.0 . plt.figure(figsize= (30, 20)) plt.imshow(tf.transpose(spectrogram)[0]) plt.show() . Data Preparation Pipeline . data= data.map(preprocess) data= data.cache() data= data.shuffle(buffer_size= 1000) data= data.batch(16) data= data.prefetch(8) . WARNING:tensorflow:Using a while_loop for converting IO&gt;AudioResample . len(data) . 51 . Train-Test Split . train= data.take(36) test= data.skip(36).take(15) . samples, labels= train.as_numpy_iterator().next() samples.shape . (16, 1491, 257, 1) . labels . array([0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.], dtype=float32) . Model Setup and Layers . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Conv2D, Dense, Flatten . model= Sequential() model.add(Conv2D(16, (3, 3), activation= &#39;relu&#39;, input_shape= (1491, 257, 1))) model.add(Conv2D(16, (3, 3), activation= &#39;relu&#39;)) model.add(Flatten()) model.add(Dense(128, activation= &#39;relu&#39;)) model.add(Dense(1, activation= &#39;sigmoid&#39;)) . model.compile(&#39;Adam&#39;, loss= &#39;BinaryCrossentropy&#39;, metrics= [tf.keras.metrics.Recall(), tf.keras.metrics.Precision()]) model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 1489, 255, 16) 160 conv2d_1 (Conv2D) (None, 1487, 253, 16) 2320 flatten (Flatten) (None, 6019376) 0 dense (Dense) (None, 128) 770480256 dense_1 (Dense) (None, 1) 129 ================================================================= Total params: 770,482,865 Trainable params: 770,482,865 Non-trainable params: 0 _________________________________________________________________ . hist= model.fit(train, epochs= 2, validation_data= test) . Epoch 1/2 36/36 [==============================] - 816s 23s/step - loss: 13.7638 - recall: 0.9085 - precision: 0.8165 - val_loss: 0.6470 - val_recall: 0.8308 - val_precision: 0.9818 Epoch 2/2 36/36 [==============================] - 788s 21s/step - loss: 0.0566 - recall: 0.9728 - precision: 0.9795 - val_loss: 0.0423 - val_recall: 0.9855 - val_precision: 0.9855 . Metrics and Visualization . Loss . plt.title(&#39;Loss&#39;) plt.plot(hist.history[&#39;loss&#39;], &#39;r&#39;) plt.plot(hist.history[&#39;val_loss&#39;], &#39;b&#39;) plt.show() . Precision . plt.title(&#39;Precision&#39;) plt.plot(hist.history[&#39;precision&#39;], &#39;r&#39;) plt.plot(hist.history[&#39;val_precision&#39;], &#39;b&#39;) plt.show() . Recall . plt.title(&#39;Recall&#39;) plt.plot(hist.history[&#39;recall&#39;], &#39;r&#39;) plt.plot(hist.history[&#39;val_recall&#39;], &#39;b&#39;) plt.show() . Predictions . X_test, y_test= test.as_numpy_iterator().next() . X_test.shape, y_test.shape . ((16, 1491, 257, 1), (16,)) . yhat= model.predict(X_test) . 1/1 [==============================] - 1s 971ms/step . yhat= [1 if prediction&gt; 0.5 else 0 for prediction in yhat] . yhat . [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0] . y_test.astype(int) . array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]) . Using our Solution on the Target Clips . Loading . def load_mp3_16k_mono(filename): &quot;&quot;&quot;&quot; wav file-&gt; float tensor-&gt; resample to single-channel audio &quot;&quot;&quot; res= tfio.audio.AudioIOTensor(filename) tensor= res.to_tensor() tensor= tf.math.reduce_sum(tensor, axis= 1)/ 2 sample_rate= res.rate sample_rate= tf.cast(sample_rate, dtype= tf.int64) wav= tfio.audio.resample(tensor, rate_in= sample_rate, rate_out= 16000) return wav . mp3= os.path.join(&#39;data&#39;, &#39;Forest Recordings&#39;, &#39;recording_00.mp3&#39;) . wav= load_mp3_16k_mono(mp3) . wav . &lt;tf.Tensor: shape=(2880666,), dtype=float32, numpy= array([ 8.1433272e-12, -5.7019250e-12, -5.3486417e-12, ..., -1.1291276e-02, -1.4230422e-02, -3.0555837e-03], dtype=float32)&gt; . Slicing . audio_slices= tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length= 48000, sequence_stride= 48000, batch_size= 1) . samples, index= audio_slices.as_numpy_iterator().next() . samples.shape . (1, 48000) . len(audio_slices) . 60 . Pre-Processing . def preprocess_mp3(sample, index): sample= sample[0] zero_padding= tf.zeros([48000] - tf.shape(sample), dtype= tf.float32) wav= tf.concat([zero_padding, sample], 0) spectrogram= tf.signal.stft(wav, frame_length= 320, frame_step= 32) spectrogram= tf.abs(spectrogram) spectrogram= tf.expand_dims(spectrogram, axis= 2) return spectrogram . audio_slices= tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length= 48000, sequence_stride= 48000, batch_size= 1) audio_slices= audio_slices.map(preprocess_mp3) audio_slices= audio_slices.batch(64) . Predictions . Initial Output . yhat= model.predict(audio_slices) yhat= [1 if prediction&gt; 0.96 else 0 for prediction in yhat] . len(yhat) . yhat . Grouping Adjacent Values for Longer Calls . from itertools import groupby . yhat= [key for key, group in groupby(yhat)] . tf.math.reduce_sum(yhat) . calls= tf.math.reduce_sum(yhat).numpy() calls . Results Workflow . Iterate . results= {} for file in os.listdir(os.path.join(&#39;data&#39;, &#39;Forest Recordings&#39;)): FILEPATH= os.path.join(&#39;data&#39;, &#39;Forest Recordings&#39;, file) wav= load_mp3_16k_mono(FILEPATH) audio_slices= tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length= 48000, sequence_stride= 48000, batch_size= 1) audio_slices= audio_slices.map(preprocess_mp3) audio_slices= audio_slices.batch(64) yhat= model.predict(audio_slices) results[file]= yhat . results . Label and Group . class_preds= {} for file, logits in results.items(): class_preds[file]= [1 if prediction&gt; 0.96 else 0 for prediction in logits] class_preds . postprocessed= {} for file, scores in class_preds.items(): postprocessed[file]= tf.math.reduce_sum([key for key, group in groupby(scores)]).numpy() postprocessed . Export . import csv . with open(&#39;Results.csv&#39;, &#39;w&#39;, newline= &#39;&#39;) as f: writer= csv.writer(f, delimiter= &#39;,&#39;) writer.writerow([&#39;recording&#39;, &#39;capuchin_calls&#39;]) for key_value in postprocessed.items(): writer.writerow([key, value]) .",
            "url": "https://kunal-bhar.github.io/blog/audio/python/pipeline/visualizations/2022/05/21/deep-audify.html",
            "relUrl": "/audio/python/pipeline/visualizations/2022/05/21/deep-audify.html",
            "date": " • May 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Multipose Detection for Humans",
            "content": "Project At A Glance . Objective: . Sucessfully track people in motion by rendering keypoints, edges and connections on the active Video Capture device i.e. be accessible through both the Webcam and external Video files. | Further enhance this setup by looping through multiple people at once and pursue Multi-pose Detection. | . Setup: OpenCV, MoveNet.Lightning Pre-Trained Model [Download] . Implementation: . Captures video input resized to Dimensions 32m x 32n where (m, n) are scaled to match the original dimensions closely. | Renders 17 keypoints inter-connected with edges to define Skeletal Blueprints on every person in the frame. | Assigns a Confidence Score to each keypoint. Points are rendered when the Confidence &gt;= 0.3. | . Results: . The model has been successfully tweaked to detect multiple people with tested high performance on resolutions upto 4K. | . Deployment: View this project on GitHub. . Dependencies . !pip install tensorflow==2.8.0 tensorflow-gpu==2.8.0 tensorflow-hub opencv-python matplotlib . Requirement already satisfied: tensorflow==2.8.0 in c: users kunal appdata roaming python python39 site-packages (2.8.0) Collecting tensorflow-gpu==2.8.0 Downloading tensorflow_gpu-2.8.0-cp39-cp39-win_amd64.whl (438.0 MB) -- 438.0/438.0 MB 2.9 MB/s eta 0:00:00 Collecting tensorflow-hub Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB) Collecting opencv-python Using cached opencv_python-4.5.5.64-cp36-abi3-win_amd64.whl (35.4 MB) Collecting matplotlib Downloading matplotlib-3.5.2-cp39-cp39-win_amd64.whl (7.2 MB) - 7.2/7.2 MB 21.9 MB/s eta 0:00:00 Collecting tf-estimator-nightly==2.8.0.dev2021122109 Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB) Requirement already satisfied: setuptools in c: programdata anaconda3 envs movenet-multipose lib site-packages (from tensorflow==2.8.0) (62.2.0) Requirement already satisfied: keras&lt;2.9,&gt;=2.8.0rc0 in c: users kunal appdata roaming python python39 site-packages (from tensorflow==2.8.0) (2.8.0) Requirement already satisfied: google-pasta&gt;=0.1.1 in c: users kunal appdata roaming python python39 site-packages (from tensorflow==2.8.0) (0.2.0) Collecting libclang&gt;=9.0.1 Using cached libclang-14.0.1-py2.py3-none-win_amd64.whl (14.2 MB) Collecting tensorboard&lt;2.9,&gt;=2.8 Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB) Collecting protobuf&gt;=3.9.2 Downloading protobuf-3.20.1-cp39-cp39-win_amd64.whl (904 kB) - 904.1/904.1 kB 14.2 MB/s eta 0:00:00 Collecting keras-preprocessing&gt;=1.1.1 Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB) Requirement already satisfied: wrapt&gt;=1.11.0 in c: users kunal appdata roaming python python39 site-packages (from tensorflow==2.8.0) (1.12.1) Collecting tensorflow-io-gcs-filesystem&gt;=0.23.1 Downloading tensorflow_io_gcs_filesystem-0.25.0-cp39-cp39-win_amd64.whl (1.5 MB) - 1.5/1.5 MB 18.5 MB/s eta 0:00:00 Requirement already satisfied: flatbuffers&gt;=1.12 in c: users kunal appdata roaming python python39 site-packages (from tensorflow==2.8.0) (2.0) Collecting numpy&gt;=1.20 Downloading numpy-1.22.3-cp39-cp39-win_amd64.whl (14.7 MB) 14.7/14.7 MB 21.8 MB/s eta 0:00:00 Collecting termcolor&gt;=1.1.0 Using cached termcolor-1.1.0-py3-none-any.whl Requirement already satisfied: astunparse&gt;=1.6.0 in c: users kunal appdata roaming python python39 site-packages (from tensorflow==2.8.0) (1.6.3) Collecting typing-extensions&gt;=3.6.6 Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB) Collecting opt-einsum&gt;=2.3.2 Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB) Requirement already satisfied: gast&gt;=0.2.1 in c: users kunal appdata roaming python python39 site-packages (from tensorflow==2.8.0) (0.5.3) Collecting absl-py&gt;=0.4.0 Using cached absl_py-1.0.0-py3-none-any.whl (126 kB) Requirement already satisfied: h5py&gt;=2.9.0 in c: users kunal appdata roaming python python39 site-packages (from tensorflow==2.8.0) (3.6.0) Collecting grpcio&lt;2.0,&gt;=1.24.3 Downloading grpcio-1.46.1-cp39-cp39-win_amd64.whl (3.5 MB) - 3.5/3.5 MB 20.3 MB/s eta 0:00:00 Requirement already satisfied: six&gt;=1.12.0 in c: programdata anaconda3 envs movenet-multipose lib site-packages (from tensorflow==2.8.0) (1.16.0) Collecting cycler&gt;=0.10 Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB) Collecting fonttools&gt;=4.22.0 Downloading fonttools-4.33.3-py3-none-any.whl (930 kB) - 930.9/930.9 kB 19.6 MB/s eta 0:00:00 Requirement already satisfied: packaging&gt;=20.0 in c: programdata anaconda3 envs movenet-multipose lib site-packages (from matplotlib) (21.3) Requirement already satisfied: python-dateutil&gt;=2.7 in c: programdata anaconda3 envs movenet-multipose lib site-packages (from matplotlib) (2.8.2) Requirement already satisfied: pyparsing&gt;=2.2.1 in c: programdata anaconda3 envs movenet-multipose lib site-packages (from matplotlib) (3.0.9) Collecting kiwisolver&gt;=1.0.1 Downloading kiwisolver-1.4.2-cp39-cp39-win_amd64.whl (55 kB) - 55.4/55.4 kB 1.4 MB/s eta 0:00:00 Collecting pillow&gt;=6.2.0 Downloading Pillow-9.1.0-cp39-cp39-win_amd64.whl (3.3 MB) - 3.3/3.3 MB 20.9 MB/s eta 0:00:00 Requirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in c: programdata anaconda3 envs movenet-multipose lib site-packages (from astunparse&gt;=1.6.0-&gt;tensorflow==2.8.0) (0.37.1) Collecting tensorboard-plugin-wit&gt;=1.6.0 Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB) Collecting tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB) Collecting google-auth&lt;3,&gt;=1.6.3 Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB) -- 156.7/156.7 kB 9.8 MB/s eta 0:00:00 Collecting markdown&gt;=2.6.8 Downloading Markdown-3.3.7-py3-none-any.whl (97 kB) - 97.8/97.8 kB 5.5 MB/s eta 0:00:00 Collecting werkzeug&gt;=0.11.15 Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB) - 224.9/224.9 kB 13.4 MB/s eta 0:00:00 Collecting google-auth-oauthlib&lt;0.5,&gt;=0.4.1 Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB) Collecting requests&lt;3,&gt;=2.21.0 Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB) - 63.1/63.1 kB 1.7 MB/s eta 0:00:00 Collecting pyasn1-modules&gt;=0.2.1 Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB) Collecting cachetools&lt;6.0,&gt;=2.0.0 Using cached cachetools-5.0.0-py3-none-any.whl (9.1 kB) Collecting rsa&lt;5,&gt;=3.1.4 Using cached rsa-4.8-py3-none-any.whl (39 kB) Collecting requests-oauthlib&gt;=0.7.0 Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB) Requirement already satisfied: importlib-metadata&gt;=4.4 in c: programdata anaconda3 envs movenet-multipose lib site-packages (from markdown&gt;=2.6.8-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow==2.8.0) (4.11.3) Collecting urllib3&lt;1.27,&gt;=1.21.1 Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB) -- 139.0/139.0 kB 8.6 MB/s eta 0:00:00 Collecting charset-normalizer~=2.0.0 Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB) Collecting certifi&gt;=2017.4.17 Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB) -- 149.2/149.2 kB 9.3 MB/s eta 0:00:00 Collecting idna&lt;4,&gt;=2.5 Using cached idna-3.3-py3-none-any.whl (61 kB) Requirement already satisfied: zipp&gt;=0.5 in c: programdata anaconda3 envs movenet-multipose lib site-packages (from importlib-metadata&gt;=4.4-&gt;markdown&gt;=2.6.8-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow==2.8.0) (3.8.0) Collecting pyasn1&lt;0.5.0,&gt;=0.4.6 Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB) Collecting oauthlib&gt;=3.0.0 Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB) Installing collected packages: tf-estimator-nightly, termcolor, tensorboard-plugin-wit, pyasn1, libclang, certifi, werkzeug, urllib3, typing-extensions, tensorflow-io-gcs-filesystem, tensorboard-data-server, rsa, pyasn1-modules, protobuf, pillow, oauthlib, numpy, kiwisolver, idna, grpcio, fonttools, cycler, charset-normalizer, cachetools, absl-py, tensorflow-hub, requests, opt-einsum, opencv-python, matplotlib, markdown, keras-preprocessing, google-auth, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow-gpu Successfully installed absl-py-1.0.0 cachetools-5.0.0 certifi-2021.10.8 charset-normalizer-2.0.12 cycler-0.11.0 fonttools-4.33.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 grpcio-1.46.1 idna-3.3 keras-preprocessing-1.1.2 kiwisolver-1.4.2 libclang-14.0.1 markdown-3.3.7 matplotlib-3.5.2 numpy-1.22.3 oauthlib-3.2.0 opencv-python-4.5.5.64 opt-einsum-3.3.0 pillow-9.1.0 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-gpu-2.8.0 tensorflow-hub-0.12.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.2.0 urllib3-1.26.9 werkzeug-2.1.2 . . import tensorflow as tf import tensorflow_hub as hub import cv2 from matplotlib import pyplot as plt import numpy as np . Load the MoveNet.Lightning Model . model= hub.load(&#39;https://tfhub.dev/google/movenet/multipose/lightning/1&#39;) . movenet= model.signatures[&#39;serving_default&#39;] . VideoCapture . Master Function . cap= cv2.VideoCapture(&#39;football.mp4&#39;) while cap.isOpened(): ret, frame= cap.read() img= frame.copy() img= tf.image.resize_with_pad(tf.expand_dims(img, axis= 0), 544, 1024) input_img= tf.cast(img, dtype= tf.int32) # VideoCapture should be in dimensions 32m x 32n, closest to actual resolution. results= movenet(input_img) keypoints_and_scores= results[&#39;output_0&#39;].numpy()[:, :, :51].reshape((6, 17, 3)) loop_through_people(frame, keypoints_and_scores, EDGES, 0.3) cv2.imshow(&#39;Movenet Multipose Window&#39;, frame) if cv2.waitKey(10) &amp; 0xFF==ord(&#39;q&#39;): break cap.release() cv2.destroyAllWindows() . Scaling (Width/Height) . 1080/2048 . 0.52734375 . Vectorized Frame . frame . array([[[217, 211, 219], [217, 211, 219], [217, 210, 221], ..., [199, 190, 194], [199, 190, 194], [199, 190, 194]], [[210, 204, 212], [210, 204, 212], [210, 203, 214], ..., [199, 190, 194], [199, 190, 194], [199, 190, 194]], [[209, 203, 211], [209, 203, 211], [209, 202, 213], ..., [196, 187, 191], [196, 187, 191], [196, 187, 191]], ..., [[ 47, 70, 69], [ 47, 70, 69], [ 47, 70, 69], ..., [ 41, 67, 70], [ 41, 67, 70], [ 41, 67, 70]], [[ 50, 73, 72], [ 50, 73, 72], [ 48, 71, 70], ..., [ 41, 67, 70], [ 41, 67, 70], [ 41, 67, 70]], [[ 51, 74, 73], [ 51, 74, 73], [ 50, 73, 72], ..., [ 40, 66, 69], [ 40, 66, 69], [ 40, 66, 69]]], dtype=uint8) . plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) . &lt;matplotlib.image.AxesImage at 0x19252116880&gt; . Keypoint Matrix and Confidence Scores . keypoints_and_scores[0] . array([[0.48517755, 0.10898407, 0.35875612], [0.47976708, 0.11077929, 0.46474373], [0.47994435, 0.1071113 , 0.4717213 ], [0.4723238 , 0.11269185, 0.4148368 ], [0.47123876, 0.10366924, 0.4909297 ], [0.4823754 , 0.11886524, 0.79927677], [0.48038223, 0.0954906 , 0.78360116], [0.51465076, 0.12093391, 0.5353545 ], [0.51162773, 0.09023584, 0.739223 ], [0.5279961 , 0.11441359, 0.5836447 ], [0.5291515 , 0.10141041, 0.48077533], [0.52872527, 0.11163795, 0.7669046 ], [0.5289806 , 0.09788188, 0.689511 ], [0.5415193 , 0.12559794, 0.8202237 ], [0.53999746, 0.08833516, 0.6755993 ], [0.5804207 , 0.11740522, 0.6815445 ], [0.58320117, 0.09349357, 0.8147298 ]], dtype=float32) . scores= keypoints_and_scores[0][:, 2] scores . array([0.35875612, 0.46474373, 0.4717213 , 0.4148368 , 0.4909297 , 0.79927677, 0.78360116, 0.5353545 , 0.739223 , 0.5836447 , 0.48077533, 0.7669046 , 0.689511 , 0.8202237 , 0.6755993 , 0.6815445 , 0.8147298 ], dtype=float32) . Logistics for Multi-Body Detection . Make Keypoints . def draw_keypoints(frame, keypoints, confidence_threshold): y, x, c = frame.shape shaped = np.squeeze(np.multiply(keypoints, [y,x,1])) for kp in shaped: ky, kx, kp_conf = kp if kp_conf &gt; confidence_threshold: cv2.circle(frame, (int(kx), int(ky)), 6, (0,255,0), -1) . Define Edges . EDGES = { (0, 1): &#39;m&#39;, (0, 2): &#39;c&#39;, (1, 3): &#39;m&#39;, (2, 4): &#39;c&#39;, (0, 5): &#39;m&#39;, (0, 6): &#39;c&#39;, (5, 7): &#39;m&#39;, (7, 9): &#39;m&#39;, (6, 8): &#39;c&#39;, (8, 10): &#39;c&#39;, (5, 6): &#39;y&#39;, (5, 11): &#39;m&#39;, (6, 12): &#39;c&#39;, (11, 12): &#39;y&#39;, (11, 13): &#39;m&#39;, (13, 15): &#39;m&#39;, (12, 14): &#39;c&#39;, (14, 16): &#39;c&#39; } . Draw Connections . def draw_connections(frame, keypoints, edges, confidence_threshold): y, x, c = frame.shape shaped = np.squeeze(np.multiply(keypoints, [y,x,1])) for edge, color in edges.items(): p1, p2 = edge y1, x1, c1 = shaped[p1] y2, x2, c2 = shaped[p2] if (c1 &gt; confidence_threshold) &amp; (c2 &gt; confidence_threshold): cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 4) . Looping Through Multiple People . def loop_through_people(frame, keypoints_and_scores, edges, confidence_threshold): for person in keypoints_and_scores: draw_connections(frame, person, edges, confidence_threshold) draw_keypoints(frame, person, confidence_threshold) .",
            "url": "https://kunal-bhar.github.io/blog/computer-vision/python/opencv/visualizations/2022/05/16/movenet-multipose.html",
            "relUrl": "/computer-vision/python/opencv/visualizations/2022/05/16/movenet-multipose.html",
            "date": " • May 16, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "LangWhich",
            "content": "Project At A Glance . Objective: . CLI project that seeks to detect programming languages using spaCy’s NER on Stack Overflow/Reddit data. | It also enables an intriguing comparison by evaluating the problem with both - Statistical as well as ML approaches for Language Processing. | LangWhich’s workflow pipelined with Sentiment Analysis can be leveraged as an attempt to investigate how different communities think about different programming languages. | . Inspiration: For a preface to this project, check out Golang Detection from Stack Overflow Questions. . Data: 2000+ Labelled Titles from Stack Overflow and Reddit in JSONL format. [Download] . Implementation: spaCy, Named Entity Recognition (Machine Learning Approach), Rule-Based Matching (Statistical Approach), Model Evaluation and Packaging . Results: . Both approaches were fairly successful but the Machine Learning solution had the edge on metrics as described below. | Named Entity Recognition: (Precision: 0.926), (Recall: 0.935), (f-Score: 0.931). | Rule-Based Matching: (Precision: 0.837), (Recall: 0.890), (f-Score: 0.863). | The ML approach was also about 3x faster on processing speed. | Developed workflow that enables storing a trained model as a package on the Local Disk. | . Deployment: View this project on GitHub. To run locally, clone the repository in an enviornment (say, conda) and refer to the commands below. . Abstract . The goal of the project is to make a model that could be used generally but it will specifically be used as a Named Entity Recognition exercise on Stack Overflow/Reddit with verticals for Sentiment Analysis. It is an attempt to investigate how the different communities think about different programming languages. . Typically, two models are evaluated using this project; a pattern matching model and a spaCy NER model to pursue a comparison between rule-based Statistics and Machine Learning approaches for NLP. . To export and run this workflow on your local machine, use the spacy project run package command. . project.yml . The project.yml defines the data assets required by the project, as well as the available commands and workflows. . Commands . The following commands are defined by the project. They can be executed using spacy project run [name]. Commands are only re-run if their inputs have changed. . Command Description . preprocess | Convert the data to spaCy’s binary format | . patternmod | Generate a named entity recognition model based on rules. | . train | Train a named entity recognition model | . evaluate | Evaluate the model and export metrics | . package | Package the trained model so it can be installed | . show-stats | Show the statistics that compares both models. | . Workflows . The following workflows are defined by the project. They can be executed using spacy project run [name] and will run the specified commands in order. Commands are only re-run if their inputs have changed. . Workflow Steps . all | preprocess → patternmod → train → evaluate | . Assets . The following assets are defined by the project. They can be fetched by running spacy project assets in the project directory. . File Source Description . assets/stackoverflow-train.jsonl | Local | JSONL-formatted training data | . assets/stackoverflow-valid.jsonl | Local | JSONL-formatted validation data | . Config Files . The following configuration files are defined by the project. . File Source Description . configs/config.cfg | Local | CFG-formatted for base config | . configs/proglang_patterns.jsonl | Local | JSONL-formatted rule patterns | . Scripts . The following Python scripts are defined by the project. . File Source Description . scripts/preprocess.py | Local | Pre-Processing Script | . scripts/save_pattern_model.py | Local | Pattern NER Script | . scripts/print_stats.py | Local | Results Comparison Script | . Command Line Interface . The commands and workflows can be used with the CLI as follows: . Initialize: project run . . Command Execution: project preprocess . . Workflow Execution: project all . . . Metrics: project show-stats . . References . spaCy and spaCy Projects: Documentation . Explosion Templates: GitHub Repository . Vincent Warmerdam: GitHub . Note . Part of this documentation has been auto-generated using the spacy project document command! .",
            "url": "https://kunal-bhar.github.io/blog/nlp/python/spacy/ner/cli/intelligent/2022/03/31/langwhich.html",
            "relUrl": "/nlp/python/spacy/ner/cli/intelligent/2022/03/31/langwhich.html",
            "date": " • Mar 31, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Golang Detection from Stack Overflow Questions",
            "content": "Project At A Glance . Objective: Off the top of my head, the word &#39;go&#39; can be represented as a verb, a noun, or a general part of a string. The goal here is to build an intelligent solution that analyzes textual relationships and locates all instances of &#39;golang&#39; from programming queries. . Data: StackSample: 10% of Stack Overflow Q&amp;A. [Download] . Implementation: spaCy&#39;s en_core_web Model, Part of Speech, Sentence Dependencies, Rule-Based Matching, Tagging . Results: . The model performs best a small (_sm) model with the following conditions: . [i] Question Tag== &#39;go&#39; . [ii] Part of Speech!= &#39;verb&#39;. . | In the above case, the model is able to collect all instances of golang. . | Therefore, Recall= (1.00) while Precision = Accuracy = (0.891) | The food for thought enabled by this project led to LangWhich: a much more concise, CLI implementation that works for all programming languages. [View LangWhich] | . Deployment: View this project on GitHub. . Initialization . import pandas as pd df = (pd.read_csv(&#39;Questions.csv&#39;, nrows=1_000_000, usecols=[&#39;Title&#39;, &#39;Id&#39;], encoding=&#39;ISO-8859-1&#39;)) titles = [_ for _ in df[&#39;Title&#39;]] . Dataset Features . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1000000 entries, 0 to 999999 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 Id 1000000 non-null int64 1 Title 1000000 non-null object dtypes: int64(1), object(1) memory usage: 15.3+ MB . import random random.choices(titles, k=20) . [&#39;Call a click on a newly added dom element&#39;, &#39;some confusion about kendo grid databind&#39;, &#39;display Jquery GIF onload Jquery Tabs&#39;, &#39;Port Chrome Extension to Firefox, Safari, IE&#39;, &#39;Uploading A SQL Server Script?&#39;, &#39;adding a tab character in swift&#39;, &#39;^M in the diff using svn&#39;, &#39;Can I open an HTML file in Ace Editor?&#39;, &#39;Cast value type to generic&#39;, &#39;What do triple curly braces indicate?&#39;, &#34;Can&#39;t get javascript to run onsubmit&#34;, &#39;Wordpress - Dynamic &#34;static&#34; permalink based on permalink taxonomy&#39;, &#39;Can depth peeling be implemented without any shader?&#39;, &#39;count(*) Does not return 0 when using group by in MySQL&#39;, &#39;How to call action on onclick-javascript in ruby on rails&#39;, &#39;How to translate Xcode 3 properties to Xcode 4&#39;, &#39;Java-JFXpanel refresh page cause JVMã x80 x80crash -- Threading issue&#39;, &#34;Can&#39;t seem to figure out why the InnerHTML statements are not working&#34;, &#39;How I can validate only some validation groups based on some fields in the form itself in Symfony2&#39;, &#39;Drawable vs Single reusable Bitmap better with memory?&#39;] . Primitive Function for &#39;go&#39; . def has_golang(text): return &#39;go&#39; in text # basic string-matching ~ unsatisfactory output g = (title for title in titles if has_golang(title)) [next(g) for i in range(2)] . [&#39;My website got hacked... What should I do?&#39;, &#34;DVCS Choices - What&#39;s good for Windows?&#34;] . spaCy Injection . import spacy #!python -m spacy download en_core_web_sm nlp = spacy.load(&#39;en_core_web_sm&#39;) . [t for t in nlp(&#39;Go is a both a verb and a programming language.&#39;)] . [Go, is, a, both, a, verb, and, a, programming, language, .] . doc = nlp(&#39;Go is a both a verb and a programming language.&#39;) t =doc[0] type(t) . spacy.tokens.token.Token . Text Relationships and Dependencies . Render . from spacy import displacy displacy.render(doc) # token-relationships . Go VERB is AUX a DET both CCONJ a DET verb NOUN and CCONJ a DET programming NOUN language. NOUN csubj det preconj det attr cc det amod conj Explanation . spacy.explain(&#39;det&#39;) . &#39;determiner&#39; . for t in doc: print(t, t.pos_, t.dep_) # pos = part of speech, dep = dependency . Go VERB csubj is AUX ROOT a DET det both CCONJ preconj a DET det verb NOUN attr and CCONJ cc a DET det programming NOUN amod language NOUN conj . PUNCT punct . Data Manipulation . Contains &#39;go&#39; . df = (pd.read_csv(&#39;Questions.csv&#39;, nrows=2_000_000, usecols=[&#39;Title&#39;, &#39;Id&#39;], encoding=&#39;ISO-8859-1&#39;)) titles = [_ for _ in df.loc[lambda d: d[&#39;Title&#39;].str.lower().str.contains(&#39;go&#39;)][&#39;Title&#39;]] . random.choices(titles, k=7) . [&#39;Trying to understand/get working a k-means clustering algorithem in MySQL&#39;, &#39;Google Cloud:- Not able to access Visual SVN via https&#39;, &#39;How can I use a &#34;For&#34; loop to map multiple polygons with the leaflet within shiny in R?&#39;, &#39;Remove marker from Google Maps API V3&#39;, &#39;Config Mongodb in Cakephp 2.8.5&#39;, &#39;Can a single Meteor instance listen and react to multiple MongoDB databases?&#39;, &#39;Going from Scrum to Kanban near &#34;release&#34;&#39;] . nlp = spacy.load(&#39;en_core_web_sm&#39;, disable=[&#39;ner&#39;]) . POS== Noun . %%time def has_golang(doc): for t in doc: if t.lower_ in [&#39;go&#39;, &#39;golang&#39;]: if t.pos_ == &#39;NOUN&#39;: return True return False # Collecting data that has &#39;go&#39;/&#39;golang&#39; where pos = Noun g = (doc for doc in nlp.pipe(titles) if has_golang(doc)) # nlp.pipe() added to optimize; takes doc as input instead of tokens [next(g) for i in range(15)] . Wall time: 7.03 s . [Deploying multiple Java web apps to Glassfish in one go, Removing all event handlers in one go, Paypal integration to serve multiple sellers in one go for a shopping site, How do I disable multiple listboxes in one go using jQuery?, multi package makefile example for go, Google&#39;s &#39;go&#39; and scope/functions, Where is App.config go after publishing?, SOAPUI &amp; Groovy Scripts, executing multiple SQL statements in one go, What&#39;s the simplest way to edit conflicted files in one go when using git and an editor like Vim or textmate?, Import large chunk of data into Google App Engine Data Store at one go, Saving all nested form objects in one go, what&#39;s the state of go language IDE support?, Decrypt many PDFs in one go using pdftk, How do I allocate memory for an array in the go programming language?, Is message passing via channels in go guaranteed to be non-blocking?] . Has Tag== &#39;go&#39; . df_tags = pd.read_csv(&#39;Tags.csv&#39;) go_ids = df_tags.loc[lambda d: d[&#39;Tag&#39;] == &#39;go&#39;][&#39;Id&#39;] # Collecting data from the Tags dataset with ID = &#39;go&#39; . POS!= Verb . def has_go_token(doc): for t in doc: if t.lower_ in [&#39;go&#39;, &#39;golang&#39;]: if t.pos_ != &#39;VERB&#39;: return True return False # Collecting data with &#39;go&#39;/&#39;golang&#39; where pos =! verb . Splitting and Benchmarking . all_go_sentences = df.loc[lambda d: d[&#39;Id&#39;].isin(go_ids)][&#39;Title&#39;].tolist() detectable = [d.text for d in nlp.pipe(all_go_sentences) if has_go_token(d)] non_detectable = (df .loc[lambda d: ~d[&#39;Id&#39;].isin(go_ids)] .loc[lambda d: d[&#39;Title&#39;].str.lower().str.contains(&#39;go&#39;)] [&#39;Title&#39;] .tolist()) non_detectable = [d.text for d in nlp.pipe(non_detectable) if has_go_token(d)] . len(all_go_sentences),len(detectable), len(non_detectable) # all_go_sentences = has go in title # detectable = golang confirmed by both title and tag # non_detectable = has go in title but not a tag # all_go_sentences - detectble = has a go tag but doesn&#39;t contain go in the title #optimal model has been found to be en_core_web_sm on logging metrics for precision, accuracy and recall for the manipulated data. . (1858, 941, 115) . Using our model on the detectable class . model_name = &#39;en_core_web_sm&#39; model = spacy.load(model_name, disable=[&#39;ner&#39;]) method = &#39;not-verb-but-pobj&#39; correct = sum(has_go_token(doc) for doc in model.pipe(detectable)) wrong = sum(has_go_token(doc) for doc in model.pipe(non_detectable)) precision = correct/(correct + wrong) recall = correct/len(detectable) accuracy = (correct + len(non_detectable) - wrong)/(len(detectable) + len(non_detectable)) f&quot;{precision},{recall},{accuracy},{model_name},{method}&quot; # custom-log . &#39;0.8910984848484849,1.0,0.8910984848484849,en_core_web_sm,not-verb-but-pobj&#39; . Metrics . print(precision), print(recall), print(accuracy) . 0.8910984848484849 1.0 0.8910984848484849 . (None, None, None) .",
            "url": "https://kunal-bhar.github.io/blog/nlp/python/spacy/english/intelligent/2022/03/23/golang-detection.html",
            "relUrl": "/nlp/python/spacy/english/intelligent/2022/03/23/golang-detection.html",
            "date": " • Mar 23, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Hourly Temperature Forecasting",
            "content": "Project At A Glance . Objective: Iteratively forecast hour-wise temperature for a region over a fairly large time-period (8 years in this case). . Data: Time-Series Weather Dataset at the Max Planck Institute in Jena, Germany. [Download] . Implementation: Time-Series Forecasting, Seqeuntial Long Short-Term Memory (LSTM) . Results: . Clear trends in data showing changes in the climate across the time of the year. | DataFrame with variance between Actual Values and Predicted Values for the test and validation sets. | Visualizations to judge model&#39;s performance. | . Deployment: View this project on GitHub. . Dependencies . import os import numpy as np import pandas as pd import tensorflow as tf . Dataset Initialization . Loading . zip_path = tf.keras.utils.get_file( origin=&#39;https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip&#39;, fname=&#39;jena_climate_2009_2016.csv.zip&#39;, extract=True) csv_path, _ = os.path.splitext(zip_path) . df = pd.read_csv(csv_path) . df . Date Time p (mbar) T (degC) Tpot (K) Tdew (degC) rh (%) VPmax (mbar) VPact (mbar) VPdef (mbar) sh (g/kg) H2OC (mmol/mol) rho (g/m**3) wv (m/s) max. wv (m/s) wd (deg) . 0 01.01.2009 00:10:00 | 996.52 | -8.02 | 265.40 | -8.90 | 93.30 | 3.33 | 3.11 | 0.22 | 1.94 | 3.12 | 1307.75 | 1.03 | 1.75 | 152.3 | . 1 01.01.2009 00:20:00 | 996.57 | -8.41 | 265.01 | -9.28 | 93.40 | 3.23 | 3.02 | 0.21 | 1.89 | 3.03 | 1309.80 | 0.72 | 1.50 | 136.1 | . 2 01.01.2009 00:30:00 | 996.53 | -8.51 | 264.91 | -9.31 | 93.90 | 3.21 | 3.01 | 0.20 | 1.88 | 3.02 | 1310.24 | 0.19 | 0.63 | 171.6 | . 3 01.01.2009 00:40:00 | 996.51 | -8.31 | 265.12 | -9.07 | 94.20 | 3.26 | 3.07 | 0.19 | 1.92 | 3.08 | 1309.19 | 0.34 | 0.50 | 198.0 | . 4 01.01.2009 00:50:00 | 996.51 | -8.27 | 265.15 | -9.04 | 94.10 | 3.27 | 3.08 | 0.19 | 1.92 | 3.09 | 1309.00 | 0.32 | 0.63 | 214.3 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 420546 31.12.2016 23:20:00 | 1000.07 | -4.05 | 269.10 | -8.13 | 73.10 | 4.52 | 3.30 | 1.22 | 2.06 | 3.30 | 1292.98 | 0.67 | 1.52 | 240.0 | . 420547 31.12.2016 23:30:00 | 999.93 | -3.35 | 269.81 | -8.06 | 69.71 | 4.77 | 3.32 | 1.44 | 2.07 | 3.32 | 1289.44 | 1.14 | 1.92 | 234.3 | . 420548 31.12.2016 23:40:00 | 999.82 | -3.16 | 270.01 | -8.21 | 67.91 | 4.84 | 3.28 | 1.55 | 2.05 | 3.28 | 1288.39 | 1.08 | 2.00 | 215.2 | . 420549 31.12.2016 23:50:00 | 999.81 | -4.23 | 268.94 | -8.53 | 71.80 | 4.46 | 3.20 | 1.26 | 1.99 | 3.20 | 1293.56 | 1.49 | 2.16 | 225.8 | . 420550 01.01.2017 00:00:00 | 999.82 | -4.82 | 268.36 | -8.42 | 75.70 | 4.27 | 3.23 | 1.04 | 2.01 | 3.23 | 1296.38 | 1.23 | 1.96 | 184.9 | . 420551 rows × 15 columns . Hour-Wise Slicing . df = df[5::6] df . Date Time p (mbar) T (degC) Tpot (K) Tdew (degC) rh (%) VPmax (mbar) VPact (mbar) VPdef (mbar) sh (g/kg) H2OC (mmol/mol) rho (g/m**3) wv (m/s) max. wv (m/s) wd (deg) . 5 01.01.2009 01:00:00 | 996.50 | -8.05 | 265.38 | -8.78 | 94.40 | 3.33 | 3.14 | 0.19 | 1.96 | 3.15 | 1307.86 | 0.21 | 0.63 | 192.7 | . 11 01.01.2009 02:00:00 | 996.62 | -8.88 | 264.54 | -9.77 | 93.20 | 3.12 | 2.90 | 0.21 | 1.81 | 2.91 | 1312.25 | 0.25 | 0.63 | 190.3 | . 17 01.01.2009 03:00:00 | 996.84 | -8.81 | 264.59 | -9.66 | 93.50 | 3.13 | 2.93 | 0.20 | 1.83 | 2.94 | 1312.18 | 0.18 | 0.63 | 167.2 | . 23 01.01.2009 04:00:00 | 996.99 | -9.05 | 264.34 | -10.02 | 92.60 | 3.07 | 2.85 | 0.23 | 1.78 | 2.85 | 1313.61 | 0.10 | 0.38 | 240.0 | . 29 01.01.2009 05:00:00 | 997.46 | -9.63 | 263.72 | -10.65 | 92.20 | 2.94 | 2.71 | 0.23 | 1.69 | 2.71 | 1317.19 | 0.40 | 0.88 | 157.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 420521 31.12.2016 19:10:00 | 1002.18 | -0.98 | 272.01 | -5.36 | 72.00 | 5.69 | 4.09 | 1.59 | 2.54 | 4.08 | 1280.70 | 0.87 | 1.36 | 190.6 | . 420527 31.12.2016 20:10:00 | 1001.40 | -1.40 | 271.66 | -6.84 | 66.29 | 5.51 | 3.65 | 1.86 | 2.27 | 3.65 | 1281.87 | 1.02 | 1.92 | 225.4 | . 420533 31.12.2016 21:10:00 | 1001.19 | -2.75 | 270.32 | -6.90 | 72.90 | 4.99 | 3.64 | 1.35 | 2.26 | 3.63 | 1288.02 | 0.71 | 1.56 | 158.7 | . 420539 31.12.2016 22:10:00 | 1000.65 | -2.89 | 270.22 | -7.15 | 72.30 | 4.93 | 3.57 | 1.37 | 2.22 | 3.57 | 1288.03 | 0.35 | 0.68 | 216.7 | . 420545 31.12.2016 23:10:00 | 1000.11 | -3.93 | 269.23 | -8.09 | 72.60 | 4.56 | 3.31 | 1.25 | 2.06 | 3.31 | 1292.41 | 0.56 | 1.00 | 202.6 | . 70091 rows × 15 columns . DateTime Indexing . df.index = pd.to_datetime(df[&#39;Date Time&#39;], format = &#39;%d.%m.%Y %H:%M:%S&#39;) df[:6] . Date Time p (mbar) T (degC) Tpot (K) Tdew (degC) rh (%) VPmax (mbar) VPact (mbar) VPdef (mbar) sh (g/kg) H2OC (mmol/mol) rho (g/m**3) wv (m/s) max. wv (m/s) wd (deg) . Date Time . 2009-01-01 01:00:00 01.01.2009 01:00:00 | 996.50 | -8.05 | 265.38 | -8.78 | 94.4 | 3.33 | 3.14 | 0.19 | 1.96 | 3.15 | 1307.86 | 0.21 | 0.63 | 192.7 | . 2009-01-01 02:00:00 01.01.2009 02:00:00 | 996.62 | -8.88 | 264.54 | -9.77 | 93.2 | 3.12 | 2.90 | 0.21 | 1.81 | 2.91 | 1312.25 | 0.25 | 0.63 | 190.3 | . 2009-01-01 03:00:00 01.01.2009 03:00:00 | 996.84 | -8.81 | 264.59 | -9.66 | 93.5 | 3.13 | 2.93 | 0.20 | 1.83 | 2.94 | 1312.18 | 0.18 | 0.63 | 167.2 | . 2009-01-01 04:00:00 01.01.2009 04:00:00 | 996.99 | -9.05 | 264.34 | -10.02 | 92.6 | 3.07 | 2.85 | 0.23 | 1.78 | 2.85 | 1313.61 | 0.10 | 0.38 | 240.0 | . 2009-01-01 05:00:00 01.01.2009 05:00:00 | 997.46 | -9.63 | 263.72 | -10.65 | 92.2 | 2.94 | 2.71 | 0.23 | 1.69 | 2.71 | 1317.19 | 0.40 | 0.88 | 157.0 | . 2009-01-01 06:00:00 01.01.2009 06:00:00 | 997.71 | -9.67 | 263.66 | -10.62 | 92.7 | 2.93 | 2.71 | 0.21 | 1.69 | 2.72 | 1317.71 | 0.05 | 0.50 | 146.0 | . Temperature Plot (degC) . df1 = df[&#39;T (degC)&#39;] . df1.plot() . &lt;AxesSubplot:xlabel=&#39;Date Time&#39;&gt; . Time-Series Window . def create_dataset(df, window_size=5): df_as_np = df.to_numpy() X = [] y = [] for i in range(len(df_as_np)-window_size): row = [[a] for a in df_as_np[i:i+window_size]] X.append(row) label = df_as_np[i+window_size] y.append(label) return np.array(X), np.array(y) . window_size = 5 X, y = create_dataset(df1, window_size) X.shape, y.shape . ((70086, 5, 1), (70086,)) . Train-Test Split . X_train, y_train = X[:60000], y[:60000] X_val, y_val = X[60000:65000], y[60000:65000] X_test, y_test = X[65000:], y[65000:] X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape . ((60000, 5, 1), (60000,), (5000, 5, 1), (5000,), (5086, 5, 1), (5086,)) . Model Setup, Layers and Callbacks . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import * from tensorflow.keras.callbacks import ModelCheckpoint from tensorflow.keras.losses import MeanSquaredError from tensorflow.keras.metrics import RootMeanSquaredError from tensorflow.keras.optimizers import Adam . model = Sequential() model.add(InputLayer((5, 1))) model.add(LSTM(64)) model.add(Dense(8, &#39;relu&#39;)) model.add(Dense(1, &#39;linear&#39;)) model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm (LSTM) (None, 64) 16896 dense (Dense) (None, 8) 520 dense_1 (Dense) (None, 1) 9 ================================================================= Total params: 17,425 Trainable params: 17,425 Non-trainable params: 0 _________________________________________________________________ . cp1 = ModelCheckpoint(&#39;model/&#39;, save_best_only=True) model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[RootMeanSquaredError()]) . model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=12, callbacks=[cp1]) . Epoch 1/12 1875/1875 [==============================] - ETA: 0s - loss: 96.9206 - root_mean_squared_error: 9.8448 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 12s 5ms/step - loss: 96.9206 - root_mean_squared_error: 9.8448 - val_loss: 7.7739 - val_root_mean_squared_error: 2.7882 Epoch 2/12 1869/1875 [============================&gt;.] - ETA: 0s - loss: 12.2745 - root_mean_squared_error: 3.5035 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 9s 5ms/step - loss: 12.2448 - root_mean_squared_error: 3.4993 - val_loss: 0.9448 - val_root_mean_squared_error: 0.9720 Epoch 3/12 1864/1875 [============================&gt;.] - ETA: 0s - loss: 2.3387 - root_mean_squared_error: 1.5293 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 9s 5ms/step - loss: 2.3328 - root_mean_squared_error: 1.5274 - val_loss: 0.5794 - val_root_mean_squared_error: 0.7612 Epoch 4/12 1875/1875 [==============================] - ETA: 0s - loss: 1.0310 - root_mean_squared_error: 1.0154 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 11s 6ms/step - loss: 1.0310 - root_mean_squared_error: 1.0154 - val_loss: 0.5157 - val_root_mean_squared_error: 0.7182 Epoch 5/12 1866/1875 [============================&gt;.] - ETA: 0s - loss: 0.7630 - root_mean_squared_error: 0.8735 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 9s 5ms/step - loss: 0.7630 - root_mean_squared_error: 0.8735 - val_loss: 0.5025 - val_root_mean_squared_error: 0.7089 Epoch 6/12 1875/1875 [==============================] - 6s 3ms/step - loss: 0.6904 - root_mean_squared_error: 0.8309 - val_loss: 0.5431 - val_root_mean_squared_error: 0.7370 Epoch 7/12 1870/1875 [============================&gt;.] - ETA: 0s - loss: 0.6661 - root_mean_squared_error: 0.8162 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 9s 5ms/step - loss: 0.6664 - root_mean_squared_error: 0.8163 - val_loss: 0.4952 - val_root_mean_squared_error: 0.7037 Epoch 8/12 1874/1875 [============================&gt;.] - ETA: 0s - loss: 0.6550 - root_mean_squared_error: 0.8093 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 10s 5ms/step - loss: 0.6550 - root_mean_squared_error: 0.8093 - val_loss: 0.4888 - val_root_mean_squared_error: 0.6992 Epoch 9/12 1875/1875 [==============================] - 5s 3ms/step - loss: 0.6496 - root_mean_squared_error: 0.8060 - val_loss: 0.4909 - val_root_mean_squared_error: 0.7006 Epoch 10/12 1875/1875 [==============================] - 6s 3ms/step - loss: 0.6462 - root_mean_squared_error: 0.8039 - val_loss: 0.5000 - val_root_mean_squared_error: 0.7071 Epoch 11/12 1869/1875 [============================&gt;.] - ETA: 0s - loss: 0.6429 - root_mean_squared_error: 0.8018 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 9s 5ms/step - loss: 0.6434 - root_mean_squared_error: 0.8021 - val_loss: 0.4830 - val_root_mean_squared_error: 0.6950 Epoch 12/12 1875/1875 [==============================] - 6s 3ms/step - loss: 0.6409 - root_mean_squared_error: 0.8006 - val_loss: 0.4838 - val_root_mean_squared_error: 0.6955 . &lt;keras.callbacks.History at 0x253e6764b20&gt; . . Predictions and Variance . from tensorflow.keras.models import load_model model = load_model(&#39;model&#39;) . Training . train_predictions = model.predict(X_train).flatten() train_results = pd.DataFrame(data={&#39;Train Predictions&#39;:train_predictions, &#39;Actuals&#39;:y_train}) train_results . Train Predictions Actuals . 0 -9.957811 | -9.67 | . 1 -9.741530 | -9.17 | . 2 -8.840203 | -8.10 | . 3 -7.372804 | -7.66 | . 4 -7.201422 | -7.04 | . ... ... | ... | . 59995 6.082879 | 6.07 | . 59996 7.174989 | 9.88 | . 59997 12.125348 | 13.53 | . 59998 15.739516 | 15.43 | . 59999 16.349472 | 15.54 | . 60000 rows × 2 columns . Test . test_predictions = model.predict(X_test).flatten() test_results = pd.DataFrame(data={&#39;Test Predictions&#39;:test_predictions, &#39;Actuals&#39;:y_test}) test_results . Test Predictions Actuals . 0 14.317657 | 13.99 | . 1 13.151199 | 13.46 | . 2 12.813556 | 12.93 | . 3 12.440071 | 12.43 | . 4 12.000203 | 12.17 | . ... ... | ... | . 5081 -1.141928 | -0.98 | . 5082 -1.407380 | -1.40 | . 5083 -1.605908 | -2.75 | . 5084 -3.087414 | -2.89 | . 5085 -3.146893 | -3.93 | . 5086 rows × 2 columns . Validation . val_predictions = model.predict(X_val).flatten() val_results = pd.DataFrame(data={&#39;Val Predictions&#39;:val_predictions, &#39;Actuals&#39;:y_val}) val_results . Val Predictions Actuals . 0 15.523746 | 14.02 | . 1 13.245414 | 13.67 | . 2 12.967385 | 12.27 | . 3 11.410678 | 11.19 | . 4 10.355401 | 10.85 | . ... ... | ... | . 4995 17.438112 | 18.27 | . 4996 17.329201 | 17.85 | . 4997 17.202988 | 16.65 | . 4998 15.849868 | 15.85 | . 4999 14.996454 | 15.09 | . 5000 rows × 2 columns . Results and Visualization . Training . import matplotlib.pyplot as plt plt.plot(train_results[&#39;Train Predictions&#39;][50:100]) plt.plot(train_results[&#39;Actuals&#39;][50:100]) . [&lt;matplotlib.lines.Line2D at 0x253842f9ca0&gt;] . Test . plt.plot(test_results[&#39;Test Predictions&#39;][:100]) plt.plot(test_results[&#39;Actuals&#39;][:100]) . [&lt;matplotlib.lines.Line2D at 0x253eda4e5b0&gt;] . Validation . plt.plot(val_results[&#39;Val Predictions&#39;][:100]) plt.plot(val_results[&#39;Actuals&#39;][:100]) . [&lt;matplotlib.lines.Line2D at 0x2538435bd00&gt;] .",
            "url": "https://kunal-bhar.github.io/blog/time-series/python/lstm/visualizations/2022/03/20/hourly-temp-forecasting.html",
            "relUrl": "/time-series/python/lstm/visualizations/2022/03/20/hourly-temp-forecasting.html",
            "date": " • Mar 20, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Apple Stock-Price Forecasting",
            "content": "Project At A Glance . Objective: Forecast and extrapolate prices for the Apple Stock (AAPL) over time using Time-Series data from the last five years. . Data: AAPL Stock Dataset from Tiingo API [Download] . Implementation: Time-Series Forecasting, Stacked Long Short-Term Memory (LSTM), Scaling and Transforms . Results: . Visualized forecasting for the stipulated period of 5 years. | Extended values from the 100 most recent days to predict the next 30 days. | The model projected a mild plateau in future valuation on the time of project instantiation. | . Deployment: View this project on GitHub. . Dataset . Fetching . import pandas_datareader as pdr key=&quot;&quot; . df = pdr.get_data_tiingo(&#39;AAPL&#39;, api_key=key) . df.to_csv(&#39;AAPL.csv&#39;) . import pandas as pd . Blueprint . df=pd.read_csv(&#39;AAPL.csv&#39;) . df.head() . Unnamed: 0 symbol date close high low open volume adjClose adjHigh adjLow adjOpen adjVolume divCash splitFactor . 0 0 | AAPL | 2015-05-27 00:00:00+00:00 | 132.045 | 132.260 | 130.05 | 130.34 | 45833246 | 121.682558 | 121.880685 | 119.844118 | 120.111360 | 45833246 | 0.0 | 1.0 | . 1 1 | AAPL | 2015-05-28 00:00:00+00:00 | 131.780 | 131.950 | 131.10 | 131.86 | 30733309 | 121.438354 | 121.595013 | 120.811718 | 121.512076 | 30733309 | 0.0 | 1.0 | . 2 2 | AAPL | 2015-05-29 00:00:00+00:00 | 130.280 | 131.450 | 129.90 | 131.23 | 50884452 | 120.056069 | 121.134251 | 119.705890 | 120.931516 | 50884452 | 0.0 | 1.0 | . 3 3 | AAPL | 2015-06-01 00:00:00+00:00 | 130.535 | 131.390 | 130.05 | 131.20 | 32112797 | 120.291057 | 121.078960 | 119.844118 | 120.903870 | 32112797 | 0.0 | 1.0 | . 4 4 | AAPL | 2015-06-02 00:00:00+00:00 | 129.960 | 130.655 | 129.32 | 129.86 | 33667627 | 119.761181 | 120.401640 | 119.171406 | 119.669029 | 33667627 | 0.0 | 1.0 | . df.tail() . Unnamed: 0 symbol date close high low open volume adjClose adjHigh adjLow adjOpen adjVolume divCash splitFactor . 1253 1253 | AAPL | 2020-05-18 00:00:00+00:00 | 314.96 | 316.50 | 310.3241 | 313.17 | 33843125 | 314.96 | 316.50 | 310.3241 | 313.17 | 33843125 | 0.0 | 1.0 | . 1254 1254 | AAPL | 2020-05-19 00:00:00+00:00 | 313.14 | 318.52 | 313.0100 | 315.03 | 25432385 | 313.14 | 318.52 | 313.0100 | 315.03 | 25432385 | 0.0 | 1.0 | . 1255 1255 | AAPL | 2020-05-20 00:00:00+00:00 | 319.23 | 319.52 | 316.2000 | 316.68 | 27876215 | 319.23 | 319.52 | 316.2000 | 316.68 | 27876215 | 0.0 | 1.0 | . 1256 1256 | AAPL | 2020-05-21 00:00:00+00:00 | 316.85 | 320.89 | 315.8700 | 318.66 | 25672211 | 316.85 | 320.89 | 315.8700 | 318.66 | 25672211 | 0.0 | 1.0 | . 1257 1257 | AAPL | 2020-05-22 00:00:00+00:00 | 318.89 | 319.23 | 315.3500 | 315.77 | 20450754 | 318.89 | 319.23 | 315.3500 | 315.77 | 20450754 | 0.0 | 1.0 | . Indexing and Plotting . df1=df.reset_index()[&#39;close&#39;] . df1 . 0 132.045 1 131.780 2 130.280 3 130.535 4 129.960 ... 1253 314.960 1254 313.140 1255 319.230 1256 316.850 1257 318.890 Name: close, Length: 1258, dtype: float64 . import matplotlib.pyplot as plt plt.plot(df1) . [&lt;matplotlib.lines.Line2D at 0x2d1a92724e0&gt;] . import numpy as np . df1 . 0 132.045 1 131.780 2 130.280 3 130.535 4 129.960 ... 1253 314.960 1254 313.140 1255 319.230 1256 316.850 1257 318.890 Name: close, Length: 1258, dtype: float64 . Scaling . from sklearn.preprocessing import MinMaxScaler scaler=MinMaxScaler(feature_range=(0,1)) df1=scaler.fit_transform(np.array(df1).reshape(-1,1)) . print(df1) . [[0.17607447] [0.17495567] [0.16862282] ... [0.96635143] [0.9563033 ] [0.96491598]] . Train-Test Split . training_size=int(len(df1)*0.65) test_size=len(df1)-training_size train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1] . training_size,test_size . (817, 441) . Time-Series Window . import numpy def create_dataset(dataset, time_step=1): dataX, dataY = [], [] for i in range(len(dataset)-time_step-1): a = dataset[i:(i+time_step), 0] ###i=0, 0,1,2,3--99 100 dataX.append(a) dataY.append(dataset[i + time_step, 0]) return numpy.array(dataX), numpy.array(dataY) . time_step = 100 X_train, y_train = create_dataset(train_data, time_step) X_test, ytest = create_dataset(test_data, time_step) . print(X_train.shape), print(y_train.shape) . (716, 100) (716,) . (None, None) . print(X_test.shape), print(ytest.shape) . (340, 100) (340,) . (None, None) . X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1) X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1) . Model Setup and Layers . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.layers import LSTM . model=Sequential() model.add(LSTM(50,return_sequences=True,input_shape=(100,1))) model.add(LSTM(50,return_sequences=True)) model.add(LSTM(50)) model.add(Dense(1)) model.compile(loss=&#39;mean_squared_error&#39;,optimizer=&#39;adam&#39;) . model.summary() . Model: &#34;sequential_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm_7 (LSTM) (None, 100, 50) 10400 _________________________________________________________________ lstm_8 (LSTM) (None, 100, 50) 20200 _________________________________________________________________ lstm_9 (LSTM) (None, 50) 20200 _________________________________________________________________ dense_3 (Dense) (None, 1) 51 ================================================================= Total params: 50,851 Trainable params: 50,851 Non-trainable params: 0 _________________________________________________________________ . model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=100,batch_size=64,verbose=1) . Epoch 1/100 12/12 [==============================] - 6s 487ms/step - loss: 0.0206 - val_loss: 0.0505 Epoch 2/100 12/12 [==============================] - 4s 309ms/step - loss: 0.0035 - val_loss: 0.0046 Epoch 3/100 12/12 [==============================] - 4s 300ms/step - loss: 0.0014 - val_loss: 0.0040 Epoch 4/100 12/12 [==============================] - 3s 287ms/step - loss: 8.1361e-04 - val_loss: 0.0073 Epoch 5/100 12/12 [==============================] - 3s 290ms/step - loss: 6.6860e-04 - val_loss: 0.0062 Epoch 6/100 12/12 [==============================] - 3s 255ms/step - loss: 6.4653e-04 - val_loss: 0.0062 Epoch 7/100 12/12 [==============================] - 3s 291ms/step - loss: 6.6186e-04 - val_loss: 0.0062 Epoch 8/100 12/12 [==============================] - 4s 300ms/step - loss: 6.2498e-04 - val_loss: 0.0049 Epoch 9/100 12/12 [==============================] - 4s 297ms/step - loss: 6.2745e-04 - val_loss: 0.0042 Epoch 10/100 12/12 [==============================] - 4s 303ms/step - loss: 6.0206e-04 - val_loss: 0.0050 Epoch 11/100 12/12 [==============================] - 4s 298ms/step - loss: 5.9884e-04 - val_loss: 0.0061 Epoch 12/100 12/12 [==============================] - 4s 304ms/step - loss: 6.1458e-04 - val_loss: 0.0044 Epoch 13/100 12/12 [==============================] - 4s 304ms/step - loss: 5.6830e-04 - val_loss: 0.0041 Epoch 14/100 12/12 [==============================] - 3s 262ms/step - loss: 5.5734e-04 - val_loss: 0.0038 Epoch 15/100 12/12 [==============================] - 3s 244ms/step - loss: 5.5456e-04 - val_loss: 0.0034 Epoch 16/100 12/12 [==============================] - 3s 277ms/step - loss: 5.3865e-04 - val_loss: 0.0034 Epoch 17/100 12/12 [==============================] - 3s 271ms/step - loss: 5.3872e-04 - val_loss: 0.0032 Epoch 18/100 12/12 [==============================] - 3s 260ms/step - loss: 5.2315e-04 - val_loss: 0.0030 Epoch 19/100 12/12 [==============================] - 3s 275ms/step - loss: 5.1791e-04 - val_loss: 0.0029 Epoch 20/100 12/12 [==============================] - 3s 274ms/step - loss: 5.0077e-04 - val_loss: 0.0028 Epoch 21/100 12/12 [==============================] - 3s 273ms/step - loss: 4.8672e-04 - val_loss: 0.0032 Epoch 22/100 12/12 [==============================] - 3s 270ms/step - loss: 4.9148e-04 - val_loss: 0.0026 Epoch 23/100 12/12 [==============================] - 3s 283ms/step - loss: 4.9279e-04 - val_loss: 0.0026 Epoch 24/100 12/12 [==============================] - 4s 308ms/step - loss: 5.2013e-04 - val_loss: 0.0024 Epoch 25/100 12/12 [==============================] - 3s 275ms/step - loss: 5.7301e-04 - val_loss: 0.0024 Epoch 26/100 12/12 [==============================] - 4s 295ms/step - loss: 5.5014e-04 - val_loss: 0.0030 Epoch 27/100 12/12 [==============================] - 4s 301ms/step - loss: 4.8608e-04 - val_loss: 0.0022 Epoch 28/100 12/12 [==============================] - 3s 278ms/step - loss: 4.4525e-04 - val_loss: 0.0022 Epoch 29/100 12/12 [==============================] - 4s 299ms/step - loss: 4.2446e-04 - val_loss: 0.0028 Epoch 30/100 12/12 [==============================] - 4s 302ms/step - loss: 4.9896e-04 - val_loss: 0.0023 Epoch 31/100 12/12 [==============================] - 3s 278ms/step - loss: 4.7568e-04 - val_loss: 0.0022 Epoch 32/100 12/12 [==============================] - 4s 294ms/step - loss: 4.3184e-04 - val_loss: 0.0027 Epoch 33/100 12/12 [==============================] - 4s 292ms/step - loss: 4.1365e-04 - val_loss: 0.0025 Epoch 34/100 12/12 [==============================] - 3s 276ms/step - loss: 4.0967e-04 - val_loss: 0.0022 Epoch 35/100 12/12 [==============================] - 3s 250ms/step - loss: 3.9084e-04 - val_loss: 0.0018 Epoch 36/100 12/12 [==============================] - 3s 291ms/step - loss: 3.8744e-04 - val_loss: 0.0016 Epoch 37/100 12/12 [==============================] - 3s 254ms/step - loss: 3.6441e-04 - val_loss: 0.0024 Epoch 38/100 12/12 [==============================] - 3s 272ms/step - loss: 4.3088e-04 - val_loss: 0.0025 Epoch 39/100 12/12 [==============================] - 3s 259ms/step - loss: 4.1398e-04 - val_loss: 0.0016 Epoch 40/100 12/12 [==============================] - 3s 274ms/step - loss: 3.8981e-04 - val_loss: 0.0016 Epoch 41/100 12/12 [==============================] - 3s 261ms/step - loss: 3.4896e-04 - val_loss: 0.0028 Epoch 42/100 12/12 [==============================] - 3s 282ms/step - loss: 3.7910e-04 - val_loss: 0.0014 Epoch 43/100 12/12 [==============================] - 3s 274ms/step - loss: 3.6404e-04 - val_loss: 0.0022 Epoch 44/100 12/12 [==============================] - 3s 277ms/step - loss: 3.8073e-04 - val_loss: 0.0014 Epoch 45/100 12/12 [==============================] - 3s 276ms/step - loss: 4.0008e-04 - val_loss: 0.0016 Epoch 46/100 12/12 [==============================] - 3s 273ms/step - loss: 4.0253e-04 - val_loss: 0.0015 Epoch 47/100 12/12 [==============================] - 3s 286ms/step - loss: 3.5930e-04 - val_loss: 0.0018 Epoch 48/100 12/12 [==============================] - 3s 264ms/step - loss: 3.0690e-04 - val_loss: 0.0016 Epoch 49/100 12/12 [==============================] - 3s 288ms/step - loss: 3.0504e-04 - val_loss: 0.0022 Epoch 50/100 12/12 [==============================] - 3s 277ms/step - loss: 3.1205e-04 - val_loss: 0.0016 Epoch 51/100 12/12 [==============================] - 3s 291ms/step - loss: 2.8386e-04 - val_loss: 0.0014 Epoch 52/100 12/12 [==============================] - 3s 282ms/step - loss: 2.9832e-04 - val_loss: 0.0016 Epoch 53/100 12/12 [==============================] - 3s 287ms/step - loss: 2.8287e-04 - val_loss: 0.0018 Epoch 54/100 12/12 [==============================] - 3s 286ms/step - loss: 2.8193e-04 - val_loss: 0.0013 Epoch 55/100 12/12 [==============================] - 4s 295ms/step - loss: 2.8989e-04 - val_loss: 0.0026 Epoch 56/100 12/12 [==============================] - 3s 262ms/step - loss: 2.7761e-04 - val_loss: 0.0014 Epoch 57/100 12/12 [==============================] - 3s 270ms/step - loss: 2.6088e-04 - val_loss: 0.0016 Epoch 58/100 12/12 [==============================] - 3s 289ms/step - loss: 2.7300e-04 - val_loss: 0.0013 Epoch 59/100 12/12 [==============================] - 3s 288ms/step - loss: 2.6058e-04 - val_loss: 0.0020 Epoch 60/100 12/12 [==============================] - 3s 285ms/step - loss: 2.5682e-04 - val_loss: 0.0014 Epoch 61/100 12/12 [==============================] - 3s 285ms/step - loss: 2.4091e-04 - val_loss: 0.0013 Epoch 62/100 12/12 [==============================] - 4s 296ms/step - loss: 2.2724e-04 - val_loss: 0.0016 Epoch 63/100 12/12 [==============================] - 3s 258ms/step - loss: 2.3206e-04 - val_loss: 0.0012 Epoch 64/100 12/12 [==============================] - 3s 277ms/step - loss: 2.4468e-04 - val_loss: 0.0014 Epoch 65/100 12/12 [==============================] - 3s 266ms/step - loss: 2.2395e-04 - val_loss: 0.0012 Epoch 66/100 12/12 [==============================] - 3s 263ms/step - loss: 2.1142e-04 - val_loss: 0.0012 Epoch 67/100 12/12 [==============================] - 3s 281ms/step - loss: 2.0540e-04 - val_loss: 0.0016 Epoch 68/100 12/12 [==============================] - 4s 297ms/step - loss: 2.0560e-04 - val_loss: 0.0012 Epoch 69/100 12/12 [==============================] - 3s 218ms/step - loss: 1.9982e-04 - val_loss: 0.0014 Epoch 70/100 12/12 [==============================] - 3s 257ms/step - loss: 2.3622e-04 - val_loss: 0.0015 Epoch 71/100 12/12 [==============================] - 3s 283ms/step - loss: 2.6216e-04 - val_loss: 0.0012 Epoch 72/100 12/12 [==============================] - 3s 282ms/step - loss: 2.4869e-04 - val_loss: 0.0017 Epoch 73/100 12/12 [==============================] - 3s 280ms/step - loss: 2.1853e-04 - val_loss: 0.0013 Epoch 74/100 12/12 [==============================] - 3s 244ms/step - loss: 2.2121e-04 - val_loss: 0.0014 Epoch 75/100 12/12 [==============================] - 3s 283ms/step - loss: 1.9690e-04 - val_loss: 0.0011 Epoch 76/100 12/12 [==============================] - 3s 261ms/step - loss: 2.2144e-04 - val_loss: 0.0011 Epoch 77/100 12/12 [==============================] - 3s 282ms/step - loss: 1.8420e-04 - val_loss: 0.0011 Epoch 78/100 12/12 [==============================] - 3s 282ms/step - loss: 1.7841e-04 - val_loss: 0.0014 Epoch 79/100 12/12 [==============================] - 3s 260ms/step - loss: 1.9611e-04 - val_loss: 0.0013 Epoch 80/100 12/12 [==============================] - 3s 281ms/step - loss: 2.0224e-04 - val_loss: 0.0012 Epoch 81/100 12/12 [==============================] - 3s 290ms/step - loss: 2.1049e-04 - val_loss: 0.0020 Epoch 82/100 12/12 [==============================] - 3s 288ms/step - loss: 1.9466e-04 - val_loss: 0.0010 Epoch 83/100 12/12 [==============================] - 3s 284ms/step - loss: 1.5801e-04 - val_loss: 0.0010 Epoch 84/100 12/12 [==============================] - 3s 272ms/step - loss: 1.6260e-04 - val_loss: 9.4397e-04 Epoch 85/100 12/12 [==============================] - 3s 249ms/step - loss: 1.5695e-04 - val_loss: 0.0013 Epoch 86/100 12/12 [==============================] - 3s 242ms/step - loss: 2.0192e-04 - val_loss: 9.7445e-04 Epoch 87/100 12/12 [==============================] - 3s 271ms/step - loss: 2.2179e-04 - val_loss: 0.0020 Epoch 88/100 12/12 [==============================] - 3s 249ms/step - loss: 2.5509e-04 - val_loss: 0.0015 Epoch 89/100 12/12 [==============================] - 3s 261ms/step - loss: 1.9912e-04 - val_loss: 0.0011 Epoch 90/100 12/12 [==============================] - 3s 265ms/step - loss: 1.6930e-04 - val_loss: 8.9285e-04 Epoch 91/100 12/12 [==============================] - 3s 276ms/step - loss: 1.6435e-04 - val_loss: 9.1264e-04 Epoch 92/100 12/12 [==============================] - 3s 259ms/step - loss: 1.6799e-04 - val_loss: 0.0014 Epoch 93/100 12/12 [==============================] - 3s 282ms/step - loss: 1.9593e-04 - val_loss: 0.0016 Epoch 94/100 12/12 [==============================] - 3s 287ms/step - loss: 1.8104e-04 - val_loss: 0.0010 Epoch 95/100 12/12 [==============================] - 3s 277ms/step - loss: 1.3988e-04 - val_loss: 8.5343e-04 Epoch 96/100 12/12 [==============================] - 3s 280ms/step - loss: 1.4097e-04 - val_loss: 9.3255e-04 Epoch 97/100 12/12 [==============================] - 3s 287ms/step - loss: 1.4070e-04 - val_loss: 8.3848e-04 Epoch 98/100 12/12 [==============================] - 3s 290ms/step - loss: 1.3528e-04 - val_loss: 8.4349e-04 Epoch 99/100 12/12 [==============================] - 3s 288ms/step - loss: 1.4087e-04 - val_loss: 9.8092e-04 Epoch 100/100 12/12 [==============================] - 3s 285ms/step - loss: 1.4775e-04 - val_loss: 9.3230e-04 . &lt;tensorflow.python.keras.callbacks.History at 0x2d1aa544a58&gt; . . Prediction and Metrics . train_predict=model.predict(X_train) test_predict=model.predict(X_test) . train_predict=scaler.inverse_transform(train_predict) test_predict=scaler.inverse_transform(test_predict) . import math from sklearn.metrics import mean_squared_error math.sqrt(mean_squared_error(y_train,train_predict)) . 140.9909210035748 . math.sqrt(mean_squared_error(ytest,test_predict)) . 235.7193088627771 . Present Forecast Plot . look_back=100 trainPredictPlot = numpy.empty_like(df1) trainPredictPlot[:, :] = np.nan trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict testPredictPlot = numpy.empty_like(df1) testPredictPlot[:, :] = numpy.nan testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict plt.plot(scaler.inverse_transform(df1)) plt.plot(trainPredictPlot) plt.plot(testPredictPlot) plt.show() . len(test_data) . 441 . x_input=test_data[341:].reshape(1,-1) x_input.shape . (1, 100) . temp_input=list(x_input) temp_input=temp_input[0].tolist() . Future Extension . from numpy import array lst_output=[] n_steps=100 i=0 while(i&lt;30): if(len(temp_input)&gt;100): #print(temp_input) x_input=np.array(temp_input[1:]) print(&quot;{} day input {}&quot;.format(i,x_input)) x_input=x_input.reshape(1,-1) x_input = x_input.reshape((1, n_steps, 1)) #print(x_input) yhat = model.predict(x_input, verbose=0) print(&quot;{} day output {}&quot;.format(i,yhat)) temp_input.extend(yhat[0].tolist()) temp_input=temp_input[1:] #print(temp_input) lst_output.extend(yhat.tolist()) i=i+1 else: x_input = x_input.reshape((1, n_steps,1)) yhat = model.predict(x_input, verbose=0) print(yhat[0]) temp_input.extend(yhat[0].tolist()) print(len(temp_input)) lst_output.extend(yhat.tolist()) i=i+1 print(lst_output) . [0.94413203] 101 1 day input [0.8866419 0.87431394 0.88431985 0.87836697 0.8986321 0.92582116 0.92877649 0.95676771 0.93869797 0.93304061 0.94950604 0.96424048 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203] 1 day output [[0.9379593]] 2 day input [0.87431394 0.88431985 0.87836697 0.8986321 0.92582116 0.92877649 0.95676771 0.93869797 0.93304061 0.94950604 0.96424048 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931] 2 day output [[0.9286534]] 3 day input [0.88431985 0.87836697 0.8986321 0.92582116 0.92877649 0.95676771 0.93869797 0.93304061 0.94950604 0.96424048 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342] 3 day output [[0.91987926]] 4 day input [0.87836697 0.8986321 0.92582116 0.92877649 0.95676771 0.93869797 0.93304061 0.94950604 0.96424048 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926] 4 day output [[0.9128097]] 5 day input [0.8986321 0.92582116 0.92877649 0.95676771 0.93869797 0.93304061 0.94950604 0.96424048 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973] 5 day output [[0.90777564]] 6 day input [0.92582116 0.92877649 0.95676771 0.93869797 0.93304061 0.94950604 0.96424048 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564] 6 day output [[0.9047326]] 7 day input [0.92877649 0.95676771 0.93869797 0.93304061 0.94950604 0.96424048 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258] 7 day output [[0.9033923]] 8 day input [0.95676771 0.93869797 0.93304061 0.94950604 0.96424048 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231] 8 day output [[0.90332204]] 9 day input [0.93869797 0.93304061 0.94950604 0.96424048 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204] 9 day output [[0.9040391]] 10 day input [0.93304061 0.94950604 0.96424048 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908] 10 day output [[0.9050924]] 11 day input [0.94950604 0.96424048 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242] 11 day output [[0.906118]] 12 day input [0.96424048 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798] 12 day output [[0.90686554]] 13 day input [0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554] 13 day output [[0.90720606]] 14 day input [0.95989192 0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606] 14 day output [[0.9071163]] 15 day input [0.96635143 0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629] 15 day output [[0.9066538]] 16 day input [0.96246728 0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382] 16 day output [[0.90592706]] 17 day input [0.92295027 0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706] 17 day output [[0.9050646]] 18 day input [0.9598497 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458] 18 day output [[0.90419257]] 19 day input [0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458 0.90419257] 19 day output [[0.9034131]] 20 day input [0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458 0.90419257 0.90341312] 20 day output [[0.90279734]] 21 day input [0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458 0.90419257 0.90341312 0.90279734] 21 day output [[0.9023812]] 22 day input [0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458 0.90419257 0.90341312 0.90279734 0.90238118] 22 day output [[0.9021694]] 23 day input [0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458 0.90419257 0.90341312 0.90279734 0.90238118 0.90216941] 23 day output [[0.90213937]] 24 day input [0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458 0.90419257 0.90341312 0.90279734 0.90238118 0.90216941 0.90213937] 24 day output [[0.9022528]] 25 day input [0.99159841 0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458 0.90419257 0.90341312 0.90279734 0.90238118 0.90216941 0.90213937 0.90225279] 25 day output [[0.90246403]] 26 day input [0.96972895 0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458 0.90419257 0.90341312 0.90279734 0.90238118 0.90216941 0.90213937 0.90225279 0.90246403] 26 day output [[0.90272856]] 27 day input [0.97614625 0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458 0.90419257 0.90341312 0.90279734 0.90238118 0.90216941 0.90213937 0.90225279 0.90246403 0.90272856] 27 day output [[0.90300757]] 28 day input [0.96795575 1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458 0.90419257 0.90341312 0.90279734 0.90238118 0.90216941 0.90213937 0.90225279 0.90246403 0.90272856 0.90300757] 28 day output [[0.903272]] 29 day input [1. 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963 0.7921557 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292 0.8289707 0.8125475 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239 0.94064004 0.96635143 0.9563033 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458 0.90419257 0.90341312 0.90279734 0.90238118 0.90216941 0.90213937 0.90225279 0.90246403 0.90272856 0.90300757 0.90327197] 29 day output [[0.90350425]] [[0.9441320300102234], [0.9379593133926392], [0.9286534190177917], [0.9198792576789856], [0.9128097295761108], [0.9077756404876709], [0.9047325849533081], [0.9033923149108887], [0.9033220410346985], [0.9040390849113464], [0.9050924181938171], [0.9061179757118225], [0.9068655371665955], [0.9072060585021973], [0.9071162939071655], [0.9066538214683533], [0.9059270620346069], [0.905064582824707], [0.9041925668716431], [0.9034131169319153], [0.9027973413467407], [0.902381181716919], [0.902169406414032], [0.9021393656730652], [0.9022527933120728], [0.9024640321731567], [0.9027285575866699], [0.9030075669288635], [0.9032719731330872], [0.9035042524337769]] . . Predicting the Next 30 Days . DataFrame Split . day_new=np.arange(1,101) day_pred=np.arange(101,131) . import matplotlib.pyplot as plt . len(df1) . 1258 . Extended Plots . plt.plot(day_new,scaler.inverse_transform(df1[1158:])) plt.plot(day_pred,scaler.inverse_transform(lst_output)) . [&lt;matplotlib.lines.Line2D at 0x2d1b0f352b0&gt;] . df3=df1.tolist() df3.extend(lst_output) plt.plot(df3[1200:]) . [&lt;matplotlib.lines.Line2D at 0x2d1b0f55ac8&gt;] . df3=scaler.inverse_transform(df3).tolist() . plt.plot(df3) . [&lt;matplotlib.lines.Line2D at 0x2d1a904c470&gt;] .",
            "url": "https://kunal-bhar.github.io/blog/time-series/python/lstm/visualizations/2022/03/19/apple-stock-forecasting.html",
            "relUrl": "/time-series/python/lstm/visualizations/2022/03/19/apple-stock-forecasting.html",
            "date": " • Mar 19, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Reddit Headline Analysis",
            "content": "Project At A Glance . Objective: Discover sentiments associated with posts in the &#39;Hot&#39; section of r/worldnews and classify them as Positive, Negative and Neutral. . Data: 754x1 Dataset of the sub-reddit&#39;s headlines scraped using the Reddit API. [View Scraper Notebook] [Download] . Implementation: Reddit API, PRAW, NLTK&#39;s Sentiment Intensity Analyzer (SIA) . Results: . More than half of the headlines were classified as Neutral (~55%). | However, Negative Headlines (33%) still outweigh Positive Headlines (12%) by about 2.75x. | Dataset generated with labelled values to formulate models with better intelligence in the future. | . Deployment: View this project on GitHub. . Dependencies . import pandas as pd from pprint import pprint . Dataset Initialization . df = pd.read_csv(&#39;reddit-headlines.csv&#39;) . df.head() . Unnamed: 0 headlines . 0 0 | Mass graves dug in the besieged Ukrainian city... | . 1 1 | British aircraft carrier leading massive fleet... | . 2 2 | Spain detains $600 million yacht linked to Rus... | . 3 3 | Photo shows officials taking down the Russian ... | . 4 4 | Marina Ovsyannikova: Russian journalist tells ... | . Using NLTK&#39;s Sentiment Intensity Analyzer . import nltk from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA . import matplotlib.pyplot as plt import seaborn as sns . sia= SIA() results = [] for line in df[&#39;headlines&#39;]: pol_score = sia.polarity_scores(line) pol_score[&#39;headline&#39;] = line results.append(pol_score) pprint(results[:3], width=100) . [{&#39;compound&#39;: -0.7579, &#39;headline&#39;: &#39;Mass graves dug in the besieged Ukrainian city of Mariupol, as locals bury their &#39; &#39;dead&#39;, &#39;neg&#39;: 0.333, &#39;neu&#39;: 0.667, &#39;pos&#39;: 0.0}, {&#39;compound&#39;: 0.0, &#39;headline&#39;: &#39;British aircraft carrier leading massive fleet off Norway&#39;, &#39;neg&#39;: 0.0, &#39;neu&#39;: 1.0, &#39;pos&#39;: 0.0}, {&#39;compound&#39;: 0.0, &#39;headline&#39;: &#39;Spain detains $600 million yacht linked to Russian oligarch: Reuters&#39;, &#39;neg&#39;: 0.0, &#39;neu&#39;: 1.0, &#39;pos&#39;: 0.0}] . Generating DataFrame with Polarity Scores . df = pd.DataFrame.from_records(results) . df.sample(4) . neg neu pos compound headline . 380 0.506 | 0.494 | 0.000 | -0.9231 | Tigray war has seen up to half a million dead ... | . 22 0.289 | 0.711 | 0.000 | -0.4291 | &#39;Why? Why? Why?&#39; Ukraine&#39;s Mariupol descends i... | . 660 0.000 | 0.806 | 0.194 | 0.3400 | Ukraine&#39;s &#39;hero&#39; President Zelensky set to rec... | . 725 0.100 | 0.594 | 0.306 | 0.6705 | There is no life for Ukrainian people: Boxing ... | . Labelling and Classification . df[&#39;label&#39;] = 0 df.loc[df[&#39;compound&#39;]&gt;0.33, &#39;label&#39;] = 1 df.loc[df[&#39;compound&#39;]&lt;-0.33, &#39;label&#39;] = -1 . df.sample(4) . neg neu pos compound headline label . 461 0.273 | 0.727 | 0.000 | -0.4588 | Trudeau and almost every Canadian MP banned fr... | -1 | . 482 0.000 | 0.647 | 0.353 | 0.5994 | Help yourself by helping us, Ukraine&#39;s Zelensk... | 1 | . 420 0.000 | 1.000 | 0.000 | 0.0000 | Russia, India explore opening alternative paym... | 0 | . 748 0.239 | 0.761 | 0.000 | -0.2960 | Russia&#39;s state TV hit by stream of resignations | 0 | . df.label.value_counts() . 0 414 -1 247 1 93 Name: label, dtype: int64 . df.label.value_counts(normalize=True)*100 . 0 54.907162 -1 32.758621 1 12.334218 Name: label, dtype: float64 . Examples . print(&#39;Positive Headlines: n&#39;) pprint(list(df[df[&#39;label&#39;] == 1].headline)[:5], width=200) print(&#39; n n Negative Headlines: n&#39;) pprint(list(df[df[&#39;label&#39;] == -1].headline)[:5], width=200) print(&#39; n n Neutral Headlines: n&#39;) pprint(list(df[df[&#39;label&#39;] == 0].headline)[:5], width=200) . Positive Headlines: [&#39;Tibetans seek justice after 63 years of uprising against Chinese rule&#39;, &#34;The Ministry of Foreign Affairs on Tuesday (March 15) praised a Russian woman for her courage after she held up an anti-war sign on live Russian TV. MOFA head: &#39;It takes courage to be the voice of &#34; &#34;conscience&#39;.&#34;, &#39;Saudi Arabia considers accepting yuan for oil sales&#39;, &#39;Russia and Ukraine looking for compromise in peace talks&#39;, &#39;Turkmenistan leader’s son wins presidential election&#39;] Negative Headlines: [&#39;Mass graves dug in the besieged Ukrainian city of Mariupol, as locals bury their dead&#39;, &#34;Russia&#39;s former chief prosecutor says oligarch Roman Abramovich amassed his fortune through a &#39;fraudulent scheme&#39;&#34;, &#39;UN makes March 15 International Day to Combat Islamophobia&#39;, &#39;Not violation of sanctions but Russian oil deal could put India on wrong side of history, says US&#39;, &#34;&#39;Why? Why? Why?&#39; Ukraine&#39;s Mariupol descends into despair&#34;] Neutral Headlines: [&#39;British aircraft carrier leading massive fleet off Norway&#39;, &#39;Spain detains $600 million yacht linked to Russian oligarch: Reuters&#39;, &#39;Photo shows officials taking down the Russian flag after Putin gets the boot from Council of Europe&#39;, &#39;Marina Ovsyannikova: Russian journalist tells of 14-hour interrogation&#39;, &#39;China wary of being impacted by Russia sanctions: Foreign Minister&#39;] . Results and Visualization . fig, ax = plt.subplots(figsize=(8,8)) counts = df.label.value_counts(normalize=True)*100 sns.barplot(x=counts.index, y=counts, ax=ax) ax.set_xticklabels([&#39;Negative&#39;, &#39;Neutral&#39;, &#39;Positive&#39;]) ax.set_ylabel(&#39;Percentage&#39;) plt.show() . Exporting Labelled Dataset as (.csv) . df_export = df[[&#39;headline&#39;, &#39;label&#39;]] . df_export.sample(4) . headline label . 399 EU &#39;Concerned&#39; Over Disrupted Gas Supply, Shoo... | 0 | . 709 Woman fearing for family in Ukraine urges Cana... | -1 | . 618 New Zealand cuts fuel tax and halves public tr... | -1 | . 560 Slovakia meets NATO defence spending commitmen... | 1 | . df_export.to_csv(&#39;reddit-headlines-labelled.csv&#39;, encoding=&#39;utf-8&#39;, index=True) .",
            "url": "https://kunal-bhar.github.io/blog/nlp/python/reddit/news/2022/03/17/reddit-news-analysis.html",
            "relUrl": "/nlp/python/reddit/news/2022/03/17/reddit-news-analysis.html",
            "date": " • Mar 17, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Concurrent Number Prognosis",
            "content": "Project At A Glance . Objective: Predicting values in numeric data that sequentially lead by a given parameter: n. . Data: . Data: Group of 100 5x1 lists that contain overlapping adjacent values. Example: [[1:5], [2:6], [3:7]...] | Target: Group of 100 1x1 lists with the target values for generation. Example: [[1], [2], [3]...] | . Implementation: Long Short-Term Memory (LSTM), Recurrent Neural Networks . Results: . The LSTM computed f-score = 0.935 and loss = 0.036 on training for 400 epochs. | Scatter plots to visualize performance and loss deprecation. | . Deployment: View this project on GitHub. . Dependencies . import numpy as np import matplotlib.pyplot as plt from keras.models import Sequential from keras.layers import Dense, LSTM from sklearn.model_selection import train_test_split . Initialization . Data = [[[(i+j)/100] for i in range(5)] for j in range(100)] Target = [(i+5)/100 for i in range(100)] . Data as NumPy Arrays . data = np.array(Data, dtype=float) target = np.array(Target, dtype=float) . data.shape . (100, 5, 1) . target.shape . (100,) . Train-Test Split . x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=4) . Model Setup and Layers . model = Sequential() . model.add(LSTM((1), batch_input_shape=(None,5,1), return_sequences=True)) model.add(LSTM((1), return_sequences=False)) # the return_sequences parameter assists in enabling convolution for the sequential LSTM . model.compile(loss=&#39;mean_absolute_error&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm (LSTM) (None, 5, 1) 12 lstm_1 (LSTM) (None, 1) 12 ================================================================= Total params: 24 Trainable params: 24 Non-trainable params: 0 _________________________________________________________________ . history = model.fit(x_train, y_train, epochs=400, validation_data=(x_test, y_test)) . Epoch 1/400 3/3 [==============================] - 5s 474ms/step - loss: 0.5351 - accuracy: 0.0000e+00 - val_loss: 0.4311 - val_accuracy: 0.0000e+00 Epoch 2/400 3/3 [==============================] - 0s 27ms/step - loss: 0.5325 - accuracy: 0.0000e+00 - val_loss: 0.4286 - val_accuracy: 0.0000e+00 Epoch 3/400 3/3 [==============================] - 0s 25ms/step - loss: 0.5297 - accuracy: 0.0000e+00 - val_loss: 0.4260 - val_accuracy: 0.0000e+00 Epoch 4/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5270 - accuracy: 0.0000e+00 - val_loss: 0.4233 - val_accuracy: 0.0000e+00 Epoch 5/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5241 - accuracy: 0.0000e+00 - val_loss: 0.4207 - val_accuracy: 0.0000e+00 Epoch 6/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5213 - accuracy: 0.0000e+00 - val_loss: 0.4179 - val_accuracy: 0.0000e+00 Epoch 7/400 3/3 [==============================] - 0s 21ms/step - loss: 0.5184 - accuracy: 0.0000e+00 - val_loss: 0.4151 - val_accuracy: 0.0000e+00 Epoch 8/400 3/3 [==============================] - 0s 15ms/step - loss: 0.5154 - accuracy: 0.0000e+00 - val_loss: 0.4123 - val_accuracy: 0.0000e+00 Epoch 9/400 3/3 [==============================] - 0s 13ms/step - loss: 0.5123 - accuracy: 0.0000e+00 - val_loss: 0.4094 - val_accuracy: 0.0000e+00 Epoch 10/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5093 - accuracy: 0.0000e+00 - val_loss: 0.4064 - val_accuracy: 0.0000e+00 Epoch 11/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5061 - accuracy: 0.0000e+00 - val_loss: 0.4034 - val_accuracy: 0.0000e+00 Epoch 12/400 3/3 [==============================] - 0s 15ms/step - loss: 0.5029 - accuracy: 0.0000e+00 - val_loss: 0.4003 - val_accuracy: 0.0000e+00 Epoch 13/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4996 - accuracy: 0.0000e+00 - val_loss: 0.3972 - val_accuracy: 0.0000e+00 Epoch 14/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4962 - accuracy: 0.0000e+00 - val_loss: 0.3940 - val_accuracy: 0.0000e+00 Epoch 15/400 3/3 [==============================] - 0s 17ms/step - loss: 0.4928 - accuracy: 0.0000e+00 - val_loss: 0.3907 - val_accuracy: 0.0000e+00 Epoch 16/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4893 - accuracy: 0.0000e+00 - val_loss: 0.3873 - val_accuracy: 0.0000e+00 Epoch 17/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4857 - accuracy: 0.0000e+00 - val_loss: 0.3839 - val_accuracy: 0.0000e+00 Epoch 18/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4820 - accuracy: 0.0000e+00 - val_loss: 0.3804 - val_accuracy: 0.0000e+00 Epoch 19/400 3/3 [==============================] - 0s 13ms/step - loss: 0.4783 - accuracy: 0.0000e+00 - val_loss: 0.3768 - val_accuracy: 0.0000e+00 Epoch 20/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4745 - accuracy: 0.0000e+00 - val_loss: 0.3731 - val_accuracy: 0.0000e+00 Epoch 21/400 3/3 [==============================] - 0s 16ms/step - loss: 0.4705 - accuracy: 0.0000e+00 - val_loss: 0.3693 - val_accuracy: 0.0000e+00 Epoch 22/400 3/3 [==============================] - 0s 18ms/step - loss: 0.4665 - accuracy: 0.0000e+00 - val_loss: 0.3654 - val_accuracy: 0.0000e+00 Epoch 23/400 3/3 [==============================] - 0s 22ms/step - loss: 0.4625 - accuracy: 0.0000e+00 - val_loss: 0.3615 - val_accuracy: 0.0000e+00 Epoch 24/400 3/3 [==============================] - 0s 25ms/step - loss: 0.4584 - accuracy: 0.0000e+00 - val_loss: 0.3575 - val_accuracy: 0.0000e+00 Epoch 25/400 3/3 [==============================] - 0s 22ms/step - loss: 0.4542 - accuracy: 0.0000e+00 - val_loss: 0.3534 - val_accuracy: 0.0000e+00 Epoch 26/400 3/3 [==============================] - 0s 29ms/step - loss: 0.4500 - accuracy: 0.0000e+00 - val_loss: 0.3492 - val_accuracy: 0.0000e+00 Epoch 27/400 3/3 [==============================] - 0s 30ms/step - loss: 0.4457 - accuracy: 0.0000e+00 - val_loss: 0.3450 - val_accuracy: 0.0000e+00 Epoch 28/400 3/3 [==============================] - 0s 29ms/step - loss: 0.4413 - accuracy: 0.0000e+00 - val_loss: 0.3408 - val_accuracy: 0.0000e+00 Epoch 29/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4369 - accuracy: 0.0000e+00 - val_loss: 0.3367 - val_accuracy: 0.0000e+00 Epoch 30/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4324 - accuracy: 0.0000e+00 - val_loss: 0.3325 - val_accuracy: 0.0000e+00 Epoch 31/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4278 - accuracy: 0.0000e+00 - val_loss: 0.3282 - val_accuracy: 0.0000e+00 Epoch 32/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4232 - accuracy: 0.0000e+00 - val_loss: 0.3238 - val_accuracy: 0.0000e+00 Epoch 33/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4185 - accuracy: 0.0000e+00 - val_loss: 0.3194 - val_accuracy: 0.0000e+00 Epoch 34/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4138 - accuracy: 0.0000e+00 - val_loss: 0.3149 - val_accuracy: 0.0000e+00 Epoch 35/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4091 - accuracy: 0.0000e+00 - val_loss: 0.3104 - val_accuracy: 0.0000e+00 Epoch 36/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4043 - accuracy: 0.0000e+00 - val_loss: 0.3057 - val_accuracy: 0.0000e+00 Epoch 37/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3994 - accuracy: 0.0000e+00 - val_loss: 0.3010 - val_accuracy: 0.0000e+00 Epoch 38/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3946 - accuracy: 0.0000e+00 - val_loss: 0.2962 - val_accuracy: 0.0000e+00 Epoch 39/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3897 - accuracy: 0.0000e+00 - val_loss: 0.2913 - val_accuracy: 0.0000e+00 Epoch 40/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3850 - accuracy: 0.0000e+00 - val_loss: 0.2864 - val_accuracy: 0.0000e+00 Epoch 41/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3802 - accuracy: 0.0000e+00 - val_loss: 0.2815 - val_accuracy: 0.0000e+00 Epoch 42/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3753 - accuracy: 0.0000e+00 - val_loss: 0.2765 - val_accuracy: 0.0000e+00 Epoch 43/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3705 - accuracy: 0.0000e+00 - val_loss: 0.2715 - val_accuracy: 0.0000e+00 Epoch 44/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3658 - accuracy: 0.0000e+00 - val_loss: 0.2664 - val_accuracy: 0.0000e+00 Epoch 45/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3612 - accuracy: 0.0000e+00 - val_loss: 0.2614 - val_accuracy: 0.0000e+00 Epoch 46/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3562 - accuracy: 0.0000e+00 - val_loss: 0.2568 - val_accuracy: 0.0000e+00 Epoch 47/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3515 - accuracy: 0.0000e+00 - val_loss: 0.2521 - val_accuracy: 0.0000e+00 Epoch 48/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3467 - accuracy: 0.0000e+00 - val_loss: 0.2474 - val_accuracy: 0.0000e+00 Epoch 49/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3420 - accuracy: 0.0000e+00 - val_loss: 0.2426 - val_accuracy: 0.0000e+00 Epoch 50/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3373 - accuracy: 0.0000e+00 - val_loss: 0.2379 - val_accuracy: 0.0000e+00 Epoch 51/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3326 - accuracy: 0.0000e+00 - val_loss: 0.2335 - val_accuracy: 0.0000e+00 Epoch 52/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3279 - accuracy: 0.0000e+00 - val_loss: 0.2294 - val_accuracy: 0.0000e+00 Epoch 53/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3231 - accuracy: 0.0000e+00 - val_loss: 0.2254 - val_accuracy: 0.0000e+00 Epoch 54/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3182 - accuracy: 0.0000e+00 - val_loss: 0.2214 - val_accuracy: 0.0000e+00 Epoch 55/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3136 - accuracy: 0.0000e+00 - val_loss: 0.2180 - val_accuracy: 0.0000e+00 Epoch 56/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3088 - accuracy: 0.0000e+00 - val_loss: 0.2145 - val_accuracy: 0.0000e+00 Epoch 57/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3043 - accuracy: 0.0000e+00 - val_loss: 0.2111 - val_accuracy: 0.0000e+00 Epoch 58/400 3/3 [==============================] - 0s 15ms/step - loss: 0.2999 - accuracy: 0.0000e+00 - val_loss: 0.2077 - val_accuracy: 0.0000e+00 Epoch 59/400 3/3 [==============================] - 0s 14ms/step - loss: 0.2952 - accuracy: 0.0000e+00 - val_loss: 0.2043 - val_accuracy: 0.0000e+00 Epoch 60/400 3/3 [==============================] - 0s 16ms/step - loss: 0.2914 - accuracy: 0.0000e+00 - val_loss: 0.2013 - val_accuracy: 0.0000e+00 Epoch 61/400 3/3 [==============================] - 0s 24ms/step - loss: 0.2867 - accuracy: 0.0000e+00 - val_loss: 0.1985 - val_accuracy: 0.0000e+00 Epoch 62/400 3/3 [==============================] - 0s 40ms/step - loss: 0.2828 - accuracy: 0.0000e+00 - val_loss: 0.1957 - val_accuracy: 0.0000e+00 Epoch 63/400 3/3 [==============================] - 0s 35ms/step - loss: 0.2788 - accuracy: 0.0000e+00 - val_loss: 0.1929 - val_accuracy: 0.0000e+00 Epoch 64/400 3/3 [==============================] - 0s 26ms/step - loss: 0.2750 - accuracy: 0.0000e+00 - val_loss: 0.1901 - val_accuracy: 0.0000e+00 Epoch 65/400 3/3 [==============================] - 0s 27ms/step - loss: 0.2710 - accuracy: 0.0000e+00 - val_loss: 0.1877 - val_accuracy: 0.0000e+00 Epoch 66/400 3/3 [==============================] - 0s 33ms/step - loss: 0.2674 - accuracy: 0.0000e+00 - val_loss: 0.1855 - val_accuracy: 0.0000e+00 Epoch 67/400 3/3 [==============================] - 0s 45ms/step - loss: 0.2639 - accuracy: 0.0000e+00 - val_loss: 0.1833 - val_accuracy: 0.0000e+00 Epoch 68/400 3/3 [==============================] - 0s 42ms/step - loss: 0.2604 - accuracy: 0.0000e+00 - val_loss: 0.1816 - val_accuracy: 0.0000e+00 Epoch 69/400 3/3 [==============================] - 0s 36ms/step - loss: 0.2568 - accuracy: 0.0000e+00 - val_loss: 0.1800 - val_accuracy: 0.0000e+00 Epoch 70/400 3/3 [==============================] - 0s 37ms/step - loss: 0.2536 - accuracy: 0.0000e+00 - val_loss: 0.1785 - val_accuracy: 0.0000e+00 Epoch 71/400 3/3 [==============================] - 0s 48ms/step - loss: 0.2504 - accuracy: 0.0000e+00 - val_loss: 0.1769 - val_accuracy: 0.0000e+00 Epoch 72/400 3/3 [==============================] - 0s 51ms/step - loss: 0.2474 - accuracy: 0.0000e+00 - val_loss: 0.1757 - val_accuracy: 0.0000e+00 Epoch 73/400 3/3 [==============================] - 0s 50ms/step - loss: 0.2446 - accuracy: 0.0000e+00 - val_loss: 0.1747 - val_accuracy: 0.0000e+00 Epoch 74/400 3/3 [==============================] - 0s 48ms/step - loss: 0.2416 - accuracy: 0.0000e+00 - val_loss: 0.1737 - val_accuracy: 0.0000e+00 Epoch 75/400 3/3 [==============================] - 0s 55ms/step - loss: 0.2390 - accuracy: 0.0000e+00 - val_loss: 0.1727 - val_accuracy: 0.0000e+00 Epoch 76/400 3/3 [==============================] - 0s 51ms/step - loss: 0.2364 - accuracy: 0.0000e+00 - val_loss: 0.1718 - val_accuracy: 0.0000e+00 Epoch 77/400 3/3 [==============================] - 0s 59ms/step - loss: 0.2339 - accuracy: 0.0000e+00 - val_loss: 0.1708 - val_accuracy: 0.0000e+00 Epoch 78/400 3/3 [==============================] - 0s 54ms/step - loss: 0.2316 - accuracy: 0.0000e+00 - val_loss: 0.1698 - val_accuracy: 0.0000e+00 Epoch 79/400 3/3 [==============================] - 0s 28ms/step - loss: 0.2293 - accuracy: 0.0000e+00 - val_loss: 0.1688 - val_accuracy: 0.0000e+00 Epoch 80/400 3/3 [==============================] - 0s 21ms/step - loss: 0.2274 - accuracy: 0.0000e+00 - val_loss: 0.1678 - val_accuracy: 0.0000e+00 Epoch 81/400 3/3 [==============================] - 0s 23ms/step - loss: 0.2253 - accuracy: 0.0000e+00 - val_loss: 0.1668 - val_accuracy: 0.0000e+00 Epoch 82/400 3/3 [==============================] - 0s 25ms/step - loss: 0.2232 - accuracy: 0.0000e+00 - val_loss: 0.1661 - val_accuracy: 0.0000e+00 Epoch 83/400 3/3 [==============================] - 0s 23ms/step - loss: 0.2214 - accuracy: 0.0000e+00 - val_loss: 0.1654 - val_accuracy: 0.0000e+00 Epoch 84/400 3/3 [==============================] - 0s 46ms/step - loss: 0.2195 - accuracy: 0.0000e+00 - val_loss: 0.1647 - val_accuracy: 0.0500 Epoch 85/400 3/3 [==============================] - 0s 46ms/step - loss: 0.2176 - accuracy: 0.0000e+00 - val_loss: 0.1640 - val_accuracy: 0.0500 Epoch 86/400 3/3 [==============================] - 0s 43ms/step - loss: 0.2158 - accuracy: 0.0000e+00 - val_loss: 0.1633 - val_accuracy: 0.0500 Epoch 87/400 3/3 [==============================] - 0s 35ms/step - loss: 0.2140 - accuracy: 0.0000e+00 - val_loss: 0.1629 - val_accuracy: 0.0500 Epoch 88/400 3/3 [==============================] - 0s 25ms/step - loss: 0.2122 - accuracy: 0.0000e+00 - val_loss: 0.1625 - val_accuracy: 0.0500 Epoch 89/400 3/3 [==============================] - 0s 23ms/step - loss: 0.2104 - accuracy: 0.0000e+00 - val_loss: 0.1621 - val_accuracy: 0.0500 Epoch 90/400 3/3 [==============================] - 0s 24ms/step - loss: 0.2085 - accuracy: 0.0000e+00 - val_loss: 0.1617 - val_accuracy: 0.0500 Epoch 91/400 3/3 [==============================] - 0s 30ms/step - loss: 0.2070 - accuracy: 0.0000e+00 - val_loss: 0.1612 - val_accuracy: 0.0500 Epoch 92/400 3/3 [==============================] - 0s 25ms/step - loss: 0.2052 - accuracy: 0.0000e+00 - val_loss: 0.1607 - val_accuracy: 0.0500 Epoch 93/400 3/3 [==============================] - 0s 25ms/step - loss: 0.2036 - accuracy: 0.0000e+00 - val_loss: 0.1601 - val_accuracy: 0.0500 Epoch 94/400 3/3 [==============================] - 0s 30ms/step - loss: 0.2020 - accuracy: 0.0000e+00 - val_loss: 0.1595 - val_accuracy: 0.0500 Epoch 95/400 3/3 [==============================] - 0s 28ms/step - loss: 0.2005 - accuracy: 0.0000e+00 - val_loss: 0.1589 - val_accuracy: 0.0500 Epoch 96/400 3/3 [==============================] - 0s 29ms/step - loss: 0.1989 - accuracy: 0.0000e+00 - val_loss: 0.1583 - val_accuracy: 0.0500 Epoch 97/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1974 - accuracy: 0.0000e+00 - val_loss: 0.1577 - val_accuracy: 0.0500 Epoch 98/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1959 - accuracy: 0.0000e+00 - val_loss: 0.1571 - val_accuracy: 0.0500 Epoch 99/400 3/3 [==============================] - 0s 22ms/step - loss: 0.1943 - accuracy: 0.0000e+00 - val_loss: 0.1565 - val_accuracy: 0.0500 Epoch 100/400 3/3 [==============================] - 0s 24ms/step - loss: 0.1928 - accuracy: 0.0000e+00 - val_loss: 0.1558 - val_accuracy: 0.0500 Epoch 101/400 3/3 [==============================] - 0s 36ms/step - loss: 0.1913 - accuracy: 0.0000e+00 - val_loss: 0.1552 - val_accuracy: 0.0500 Epoch 102/400 3/3 [==============================] - 0s 49ms/step - loss: 0.1899 - accuracy: 0.0000e+00 - val_loss: 0.1544 - val_accuracy: 0.0500 Epoch 103/400 3/3 [==============================] - 0s 56ms/step - loss: 0.1883 - accuracy: 0.0000e+00 - val_loss: 0.1537 - val_accuracy: 0.0500 Epoch 104/400 3/3 [==============================] - 0s 52ms/step - loss: 0.1867 - accuracy: 0.0000e+00 - val_loss: 0.1529 - val_accuracy: 0.0500 Epoch 105/400 3/3 [==============================] - 0s 46ms/step - loss: 0.1852 - accuracy: 0.0000e+00 - val_loss: 0.1521 - val_accuracy: 0.0500 Epoch 106/400 3/3 [==============================] - 0s 37ms/step - loss: 0.1837 - accuracy: 0.0000e+00 - val_loss: 0.1512 - val_accuracy: 0.0500 Epoch 107/400 3/3 [==============================] - 0s 40ms/step - loss: 0.1822 - accuracy: 0.0000e+00 - val_loss: 0.1503 - val_accuracy: 0.0500 Epoch 108/400 3/3 [==============================] - 0s 47ms/step - loss: 0.1806 - accuracy: 0.0000e+00 - val_loss: 0.1494 - val_accuracy: 0.0500 Epoch 109/400 3/3 [==============================] - 0s 37ms/step - loss: 0.1790 - accuracy: 0.0000e+00 - val_loss: 0.1485 - val_accuracy: 0.0500 Epoch 110/400 3/3 [==============================] - 0s 29ms/step - loss: 0.1775 - accuracy: 0.0000e+00 - val_loss: 0.1476 - val_accuracy: 0.0500 Epoch 111/400 3/3 [==============================] - 0s 25ms/step - loss: 0.1759 - accuracy: 0.0000e+00 - val_loss: 0.1466 - val_accuracy: 0.0500 Epoch 112/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1744 - accuracy: 0.0000e+00 - val_loss: 0.1457 - val_accuracy: 0.0500 Epoch 113/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1727 - accuracy: 0.0000e+00 - val_loss: 0.1447 - val_accuracy: 0.0500 Epoch 114/400 3/3 [==============================] - 0s 28ms/step - loss: 0.1711 - accuracy: 0.0000e+00 - val_loss: 0.1434 - val_accuracy: 0.0500 Epoch 115/400 3/3 [==============================] - 0s 47ms/step - loss: 0.1695 - accuracy: 0.0000e+00 - val_loss: 0.1421 - val_accuracy: 0.0500 Epoch 116/400 3/3 [==============================] - 0s 44ms/step - loss: 0.1679 - accuracy: 0.0000e+00 - val_loss: 0.1406 - val_accuracy: 0.0500 Epoch 117/400 3/3 [==============================] - 0s 46ms/step - loss: 0.1662 - accuracy: 0.0000e+00 - val_loss: 0.1391 - val_accuracy: 0.0500 Epoch 118/400 3/3 [==============================] - 0s 45ms/step - loss: 0.1646 - accuracy: 0.0000e+00 - val_loss: 0.1373 - val_accuracy: 0.0500 Epoch 119/400 3/3 [==============================] - 0s 49ms/step - loss: 0.1628 - accuracy: 0.0000e+00 - val_loss: 0.1358 - val_accuracy: 0.0500 Epoch 120/400 3/3 [==============================] - 0s 51ms/step - loss: 0.1612 - accuracy: 0.0000e+00 - val_loss: 0.1339 - val_accuracy: 0.0500 Epoch 121/400 3/3 [==============================] - 0s 52ms/step - loss: 0.1595 - accuracy: 0.0000e+00 - val_loss: 0.1321 - val_accuracy: 0.0500 Epoch 122/400 3/3 [==============================] - 0s 52ms/step - loss: 0.1577 - accuracy: 0.0000e+00 - val_loss: 0.1304 - val_accuracy: 0.0500 Epoch 123/400 3/3 [==============================] - 0s 33ms/step - loss: 0.1560 - accuracy: 0.0000e+00 - val_loss: 0.1285 - val_accuracy: 0.0500 Epoch 124/400 3/3 [==============================] - 0s 22ms/step - loss: 0.1542 - accuracy: 0.0000e+00 - val_loss: 0.1266 - val_accuracy: 0.0500 Epoch 125/400 3/3 [==============================] - 0s 21ms/step - loss: 0.1525 - accuracy: 0.0000e+00 - val_loss: 0.1245 - val_accuracy: 0.0500 Epoch 126/400 3/3 [==============================] - 0s 45ms/step - loss: 0.1507 - accuracy: 0.0000e+00 - val_loss: 0.1227 - val_accuracy: 0.0500 Epoch 127/400 3/3 [==============================] - 0s 48ms/step - loss: 0.1489 - accuracy: 0.0000e+00 - val_loss: 0.1205 - val_accuracy: 0.0500 Epoch 128/400 3/3 [==============================] - 0s 42ms/step - loss: 0.1470 - accuracy: 0.0000e+00 - val_loss: 0.1186 - val_accuracy: 0.0500 Epoch 129/400 3/3 [==============================] - 0s 18ms/step - loss: 0.1452 - accuracy: 0.0000e+00 - val_loss: 0.1165 - val_accuracy: 0.0500 Epoch 130/400 3/3 [==============================] - 0s 14ms/step - loss: 0.1434 - accuracy: 0.0000e+00 - val_loss: 0.1144 - val_accuracy: 0.0500 Epoch 131/400 3/3 [==============================] - 0s 16ms/step - loss: 0.1416 - accuracy: 0.0000e+00 - val_loss: 0.1124 - val_accuracy: 0.0500 Epoch 132/400 3/3 [==============================] - 0s 14ms/step - loss: 0.1396 - accuracy: 0.0000e+00 - val_loss: 0.1106 - val_accuracy: 0.0500 Epoch 133/400 3/3 [==============================] - 0s 14ms/step - loss: 0.1378 - accuracy: 0.0000e+00 - val_loss: 0.1085 - val_accuracy: 0.0500 Epoch 134/400 3/3 [==============================] - 0s 15ms/step - loss: 0.1358 - accuracy: 0.0000e+00 - val_loss: 0.1066 - val_accuracy: 0.0500 Epoch 135/400 3/3 [==============================] - 0s 15ms/step - loss: 0.1339 - accuracy: 0.0000e+00 - val_loss: 0.1044 - val_accuracy: 0.0500 Epoch 136/400 3/3 [==============================] - 0s 15ms/step - loss: 0.1319 - accuracy: 0.0000e+00 - val_loss: 0.1023 - val_accuracy: 0.0500 Epoch 137/400 3/3 [==============================] - 0s 17ms/step - loss: 0.1300 - accuracy: 0.0000e+00 - val_loss: 0.1002 - val_accuracy: 0.0500 Epoch 138/400 3/3 [==============================] - 0s 35ms/step - loss: 0.1279 - accuracy: 0.0000e+00 - val_loss: 0.0982 - val_accuracy: 0.0500 Epoch 139/400 3/3 [==============================] - 0s 52ms/step - loss: 0.1259 - accuracy: 0.0000e+00 - val_loss: 0.0961 - val_accuracy: 0.0500 Epoch 140/400 3/3 [==============================] - 0s 46ms/step - loss: 0.1238 - accuracy: 0.0000e+00 - val_loss: 0.0941 - val_accuracy: 0.0500 Epoch 141/400 3/3 [==============================] - 0s 34ms/step - loss: 0.1217 - accuracy: 0.0000e+00 - val_loss: 0.0921 - val_accuracy: 0.0500 Epoch 142/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1196 - accuracy: 0.0000e+00 - val_loss: 0.0899 - val_accuracy: 0.0500 Epoch 143/400 3/3 [==============================] - 0s 25ms/step - loss: 0.1174 - accuracy: 0.0000e+00 - val_loss: 0.0875 - val_accuracy: 0.0500 Epoch 144/400 3/3 [==============================] - 0s 25ms/step - loss: 0.1153 - accuracy: 0.0000e+00 - val_loss: 0.0851 - val_accuracy: 0.0500 Epoch 145/400 3/3 [==============================] - 0s 31ms/step - loss: 0.1130 - accuracy: 0.0000e+00 - val_loss: 0.0829 - val_accuracy: 0.0500 Epoch 146/400 3/3 [==============================] - 0s 50ms/step - loss: 0.1108 - accuracy: 0.0000e+00 - val_loss: 0.0808 - val_accuracy: 0.0500 Epoch 147/400 3/3 [==============================] - 0s 51ms/step - loss: 0.1085 - accuracy: 0.0000e+00 - val_loss: 0.0786 - val_accuracy: 0.0500 Epoch 148/400 3/3 [==============================] - 0s 46ms/step - loss: 0.1062 - accuracy: 0.0000e+00 - val_loss: 0.0763 - val_accuracy: 0.0500 Epoch 149/400 3/3 [==============================] - 0s 28ms/step - loss: 0.1038 - accuracy: 0.0000e+00 - val_loss: 0.0739 - val_accuracy: 0.0500 Epoch 150/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1014 - accuracy: 0.0000e+00 - val_loss: 0.0712 - val_accuracy: 0.0500 Epoch 151/400 3/3 [==============================] - 0s 51ms/step - loss: 0.0990 - accuracy: 0.0000e+00 - val_loss: 0.0686 - val_accuracy: 0.0500 Epoch 152/400 3/3 [==============================] - 0s 67ms/step - loss: 0.0966 - accuracy: 0.0000e+00 - val_loss: 0.0660 - val_accuracy: 0.0500 Epoch 153/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0941 - accuracy: 0.0000e+00 - val_loss: 0.0635 - val_accuracy: 0.0500 Epoch 154/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0916 - accuracy: 0.0000e+00 - val_loss: 0.0610 - val_accuracy: 0.0500 Epoch 155/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0892 - accuracy: 0.0000e+00 - val_loss: 0.0595 - val_accuracy: 0.0500 Epoch 156/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0870 - accuracy: 0.0000e+00 - val_loss: 0.0585 - val_accuracy: 0.0500 Epoch 157/400 3/3 [==============================] - 0s 51ms/step - loss: 0.0849 - accuracy: 0.0000e+00 - val_loss: 0.0578 - val_accuracy: 0.0500 Epoch 158/400 3/3 [==============================] - 0s 62ms/step - loss: 0.0829 - accuracy: 0.0000e+00 - val_loss: 0.0570 - val_accuracy: 0.0500 Epoch 159/400 3/3 [==============================] - 0s 56ms/step - loss: 0.0814 - accuracy: 0.0000e+00 - val_loss: 0.0563 - val_accuracy: 0.0500 Epoch 160/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0799 - accuracy: 0.0000e+00 - val_loss: 0.0556 - val_accuracy: 0.0500 Epoch 161/400 3/3 [==============================] - 0s 52ms/step - loss: 0.0787 - accuracy: 0.0000e+00 - val_loss: 0.0553 - val_accuracy: 0.0500 Epoch 162/400 3/3 [==============================] - 0s 65ms/step - loss: 0.0774 - accuracy: 0.0000e+00 - val_loss: 0.0551 - val_accuracy: 0.0500 Epoch 163/400 3/3 [==============================] - 0s 58ms/step - loss: 0.0762 - accuracy: 0.0000e+00 - val_loss: 0.0550 - val_accuracy: 0.0500 Epoch 164/400 3/3 [==============================] - 0s 60ms/step - loss: 0.0752 - accuracy: 0.0000e+00 - val_loss: 0.0550 - val_accuracy: 0.0500 Epoch 165/400 3/3 [==============================] - 0s 37ms/step - loss: 0.0742 - accuracy: 0.0000e+00 - val_loss: 0.0552 - val_accuracy: 0.0500 Epoch 166/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0733 - accuracy: 0.0000e+00 - val_loss: 0.0554 - val_accuracy: 0.0500 Epoch 167/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0723 - accuracy: 0.0000e+00 - val_loss: 0.0555 - val_accuracy: 0.0500 Epoch 168/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0715 - accuracy: 0.0000e+00 - val_loss: 0.0557 - val_accuracy: 0.0500 Epoch 169/400 3/3 [==============================] - 0s 34ms/step - loss: 0.0708 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 170/400 3/3 [==============================] - 0s 31ms/step - loss: 0.0701 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 171/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0694 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 172/400 3/3 [==============================] - 0s 39ms/step - loss: 0.0688 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 173/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0682 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 174/400 3/3 [==============================] - 0s 45ms/step - loss: 0.0676 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 175/400 3/3 [==============================] - 0s 39ms/step - loss: 0.0671 - accuracy: 0.0000e+00 - val_loss: 0.0557 - val_accuracy: 0.0500 Epoch 176/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0666 - accuracy: 0.0000e+00 - val_loss: 0.0557 - val_accuracy: 0.0500 Epoch 177/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0661 - accuracy: 0.0000e+00 - val_loss: 0.0556 - val_accuracy: 0.0500 Epoch 178/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0657 - accuracy: 0.0000e+00 - val_loss: 0.0556 - val_accuracy: 0.0500 Epoch 179/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0652 - accuracy: 0.0000e+00 - val_loss: 0.0555 - val_accuracy: 0.0500 Epoch 180/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0648 - accuracy: 0.0000e+00 - val_loss: 0.0554 - val_accuracy: 0.0500 Epoch 181/400 3/3 [==============================] - 0s 23ms/step - loss: 0.0644 - accuracy: 0.0000e+00 - val_loss: 0.0554 - val_accuracy: 0.0500 Epoch 182/400 3/3 [==============================] - 0s 21ms/step - loss: 0.0640 - accuracy: 0.0000e+00 - val_loss: 0.0553 - val_accuracy: 0.0500 Epoch 183/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0637 - accuracy: 0.0000e+00 - val_loss: 0.0551 - val_accuracy: 0.0500 Epoch 184/400 3/3 [==============================] - 0s 51ms/step - loss: 0.0633 - accuracy: 0.0000e+00 - val_loss: 0.0549 - val_accuracy: 0.0500 Epoch 185/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0629 - accuracy: 0.0000e+00 - val_loss: 0.0547 - val_accuracy: 0.0500 Epoch 186/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0625 - accuracy: 0.0000e+00 - val_loss: 0.0545 - val_accuracy: 0.0500 Epoch 187/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0622 - accuracy: 0.0000e+00 - val_loss: 0.0543 - val_accuracy: 0.0500 Epoch 188/400 3/3 [==============================] - 0s 41ms/step - loss: 0.0618 - accuracy: 0.0000e+00 - val_loss: 0.0543 - val_accuracy: 0.0500 Epoch 189/400 3/3 [==============================] - 0s 41ms/step - loss: 0.0615 - accuracy: 0.0000e+00 - val_loss: 0.0542 - val_accuracy: 0.0500 Epoch 190/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0611 - accuracy: 0.0000e+00 - val_loss: 0.0542 - val_accuracy: 0.0500 Epoch 191/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0609 - accuracy: 0.0000e+00 - val_loss: 0.0541 - val_accuracy: 0.0500 Epoch 192/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0605 - accuracy: 0.0000e+00 - val_loss: 0.0538 - val_accuracy: 0.0500 Epoch 193/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0601 - accuracy: 0.0000e+00 - val_loss: 0.0536 - val_accuracy: 0.0500 Epoch 194/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0598 - accuracy: 0.0000e+00 - val_loss: 0.0536 - val_accuracy: 0.0500 Epoch 195/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0595 - accuracy: 0.0000e+00 - val_loss: 0.0535 - val_accuracy: 0.0500 Epoch 196/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0592 - accuracy: 0.0000e+00 - val_loss: 0.0534 - val_accuracy: 0.0500 Epoch 197/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0589 - accuracy: 0.0000e+00 - val_loss: 0.0533 - val_accuracy: 0.0500 Epoch 198/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0586 - accuracy: 0.0000e+00 - val_loss: 0.0532 - val_accuracy: 0.0500 Epoch 199/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0583 - accuracy: 0.0000e+00 - val_loss: 0.0531 - val_accuracy: 0.0500 Epoch 200/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0581 - accuracy: 0.0000e+00 - val_loss: 0.0530 - val_accuracy: 0.0500 Epoch 201/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0579 - accuracy: 0.0000e+00 - val_loss: 0.0528 - val_accuracy: 0.0500 Epoch 202/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0575 - accuracy: 0.0000e+00 - val_loss: 0.0526 - val_accuracy: 0.0500 Epoch 203/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0572 - accuracy: 0.0000e+00 - val_loss: 0.0525 - val_accuracy: 0.0500 Epoch 204/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0570 - accuracy: 0.0000e+00 - val_loss: 0.0525 - val_accuracy: 0.0500 Epoch 205/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0567 - accuracy: 0.0000e+00 - val_loss: 0.0524 - val_accuracy: 0.0500 Epoch 206/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0565 - accuracy: 0.0000e+00 - val_loss: 0.0523 - val_accuracy: 0.0500 Epoch 207/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0562 - accuracy: 0.0000e+00 - val_loss: 0.0522 - val_accuracy: 0.0500 Epoch 208/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0561 - accuracy: 0.0000e+00 - val_loss: 0.0522 - val_accuracy: 0.0500 Epoch 209/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0558 - accuracy: 0.0000e+00 - val_loss: 0.0520 - val_accuracy: 0.0500 Epoch 210/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0555 - accuracy: 0.0000e+00 - val_loss: 0.0518 - val_accuracy: 0.0500 Epoch 211/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0553 - accuracy: 0.0000e+00 - val_loss: 0.0517 - val_accuracy: 0.0500 Epoch 212/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0551 - accuracy: 0.0000e+00 - val_loss: 0.0514 - val_accuracy: 0.0500 Epoch 213/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0549 - accuracy: 0.0000e+00 - val_loss: 0.0512 - val_accuracy: 0.0500 Epoch 214/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0547 - accuracy: 0.0000e+00 - val_loss: 0.0509 - val_accuracy: 0.0500 Epoch 215/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0545 - accuracy: 0.0000e+00 - val_loss: 0.0507 - val_accuracy: 0.0500 Epoch 216/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0541 - accuracy: 0.0000e+00 - val_loss: 0.0506 - val_accuracy: 0.0500 Epoch 217/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0540 - accuracy: 0.0000e+00 - val_loss: 0.0505 - val_accuracy: 0.0500 Epoch 218/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0538 - accuracy: 0.0000e+00 - val_loss: 0.0503 - val_accuracy: 0.0500 Epoch 219/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0536 - accuracy: 0.0000e+00 - val_loss: 0.0502 - val_accuracy: 0.0500 Epoch 220/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0533 - accuracy: 0.0000e+00 - val_loss: 0.0500 - val_accuracy: 0.0500 Epoch 221/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0531 - accuracy: 0.0000e+00 - val_loss: 0.0498 - val_accuracy: 0.0500 Epoch 222/400 3/3 [==============================] - 0s 22ms/step - loss: 0.0530 - accuracy: 0.0000e+00 - val_loss: 0.0496 - val_accuracy: 0.0500 Epoch 223/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0527 - accuracy: 0.0000e+00 - val_loss: 0.0495 - val_accuracy: 0.0500 Epoch 224/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0526 - accuracy: 0.0000e+00 - val_loss: 0.0494 - val_accuracy: 0.0500 Epoch 225/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0523 - accuracy: 0.0000e+00 - val_loss: 0.0494 - val_accuracy: 0.0500 Epoch 226/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0521 - accuracy: 0.0000e+00 - val_loss: 0.0494 - val_accuracy: 0.0500 Epoch 227/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0519 - accuracy: 0.0000e+00 - val_loss: 0.0493 - val_accuracy: 0.0500 Epoch 228/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0518 - accuracy: 0.0000e+00 - val_loss: 0.0492 - val_accuracy: 0.0500 Epoch 229/400 3/3 [==============================] - 0s 21ms/step - loss: 0.0516 - accuracy: 0.0000e+00 - val_loss: 0.0490 - val_accuracy: 0.0500 Epoch 230/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0514 - accuracy: 0.0000e+00 - val_loss: 0.0488 - val_accuracy: 0.0500 Epoch 231/400 3/3 [==============================] - 0s 37ms/step - loss: 0.0512 - accuracy: 0.0000e+00 - val_loss: 0.0486 - val_accuracy: 0.0500 Epoch 232/400 3/3 [==============================] - 0s 36ms/step - loss: 0.0510 - accuracy: 0.0000e+00 - val_loss: 0.0484 - val_accuracy: 0.0500 Epoch 233/400 3/3 [==============================] - 0s 19ms/step - loss: 0.0508 - accuracy: 0.0000e+00 - val_loss: 0.0483 - val_accuracy: 0.0500 Epoch 234/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0507 - accuracy: 0.0000e+00 - val_loss: 0.0481 - val_accuracy: 0.0500 Epoch 235/400 3/3 [==============================] - 0s 41ms/step - loss: 0.0505 - accuracy: 0.0000e+00 - val_loss: 0.0480 - val_accuracy: 0.0500 Epoch 236/400 3/3 [==============================] - 0s 43ms/step - loss: 0.0504 - accuracy: 0.0000e+00 - val_loss: 0.0480 - val_accuracy: 0.0500 Epoch 237/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0502 - accuracy: 0.0000e+00 - val_loss: 0.0478 - val_accuracy: 0.0500 Epoch 238/400 3/3 [==============================] - 0s 36ms/step - loss: 0.0499 - accuracy: 0.0000e+00 - val_loss: 0.0477 - val_accuracy: 0.0500 Epoch 239/400 3/3 [==============================] - 0s 36ms/step - loss: 0.0498 - accuracy: 0.0000e+00 - val_loss: 0.0477 - val_accuracy: 0.0500 Epoch 240/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0496 - accuracy: 0.0000e+00 - val_loss: 0.0475 - val_accuracy: 0.0500 Epoch 241/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0494 - accuracy: 0.0000e+00 - val_loss: 0.0474 - val_accuracy: 0.0500 Epoch 242/400 3/3 [==============================] - 0s 22ms/step - loss: 0.0493 - accuracy: 0.0000e+00 - val_loss: 0.0473 - val_accuracy: 0.0500 Epoch 243/400 3/3 [==============================] - 0s 21ms/step - loss: 0.0492 - accuracy: 0.0000e+00 - val_loss: 0.0472 - val_accuracy: 0.0500 Epoch 244/400 3/3 [==============================] - 0s 23ms/step - loss: 0.0489 - accuracy: 0.0000e+00 - val_loss: 0.0472 - val_accuracy: 0.0500 Epoch 245/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0488 - accuracy: 0.0000e+00 - val_loss: 0.0472 - val_accuracy: 0.0500 Epoch 246/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0487 - accuracy: 0.0000e+00 - val_loss: 0.0473 - val_accuracy: 0.0500 Epoch 247/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0485 - accuracy: 0.0000e+00 - val_loss: 0.0472 - val_accuracy: 0.0500 Epoch 248/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0483 - accuracy: 0.0000e+00 - val_loss: 0.0471 - val_accuracy: 0.0500 Epoch 249/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0482 - accuracy: 0.0000e+00 - val_loss: 0.0470 - val_accuracy: 0.0500 Epoch 250/400 3/3 [==============================] - 0s 13ms/step - loss: 0.0481 - accuracy: 0.0000e+00 - val_loss: 0.0470 - val_accuracy: 0.0500 Epoch 251/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0479 - accuracy: 0.0000e+00 - val_loss: 0.0469 - val_accuracy: 0.0500 Epoch 252/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0478 - accuracy: 0.0000e+00 - val_loss: 0.0468 - val_accuracy: 0.0500 Epoch 253/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0477 - accuracy: 0.0000e+00 - val_loss: 0.0467 - val_accuracy: 0.0500 Epoch 254/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0475 - accuracy: 0.0000e+00 - val_loss: 0.0465 - val_accuracy: 0.0500 Epoch 255/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0474 - accuracy: 0.0000e+00 - val_loss: 0.0463 - val_accuracy: 0.0500 Epoch 256/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0472 - accuracy: 0.0000e+00 - val_loss: 0.0462 - val_accuracy: 0.0500 Epoch 257/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0471 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 258/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0470 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 259/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0468 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 260/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0467 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 261/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0466 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 262/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0466 - accuracy: 0.0000e+00 - val_loss: 0.0459 - val_accuracy: 0.0500 Epoch 263/400 3/3 [==============================] - 0s 22ms/step - loss: 0.0465 - accuracy: 0.0000e+00 - val_loss: 0.0457 - val_accuracy: 0.0500 Epoch 264/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0463 - accuracy: 0.0000e+00 - val_loss: 0.0455 - val_accuracy: 0.0500 Epoch 265/400 3/3 [==============================] - 0s 39ms/step - loss: 0.0462 - accuracy: 0.0000e+00 - val_loss: 0.0453 - val_accuracy: 0.0500 Epoch 266/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0461 - accuracy: 0.0000e+00 - val_loss: 0.0451 - val_accuracy: 0.0500 Epoch 267/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0460 - accuracy: 0.0000e+00 - val_loss: 0.0449 - val_accuracy: 0.0500 Epoch 268/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0458 - accuracy: 0.0000e+00 - val_loss: 0.0449 - val_accuracy: 0.0500 Epoch 269/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0457 - accuracy: 0.0000e+00 - val_loss: 0.0449 - val_accuracy: 0.0500 Epoch 270/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0455 - accuracy: 0.0000e+00 - val_loss: 0.0447 - val_accuracy: 0.0500 Epoch 271/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0454 - accuracy: 0.0000e+00 - val_loss: 0.0445 - val_accuracy: 0.0500 Epoch 272/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0454 - accuracy: 0.0000e+00 - val_loss: 0.0444 - val_accuracy: 0.0500 Epoch 273/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0453 - accuracy: 0.0000e+00 - val_loss: 0.0443 - val_accuracy: 0.0500 Epoch 274/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0452 - accuracy: 0.0000e+00 - val_loss: 0.0442 - val_accuracy: 0.0500 Epoch 275/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0450 - accuracy: 0.0000e+00 - val_loss: 0.0441 - val_accuracy: 0.0500 Epoch 276/400 3/3 [==============================] - 0s 19ms/step - loss: 0.0449 - accuracy: 0.0000e+00 - val_loss: 0.0440 - val_accuracy: 0.0500 Epoch 277/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0449 - accuracy: 0.0000e+00 - val_loss: 0.0439 - val_accuracy: 0.0500 Epoch 278/400 3/3 [==============================] - 0s 21ms/step - loss: 0.0447 - accuracy: 0.0000e+00 - val_loss: 0.0438 - val_accuracy: 0.0500 Epoch 279/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0446 - accuracy: 0.0000e+00 - val_loss: 0.0437 - val_accuracy: 0.0500 Epoch 280/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0445 - accuracy: 0.0000e+00 - val_loss: 0.0436 - val_accuracy: 0.0500 Epoch 281/400 3/3 [==============================] - 0s 19ms/step - loss: 0.0444 - accuracy: 0.0000e+00 - val_loss: 0.0436 - val_accuracy: 0.0500 Epoch 282/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0443 - accuracy: 0.0000e+00 - val_loss: 0.0436 - val_accuracy: 0.0500 Epoch 283/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0442 - accuracy: 0.0000e+00 - val_loss: 0.0435 - val_accuracy: 0.0500 Epoch 284/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0441 - accuracy: 0.0000e+00 - val_loss: 0.0434 - val_accuracy: 0.0500 Epoch 285/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0440 - accuracy: 0.0000e+00 - val_loss: 0.0434 - val_accuracy: 0.0500 Epoch 286/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0440 - accuracy: 0.0000e+00 - val_loss: 0.0433 - val_accuracy: 0.0500 Epoch 287/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0438 - accuracy: 0.0000e+00 - val_loss: 0.0433 - val_accuracy: 0.0500 Epoch 288/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0438 - accuracy: 0.0000e+00 - val_loss: 0.0432 - val_accuracy: 0.0500 Epoch 289/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0436 - accuracy: 0.0000e+00 - val_loss: 0.0431 - val_accuracy: 0.0500 Epoch 290/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0435 - accuracy: 0.0000e+00 - val_loss: 0.0430 - val_accuracy: 0.0500 Epoch 291/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0434 - accuracy: 0.0000e+00 - val_loss: 0.0429 - val_accuracy: 0.0500 Epoch 292/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0433 - accuracy: 0.0000e+00 - val_loss: 0.0429 - val_accuracy: 0.0500 Epoch 293/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0433 - accuracy: 0.0000e+00 - val_loss: 0.0429 - val_accuracy: 0.0500 Epoch 294/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0432 - accuracy: 0.0000e+00 - val_loss: 0.0429 - val_accuracy: 0.0500 Epoch 295/400 3/3 [==============================] - 0s 19ms/step - loss: 0.0431 - accuracy: 0.0000e+00 - val_loss: 0.0426 - val_accuracy: 0.0500 Epoch 296/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0430 - accuracy: 0.0000e+00 - val_loss: 0.0424 - val_accuracy: 0.0500 Epoch 297/400 3/3 [==============================] - 0s 23ms/step - loss: 0.0429 - accuracy: 0.0000e+00 - val_loss: 0.0424 - val_accuracy: 0.0500 Epoch 298/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0428 - accuracy: 0.0000e+00 - val_loss: 0.0423 - val_accuracy: 0.0500 Epoch 299/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0427 - accuracy: 0.0000e+00 - val_loss: 0.0423 - val_accuracy: 0.0500 Epoch 300/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0426 - accuracy: 0.0000e+00 - val_loss: 0.0422 - val_accuracy: 0.0500 Epoch 301/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0425 - accuracy: 0.0000e+00 - val_loss: 0.0421 - val_accuracy: 0.0500 Epoch 302/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0424 - accuracy: 0.0000e+00 - val_loss: 0.0422 - val_accuracy: 0.0500 Epoch 303/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0424 - accuracy: 0.0000e+00 - val_loss: 0.0423 - val_accuracy: 0.0500 Epoch 304/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0423 - accuracy: 0.0000e+00 - val_loss: 0.0422 - val_accuracy: 0.0500 Epoch 305/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0422 - accuracy: 0.0000e+00 - val_loss: 0.0421 - val_accuracy: 0.0500 Epoch 306/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0422 - accuracy: 0.0000e+00 - val_loss: 0.0418 - val_accuracy: 0.0500 Epoch 307/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0421 - accuracy: 0.0000e+00 - val_loss: 0.0417 - val_accuracy: 0.0500 Epoch 308/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0419 - accuracy: 0.0000e+00 - val_loss: 0.0417 - val_accuracy: 0.0500 Epoch 309/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0419 - accuracy: 0.0000e+00 - val_loss: 0.0417 - val_accuracy: 0.0500 Epoch 310/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0418 - accuracy: 0.0000e+00 - val_loss: 0.0416 - val_accuracy: 0.0500 Epoch 311/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0417 - accuracy: 0.0000e+00 - val_loss: 0.0414 - val_accuracy: 0.0500 Epoch 312/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0417 - accuracy: 0.0000e+00 - val_loss: 0.0412 - val_accuracy: 0.0500 Epoch 313/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0416 - accuracy: 0.0000e+00 - val_loss: 0.0412 - val_accuracy: 0.0500 Epoch 314/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0416 - accuracy: 0.0000e+00 - val_loss: 0.0412 - val_accuracy: 0.0500 Epoch 315/400 3/3 [==============================] - 0s 22ms/step - loss: 0.0414 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 316/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0413 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 317/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0413 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 318/400 3/3 [==============================] - 0s 92ms/step - loss: 0.0411 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 319/400 3/3 [==============================] - 0s 31ms/step - loss: 0.0411 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 320/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0410 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 321/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0410 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 322/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0409 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 323/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0408 - accuracy: 0.0000e+00 - val_loss: 0.0406 - val_accuracy: 0.0500 Epoch 324/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0407 - accuracy: 0.0000e+00 - val_loss: 0.0405 - val_accuracy: 0.0500 Epoch 325/400 3/3 [==============================] - 0s 23ms/step - loss: 0.0407 - accuracy: 0.0000e+00 - val_loss: 0.0404 - val_accuracy: 0.0500 Epoch 326/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0406 - accuracy: 0.0000e+00 - val_loss: 0.0404 - val_accuracy: 0.0500 Epoch 327/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0405 - accuracy: 0.0000e+00 - val_loss: 0.0404 - val_accuracy: 0.0500 Epoch 328/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0406 - accuracy: 0.0000e+00 - val_loss: 0.0404 - val_accuracy: 0.0500 Epoch 329/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0404 - accuracy: 0.0000e+00 - val_loss: 0.0402 - val_accuracy: 0.0500 Epoch 330/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0403 - accuracy: 0.0000e+00 - val_loss: 0.0401 - val_accuracy: 0.0500 Epoch 331/400 3/3 [==============================] - 0s 31ms/step - loss: 0.0402 - accuracy: 0.0000e+00 - val_loss: 0.0402 - val_accuracy: 0.0500 Epoch 332/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0402 - accuracy: 0.0000e+00 - val_loss: 0.0402 - val_accuracy: 0.0500 Epoch 333/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0401 - accuracy: 0.0000e+00 - val_loss: 0.0405 - val_accuracy: 0.0500 Epoch 334/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0401 - accuracy: 0.0000e+00 - val_loss: 0.0408 - val_accuracy: 0.0500 Epoch 335/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0401 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 336/400 3/3 [==============================] - 0s 37ms/step - loss: 0.0400 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 337/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0399 - accuracy: 0.0000e+00 - val_loss: 0.0405 - val_accuracy: 0.0500 Epoch 338/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0398 - accuracy: 0.0000e+00 - val_loss: 0.0401 - val_accuracy: 0.0500 Epoch 339/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0397 - accuracy: 0.0000e+00 - val_loss: 0.0400 - val_accuracy: 0.0500 Epoch 340/400 3/3 [==============================] - 0s 40ms/step - loss: 0.0396 - accuracy: 0.0000e+00 - val_loss: 0.0400 - val_accuracy: 0.0500 Epoch 341/400 3/3 [==============================] - 0s 53ms/step - loss: 0.0395 - accuracy: 0.0000e+00 - val_loss: 0.0399 - val_accuracy: 0.0500 Epoch 342/400 3/3 [==============================] - 0s 54ms/step - loss: 0.0395 - accuracy: 0.0000e+00 - val_loss: 0.0399 - val_accuracy: 0.0500 Epoch 343/400 3/3 [==============================] - 0s 45ms/step - loss: 0.0394 - accuracy: 0.0000e+00 - val_loss: 0.0398 - val_accuracy: 0.0500 Epoch 344/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0393 - accuracy: 0.0000e+00 - val_loss: 0.0398 - val_accuracy: 0.0500 Epoch 345/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0393 - accuracy: 0.0000e+00 - val_loss: 0.0397 - val_accuracy: 0.0500 Epoch 346/400 3/3 [==============================] - 0s 53ms/step - loss: 0.0392 - accuracy: 0.0000e+00 - val_loss: 0.0396 - val_accuracy: 0.0500 Epoch 347/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0391 - accuracy: 0.0000e+00 - val_loss: 0.0395 - val_accuracy: 0.0500 Epoch 348/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0391 - accuracy: 0.0000e+00 - val_loss: 0.0394 - val_accuracy: 0.0500 Epoch 349/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0390 - accuracy: 0.0000e+00 - val_loss: 0.0394 - val_accuracy: 0.0500 Epoch 350/400 3/3 [==============================] - 0s 34ms/step - loss: 0.0390 - accuracy: 0.0000e+00 - val_loss: 0.0394 - val_accuracy: 0.0500 Epoch 351/400 3/3 [==============================] - 0s 90ms/step - loss: 0.0389 - accuracy: 0.0000e+00 - val_loss: 0.0393 - val_accuracy: 0.0500 Epoch 352/400 3/3 [==============================] - 0s 41ms/step - loss: 0.0388 - accuracy: 0.0000e+00 - val_loss: 0.0392 - val_accuracy: 0.0500 Epoch 353/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0389 - accuracy: 0.0000e+00 - val_loss: 0.0392 - val_accuracy: 0.0500 Epoch 354/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0387 - accuracy: 0.0000e+00 - val_loss: 0.0391 - val_accuracy: 0.0500 Epoch 355/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0386 - accuracy: 0.0000e+00 - val_loss: 0.0390 - val_accuracy: 0.0500 Epoch 356/400 3/3 [==============================] - 0s 61ms/step - loss: 0.0386 - accuracy: 0.0000e+00 - val_loss: 0.0389 - val_accuracy: 0.0500 Epoch 357/400 3/3 [==============================] - 0s 58ms/step - loss: 0.0386 - accuracy: 0.0000e+00 - val_loss: 0.0390 - val_accuracy: 0.0500 Epoch 358/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0386 - accuracy: 0.0000e+00 - val_loss: 0.0389 - val_accuracy: 0.0500 Epoch 359/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0385 - accuracy: 0.0000e+00 - val_loss: 0.0388 - val_accuracy: 0.0500 Epoch 360/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0384 - accuracy: 0.0000e+00 - val_loss: 0.0387 - val_accuracy: 0.0500 Epoch 361/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0384 - accuracy: 0.0000e+00 - val_loss: 0.0388 - val_accuracy: 0.0500 Epoch 362/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0382 - accuracy: 0.0000e+00 - val_loss: 0.0389 - val_accuracy: 0.0500 Epoch 363/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0382 - accuracy: 0.0000e+00 - val_loss: 0.0391 - val_accuracy: 0.0500 Epoch 364/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0381 - accuracy: 0.0000e+00 - val_loss: 0.0391 - val_accuracy: 0.0500 Epoch 365/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0381 - accuracy: 0.0000e+00 - val_loss: 0.0390 - val_accuracy: 0.0500 Epoch 366/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0380 - accuracy: 0.0000e+00 - val_loss: 0.0390 - val_accuracy: 0.0500 Epoch 367/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0380 - accuracy: 0.0000e+00 - val_loss: 0.0388 - val_accuracy: 0.0500 Epoch 368/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0379 - accuracy: 0.0000e+00 - val_loss: 0.0386 - val_accuracy: 0.0500 Epoch 369/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0379 - accuracy: 0.0000e+00 - val_loss: 0.0386 - val_accuracy: 0.0500 Epoch 370/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0379 - accuracy: 0.0000e+00 - val_loss: 0.0384 - val_accuracy: 0.0500 Epoch 371/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0377 - accuracy: 0.0000e+00 - val_loss: 0.0384 - val_accuracy: 0.0500 Epoch 372/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0377 - accuracy: 0.0000e+00 - val_loss: 0.0383 - val_accuracy: 0.0500 Epoch 373/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0376 - accuracy: 0.0000e+00 - val_loss: 0.0382 - val_accuracy: 0.0500 Epoch 374/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0376 - accuracy: 0.0000e+00 - val_loss: 0.0381 - val_accuracy: 0.0500 Epoch 375/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0375 - accuracy: 0.0000e+00 - val_loss: 0.0380 - val_accuracy: 0.0500 Epoch 376/400 3/3 [==============================] - 0s 42ms/step - loss: 0.0375 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 377/400 3/3 [==============================] - 0s 34ms/step - loss: 0.0375 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 378/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0374 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 379/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0373 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 380/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0372 - accuracy: 0.0000e+00 - val_loss: 0.0381 - val_accuracy: 0.0500 Epoch 381/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0372 - accuracy: 0.0000e+00 - val_loss: 0.0383 - val_accuracy: 0.0500 Epoch 382/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0372 - accuracy: 0.0000e+00 - val_loss: 0.0382 - val_accuracy: 0.0500 Epoch 383/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0371 - accuracy: 0.0000e+00 - val_loss: 0.0381 - val_accuracy: 0.0500 Epoch 384/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0371 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 385/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0370 - accuracy: 0.0000e+00 - val_loss: 0.0377 - val_accuracy: 0.0500 Epoch 386/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0371 - accuracy: 0.0000e+00 - val_loss: 0.0376 - val_accuracy: 0.0500 Epoch 387/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0369 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 388/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0369 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 389/400 3/3 [==============================] - 0s 31ms/step - loss: 0.0368 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 390/400 3/3 [==============================] - 0s 47ms/step - loss: 0.0368 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 391/400 3/3 [==============================] - 0s 63ms/step - loss: 0.0367 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 392/400 3/3 [==============================] - 0s 57ms/step - loss: 0.0367 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 393/400 3/3 [==============================] - 0s 54ms/step - loss: 0.0366 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 394/400 3/3 [==============================] - 0s 54ms/step - loss: 0.0366 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 395/400 3/3 [==============================] - 0s 54ms/step - loss: 0.0365 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 396/400 3/3 [==============================] - 0s 49ms/step - loss: 0.0365 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 397/400 3/3 [==============================] - 0s 51ms/step - loss: 0.0364 - accuracy: 0.0000e+00 - val_loss: 0.0373 - val_accuracy: 0.0500 Epoch 398/400 3/3 [==============================] - 0s 52ms/step - loss: 0.0364 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 399/400 3/3 [==============================] - 0s 48ms/step - loss: 0.0364 - accuracy: 0.0000e+00 - val_loss: 0.0373 - val_accuracy: 0.0500 Epoch 400/400 3/3 [==============================] - 0s 56ms/step - loss: 0.0363 - accuracy: 0.0000e+00 - val_loss: 0.0371 - val_accuracy: 0.0500 . . Results and Visualization . results = model.predict(x_test) . plt.scatter(range(20), results, c=&#39;r&#39;) plt.scatter(range(20), y_test,c=&#39;g&#39;) plt.show() . plt.plot(history.history[&#39;loss&#39;]) plt.show() #### The loss nearly stagnates during the later stages - therefore the model would still produce a satisfactory output at epochs~300 .",
            "url": "https://kunal-bhar.github.io/blog/python/math/lstm/visualizations/2022/03/11/concurrent-num-prognosis.html",
            "relUrl": "/python/math/lstm/visualizations/2022/03/11/concurrent-num-prognosis.html",
            "date": " • Mar 11, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Shakespearean English Semantics",
            "content": "Project At A Glance . Objective: Evaluate and visualize relationships between annotations of the era&#39;s literature. . Data: Compiled Shakespeare Dataset [Download] . Implementation: Word2Vec, Word Embeddings, Principal Component Analysis . Results: . 100-parameter vectorized representation of every word in the corpus. | Computed similarility scores for distinct words of the time period relative to today&#39;s English language. | Scatter plots to visualize Word-Embeddings and their Principal Components. | . Deployment: View this project on GitHub. . Dependencies . import pandas as pd import nltk import string import matplotlib.pyplot as plt from nltk.corpus import stopwords from nltk import word_tokenize from gensim.models import Word2Vec as w2v from sklearn.decomposition import PCA . Initialization . PATH = &#39;ShakespeareDataset.txt&#39; sw = stopwords.words(&#39;english&#39;) plt.style.use(&#39;ggplot&#39;) . %%time lines = [] with open(PATH, &#39;r&#39;) as f: for l in f: lines.append(l) . Wall time: 70.5 ms . Text Pre-Processing . lines = [line.rstrip(&#39; n&#39;) for line in lines] lines = [line.lower() for line in lines] lines = [line.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation)) for line in lines] . %time lines = [word_tokenize(line) for line in lines] . Wall time: 15.3 s . def remove_stopwords(lines, sw = sw): res = [] for line in lines: original = line line = [w for w in line if w not in sw] if len(line) &lt; 1: line = original res.append(line) return res . %time filtered_lines = remove_stopwords(lines = lines, sw = sw) . Wall time: 2.03 s . Custom Word2Vec and Word-Similarity . %%time w = w2v( filtered_lines, min_count=3, sg = 1, window=7 ) . Wall time: 3.22 s . w.wv.most_similar(&#39;thou&#39;) . [(&#39;art&#39;, 0.8374333381652832), (&#39;thyself&#39;, 0.8162680864334106), (&#39;dost&#39;, 0.7874499559402466), (&#39;villain&#39;, 0.7856082320213318), (&#39;kill&#39;, 0.733100950717926), (&#39;hast&#39;, 0.7226855158805847), (&#39;wilt&#39;, 0.7046181559562683), (&#39;didst&#39;, 0.6970406770706177), (&#39;fellow&#39;, 0.696016788482666), (&#39;traitor&#39;, 0.6928953528404236)] . w.wv.most_similar(&#39;shall&#39;) . [(&#39;may&#39;, 0.8649993538856506), (&#39;could&#39;, 0.8336020708084106), (&#39;youll&#39;, 0.8054620623588562), (&#39;doth&#39;, 0.8045750856399536), (&#39;till&#39;, 0.7994358539581299), (&#39;business&#39;, 0.7948688864707947), (&#39;ill&#39;, 0.7912845611572266), (&#39;dare&#39;, 0.7817249298095703), (&#39;let&#39;, 0.7762875556945801), (&#39;might&#39;, 0.776180624961853)] . w.wv.most_similar(&#39;abhor&#39;) . [(&#39;revenue&#39;, 0.9962968826293945), (&#39;exercise&#39;, 0.9959706664085388), (&#39;wedlock&#39;, 0.9957810640335083), (&#39;fever&#39;, 0.995772659778595), (&#39;painting&#39;, 0.9955847263336182), (&#39;arthurs&#39;, 0.9955565929412842), (&#39;touched&#39;, 0.9955466985702515), (&#39;purgation&#39;, 0.9953007698059082), (&#39;devotion&#39;, 0.9951791763305664), (&#39;havior&#39;, 0.9951609373092651)] . w.wv.most_similar(&#39;vile&#39;) . [(&#39;form&#39;, 0.9728872776031494), (&#39;monstrous&#39;, 0.9698807597160339), (&#39;smiling&#39;, 0.9687707424163818), (&#39;merit&#39;, 0.9668200016021729), (&#39;savage&#39;, 0.9668026566505432), (&#39;blown&#39;, 0.9667961597442627), (&#39;tremble&#39;, 0.9661442041397095), (&#39;worm&#39;, 0.9657843112945557), (&#39;quite&#39;, 0.9650156497955322), (&#39;giddy&#39;, 0.9649045467376709)] . Generate Embedding DataFrame . %%time emb_df = ( pd.DataFrame( [w.wv.get_vector(str(n)) for n in w.wv.key_to_index], index = w.wv.key_to_index ) ) . Wall time: 797 ms . emb_df.shape . (11628, 100) . emb_df.head() . 0 1 2 3 4 5 6 7 8 9 ... 90 91 92 93 94 95 96 97 98 99 . thou 0.048444 | -0.179371 | 0.656354 | 0.553357 | -0.390886 | -0.399313 | 0.408025 | 0.459145 | -0.405728 | -0.201803 | ... | 0.168611 | 0.248811 | 0.342443 | -0.067844 | 1.087360 | 0.751683 | -0.544332 | -0.557544 | 0.064042 | 0.441012 | . thy 0.025636 | 0.257252 | 0.512183 | 0.255503 | 0.298671 | -0.194974 | 0.258179 | 0.727472 | -0.141695 | -0.345477 | ... | 0.183475 | -0.138357 | 0.696960 | -0.215607 | 0.402519 | 0.359478 | 0.072248 | -0.589917 | 0.021417 | 0.011141 | . shall -0.059704 | 0.162047 | -0.045949 | -0.057693 | 0.579027 | -0.023031 | 0.043291 | 0.445429 | -0.326463 | 0.111084 | ... | 0.486898 | 0.051414 | 0.056035 | -0.147839 | 0.489713 | 0.178388 | 0.121662 | -0.007791 | 0.305106 | 0.014491 | . thee -0.437681 | 0.170327 | 0.516488 | 0.250964 | 0.051163 | -0.085569 | 0.174727 | 0.692341 | -0.266182 | -0.164628 | ... | 0.455216 | 0.022709 | 0.256135 | -0.054368 | 0.635989 | 0.482457 | -0.008304 | -0.079286 | 0.276732 | 0.008572 | . good 0.068176 | 0.297002 | 0.267275 | -0.144337 | 0.127091 | 0.131793 | 0.484557 | 0.709339 | -0.335214 | 0.117829 | ... | 0.368580 | -0.192079 | 0.248281 | -0.377771 | 0.294786 | -0.042967 | 0.284816 | -0.195110 | 0.345721 | -0.013074 | . 5 rows × 100 columns . Word-Embedding Visualization . plt.clf() fig=plt.figure(figsize=(6,4)) plt.scatter( x = emb_df.iloc[:,0], y = emb_df.iloc[:,1], s = 0.2, color = &#39;maroon&#39;, alpha = 0.5 ) plt.title(&#39;Embedding Visualizations&#39;) plt.show() . &lt;Figure size 432x288 with 0 Axes&gt; . Principal Component Analysis . Processing . pca = PCA(n_components=2, random_state=7) pca_model = pca.fit_transform(emb_df) . emb_df_PCA = ( pd.DataFrame( pca_model, columns=[&#39;x&#39;,&#39;y&#39;], index = emb_df.index ) ) . Visualization . plt.clf() fig = plt.figure(figsize=(6,4)) plt.scatter( x = emb_df_PCA[&#39;x&#39;], y = emb_df_PCA[&#39;y&#39;], s = 0.4, color = &#39;maroon&#39;, alpha = 0.5 ) plt.xlabel(&#39;PCA-1&#39;) plt.ylabel(&#39;PCA-2&#39;) plt.title(&#39;PCA Visualization&#39;) plt.plot() plt.show() . &lt;Figure size 432x288 with 0 Axes&gt; .",
            "url": "https://kunal-bhar.github.io/blog/nlp/python/english/visualizations/2022/03/10/shakespeare-semantics.html",
            "relUrl": "/nlp/python/english/visualizations/2022/03/10/shakespeare-semantics.html",
            "date": " • Mar 10, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "probplotlib- Open Source Python Library",
            "content": "Project At A Glance . Objective: Higher-order mathematical operations using Python3. . Implementation: A lightwight library capable of parsing data directly as Probability Distributions - available as a Python installation through pip. . Results: . Concise calculations that eliminate double-precision. | Compatibility with external datasets (.txt). | Faster than NumPy operatives by ~13%. | . Deployment: View this project on the Python Package Index or GitHub. . Probability Distributions for Python . . The Statistical Void . Stats can get tricky in the transition from plotting fun graphs to advanced algebraic equations. A classic example is the given sum: . 1.0e14 + 1.0 - 1.0e14 . The actual result is 1.0 but in double precision, this will result in 0.0. While in this example the failure is quite obvious, it can get a lot trickier than that. Instances like these hinder the community from exploring the inferential potential of complex entities. . p=Gaussian(a,b) q=Gaussian(x,y) p+q . This snippet would be close to useless as python addition doesn’t isn’t attributed for higher-level declarables such as Gaussian variables. probplotlib provides simple solutions for probability distributions; posing a highly-optimized alternative to numpy and math, in a niche that is scarce in options. . Usage . probplotlib has the following operative methods: . + : uses Dunder Methods for facilitating dist-additions. . | calculate_mean(): returns the mean of a distribution. . | . gaussianex = Gaussian() calculate_mean(gaussianx) . calculate_stdev(): returns the standard deviation of a distribution. | . binomialex = Binomial() calculate_stdev(binomialex) . read_dataset(): reads an external .txt dataset directly as a distribution. | . gaussianex.read_dataset(&#39;values.txt&#39;) binomialex.read_dataset(&#39;values.txt&#39;) . params(): retrieves the identity parameters of an imported dataset. | . gaussianex.params() binomialex.params() . pdf(): returns the probability density function at a given point. | . pdf(gaussianex, 2) . functions unique to Gaussian Distributions: . plot_histogram(): uses matplotlib to display a histogram of the Gaussian Distribution. | . gaussianex.plot_histogram() . plot_histogram_pdf(): uses matplotlib to display a co-relative plot along with the Gaussian probability density function. | . gaussianex.plot_histogram_pdf() . functions unique to Binomial Distributions: . plot_bar(): uses matplotlib to display a bar graph of the Binomial Distribution. | . binomialex.plot_bar() . plot_bar_pdf(): uses matplotlib to display a co-relative plot along with the Binomial probability density function. | . binomialex.plot_bar_pdf() . Data Visualization . probplotlib therefore allows you to analyze raw numerical data graphically in minimial lines of code. The example below makes for better understanding. . . a bag of numbers in a .txt file corresponds to the following plots: . histogram plot: . . bar plot: . . histogram plot with pdf: . . References . Stanford Archives: CS109- The Normal(Gaussian) Distribution . A Practical Overview on Probability Distributions: Andrea Viti, Alberto Terzi, Luca Bertolaccini . Awesome Scientific Computing: Nico Schlömer, GitHub Repository . math.statistics: Python 3.10 Source Code . Stack Overflow . Dependencies . probplotlib depends on the matplotlib library on top of your regular python installation. . pip install matplotlib . or . conda install matplotlib . Installation . probplotlib is available on the Python Package Index. You can install it directly using pip. . pip install probplotlib . Testing . To run the tests, simply check to this directory and run the code below. . python -m unittest test_probplotlib .",
            "url": "https://kunal-bhar.github.io/blog/python/math/probability/visualizations/2021/10/28/probplotlib.html",
            "relUrl": "/python/math/probability/visualizations/2021/10/28/probplotlib.html",
            "date": " • Oct 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Here’s an archive with some of my work in Python, Data Science and Deep Learning. Feel free to look around! .",
          "url": "https://kunal-bhar.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kunal-bhar.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}