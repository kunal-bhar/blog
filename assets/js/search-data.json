{
  
    
        "post0": {
            "title": "Hourly Temperature Forecasting",
            "content": "Project At A Glance . Objective: Iteratively forecast hour-wise temperature for a region over a fairly large time-period (8 years in this case). . Data: Time-Series Weather Dataset at the Max Planck Institute in Jena, Germany. [Download] . Implementation: Time-Series Forecasting, Seqeuntial Long Short-Term Memory (LSTM) . Results: . Clear trends in data showing changes in the climate across the time of the year. | DataFrame with variance between Actual Values and Predicted Values for the test and validation sets. | Visualizations to judge model&#39;s performance. | . Deployment: View this project on GitHub. . Dependencies . import os import numpy as np import pandas as pd import tensorflow as tf . Dataset Initialization . Loading . zip_path = tf.keras.utils.get_file( origin=&#39;https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip&#39;, fname=&#39;jena_climate_2009_2016.csv.zip&#39;, extract=True) csv_path, _ = os.path.splitext(zip_path) . df = pd.read_csv(csv_path) . df . Date Time p (mbar) T (degC) Tpot (K) Tdew (degC) rh (%) VPmax (mbar) VPact (mbar) VPdef (mbar) sh (g/kg) H2OC (mmol/mol) rho (g/m**3) wv (m/s) max. wv (m/s) wd (deg) . 0 01.01.2009 00:10:00 | 996.52 | -8.02 | 265.40 | -8.90 | 93.30 | 3.33 | 3.11 | 0.22 | 1.94 | 3.12 | 1307.75 | 1.03 | 1.75 | 152.3 | . 1 01.01.2009 00:20:00 | 996.57 | -8.41 | 265.01 | -9.28 | 93.40 | 3.23 | 3.02 | 0.21 | 1.89 | 3.03 | 1309.80 | 0.72 | 1.50 | 136.1 | . 2 01.01.2009 00:30:00 | 996.53 | -8.51 | 264.91 | -9.31 | 93.90 | 3.21 | 3.01 | 0.20 | 1.88 | 3.02 | 1310.24 | 0.19 | 0.63 | 171.6 | . 3 01.01.2009 00:40:00 | 996.51 | -8.31 | 265.12 | -9.07 | 94.20 | 3.26 | 3.07 | 0.19 | 1.92 | 3.08 | 1309.19 | 0.34 | 0.50 | 198.0 | . 4 01.01.2009 00:50:00 | 996.51 | -8.27 | 265.15 | -9.04 | 94.10 | 3.27 | 3.08 | 0.19 | 1.92 | 3.09 | 1309.00 | 0.32 | 0.63 | 214.3 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 420546 31.12.2016 23:20:00 | 1000.07 | -4.05 | 269.10 | -8.13 | 73.10 | 4.52 | 3.30 | 1.22 | 2.06 | 3.30 | 1292.98 | 0.67 | 1.52 | 240.0 | . 420547 31.12.2016 23:30:00 | 999.93 | -3.35 | 269.81 | -8.06 | 69.71 | 4.77 | 3.32 | 1.44 | 2.07 | 3.32 | 1289.44 | 1.14 | 1.92 | 234.3 | . 420548 31.12.2016 23:40:00 | 999.82 | -3.16 | 270.01 | -8.21 | 67.91 | 4.84 | 3.28 | 1.55 | 2.05 | 3.28 | 1288.39 | 1.08 | 2.00 | 215.2 | . 420549 31.12.2016 23:50:00 | 999.81 | -4.23 | 268.94 | -8.53 | 71.80 | 4.46 | 3.20 | 1.26 | 1.99 | 3.20 | 1293.56 | 1.49 | 2.16 | 225.8 | . 420550 01.01.2017 00:00:00 | 999.82 | -4.82 | 268.36 | -8.42 | 75.70 | 4.27 | 3.23 | 1.04 | 2.01 | 3.23 | 1296.38 | 1.23 | 1.96 | 184.9 | . 420551 rows × 15 columns . Hour-Wise Slicing . df = df[5::6] df . Date Time p (mbar) T (degC) Tpot (K) Tdew (degC) rh (%) VPmax (mbar) VPact (mbar) VPdef (mbar) sh (g/kg) H2OC (mmol/mol) rho (g/m**3) wv (m/s) max. wv (m/s) wd (deg) . 5 01.01.2009 01:00:00 | 996.50 | -8.05 | 265.38 | -8.78 | 94.40 | 3.33 | 3.14 | 0.19 | 1.96 | 3.15 | 1307.86 | 0.21 | 0.63 | 192.7 | . 11 01.01.2009 02:00:00 | 996.62 | -8.88 | 264.54 | -9.77 | 93.20 | 3.12 | 2.90 | 0.21 | 1.81 | 2.91 | 1312.25 | 0.25 | 0.63 | 190.3 | . 17 01.01.2009 03:00:00 | 996.84 | -8.81 | 264.59 | -9.66 | 93.50 | 3.13 | 2.93 | 0.20 | 1.83 | 2.94 | 1312.18 | 0.18 | 0.63 | 167.2 | . 23 01.01.2009 04:00:00 | 996.99 | -9.05 | 264.34 | -10.02 | 92.60 | 3.07 | 2.85 | 0.23 | 1.78 | 2.85 | 1313.61 | 0.10 | 0.38 | 240.0 | . 29 01.01.2009 05:00:00 | 997.46 | -9.63 | 263.72 | -10.65 | 92.20 | 2.94 | 2.71 | 0.23 | 1.69 | 2.71 | 1317.19 | 0.40 | 0.88 | 157.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 420521 31.12.2016 19:10:00 | 1002.18 | -0.98 | 272.01 | -5.36 | 72.00 | 5.69 | 4.09 | 1.59 | 2.54 | 4.08 | 1280.70 | 0.87 | 1.36 | 190.6 | . 420527 31.12.2016 20:10:00 | 1001.40 | -1.40 | 271.66 | -6.84 | 66.29 | 5.51 | 3.65 | 1.86 | 2.27 | 3.65 | 1281.87 | 1.02 | 1.92 | 225.4 | . 420533 31.12.2016 21:10:00 | 1001.19 | -2.75 | 270.32 | -6.90 | 72.90 | 4.99 | 3.64 | 1.35 | 2.26 | 3.63 | 1288.02 | 0.71 | 1.56 | 158.7 | . 420539 31.12.2016 22:10:00 | 1000.65 | -2.89 | 270.22 | -7.15 | 72.30 | 4.93 | 3.57 | 1.37 | 2.22 | 3.57 | 1288.03 | 0.35 | 0.68 | 216.7 | . 420545 31.12.2016 23:10:00 | 1000.11 | -3.93 | 269.23 | -8.09 | 72.60 | 4.56 | 3.31 | 1.25 | 2.06 | 3.31 | 1292.41 | 0.56 | 1.00 | 202.6 | . 70091 rows × 15 columns . DateTime Indexing . df.index = pd.to_datetime(df[&#39;Date Time&#39;], format = &#39;%d.%m.%Y %H:%M:%S&#39;) df[:6] . Date Time p (mbar) T (degC) Tpot (K) Tdew (degC) rh (%) VPmax (mbar) VPact (mbar) VPdef (mbar) sh (g/kg) H2OC (mmol/mol) rho (g/m**3) wv (m/s) max. wv (m/s) wd (deg) . Date Time . 2009-01-01 01:00:00 01.01.2009 01:00:00 | 996.50 | -8.05 | 265.38 | -8.78 | 94.4 | 3.33 | 3.14 | 0.19 | 1.96 | 3.15 | 1307.86 | 0.21 | 0.63 | 192.7 | . 2009-01-01 02:00:00 01.01.2009 02:00:00 | 996.62 | -8.88 | 264.54 | -9.77 | 93.2 | 3.12 | 2.90 | 0.21 | 1.81 | 2.91 | 1312.25 | 0.25 | 0.63 | 190.3 | . 2009-01-01 03:00:00 01.01.2009 03:00:00 | 996.84 | -8.81 | 264.59 | -9.66 | 93.5 | 3.13 | 2.93 | 0.20 | 1.83 | 2.94 | 1312.18 | 0.18 | 0.63 | 167.2 | . 2009-01-01 04:00:00 01.01.2009 04:00:00 | 996.99 | -9.05 | 264.34 | -10.02 | 92.6 | 3.07 | 2.85 | 0.23 | 1.78 | 2.85 | 1313.61 | 0.10 | 0.38 | 240.0 | . 2009-01-01 05:00:00 01.01.2009 05:00:00 | 997.46 | -9.63 | 263.72 | -10.65 | 92.2 | 2.94 | 2.71 | 0.23 | 1.69 | 2.71 | 1317.19 | 0.40 | 0.88 | 157.0 | . 2009-01-01 06:00:00 01.01.2009 06:00:00 | 997.71 | -9.67 | 263.66 | -10.62 | 92.7 | 2.93 | 2.71 | 0.21 | 1.69 | 2.72 | 1317.71 | 0.05 | 0.50 | 146.0 | . Temperature Plot (degC) . df1 = df[&#39;T (degC)&#39;] . df1.plot() . &lt;AxesSubplot:xlabel=&#39;Date Time&#39;&gt; . Time-Series Window . def create_dataset(df, window_size=5): df_as_np = df.to_numpy() X = [] y = [] for i in range(len(df_as_np)-window_size): row = [[a] for a in df_as_np[i:i+window_size]] X.append(row) label = df_as_np[i+window_size] y.append(label) return np.array(X), np.array(y) . window_size = 5 X, y = create_dataset(df1, window_size) X.shape, y.shape . ((70086, 5, 1), (70086,)) . Train-Test Split . X_train, y_train = X[:60000], y[:60000] X_val, y_val = X[60000:65000], y[60000:65000] X_test, y_test = X[65000:], y[65000:] X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape . ((60000, 5, 1), (60000,), (5000, 5, 1), (5000,), (5086, 5, 1), (5086,)) . Model Setup, Layers and Callbacks . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import * from tensorflow.keras.callbacks import ModelCheckpoint from tensorflow.keras.losses import MeanSquaredError from tensorflow.keras.metrics import RootMeanSquaredError from tensorflow.keras.optimizers import Adam . model = Sequential() model.add(InputLayer((5, 1))) model.add(LSTM(64)) model.add(Dense(8, &#39;relu&#39;)) model.add(Dense(1, &#39;linear&#39;)) model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm (LSTM) (None, 64) 16896 dense (Dense) (None, 8) 520 dense_1 (Dense) (None, 1) 9 ================================================================= Total params: 17,425 Trainable params: 17,425 Non-trainable params: 0 _________________________________________________________________ . cp1 = ModelCheckpoint(&#39;model/&#39;, save_best_only=True) model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[RootMeanSquaredError()]) . model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=12, callbacks=[cp1]) . Epoch 1/12 1875/1875 [==============================] - ETA: 0s - loss: 96.9206 - root_mean_squared_error: 9.8448 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 12s 5ms/step - loss: 96.9206 - root_mean_squared_error: 9.8448 - val_loss: 7.7739 - val_root_mean_squared_error: 2.7882 Epoch 2/12 1869/1875 [============================&gt;.] - ETA: 0s - loss: 12.2745 - root_mean_squared_error: 3.5035 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 9s 5ms/step - loss: 12.2448 - root_mean_squared_error: 3.4993 - val_loss: 0.9448 - val_root_mean_squared_error: 0.9720 Epoch 3/12 1864/1875 [============================&gt;.] - ETA: 0s - loss: 2.3387 - root_mean_squared_error: 1.5293 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 9s 5ms/step - loss: 2.3328 - root_mean_squared_error: 1.5274 - val_loss: 0.5794 - val_root_mean_squared_error: 0.7612 Epoch 4/12 1875/1875 [==============================] - ETA: 0s - loss: 1.0310 - root_mean_squared_error: 1.0154 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 11s 6ms/step - loss: 1.0310 - root_mean_squared_error: 1.0154 - val_loss: 0.5157 - val_root_mean_squared_error: 0.7182 Epoch 5/12 1866/1875 [============================&gt;.] - ETA: 0s - loss: 0.7630 - root_mean_squared_error: 0.8735 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 9s 5ms/step - loss: 0.7630 - root_mean_squared_error: 0.8735 - val_loss: 0.5025 - val_root_mean_squared_error: 0.7089 Epoch 6/12 1875/1875 [==============================] - 6s 3ms/step - loss: 0.6904 - root_mean_squared_error: 0.8309 - val_loss: 0.5431 - val_root_mean_squared_error: 0.7370 Epoch 7/12 1870/1875 [============================&gt;.] - ETA: 0s - loss: 0.6661 - root_mean_squared_error: 0.8162 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 9s 5ms/step - loss: 0.6664 - root_mean_squared_error: 0.8163 - val_loss: 0.4952 - val_root_mean_squared_error: 0.7037 Epoch 8/12 1874/1875 [============================&gt;.] - ETA: 0s - loss: 0.6550 - root_mean_squared_error: 0.8093 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 10s 5ms/step - loss: 0.6550 - root_mean_squared_error: 0.8093 - val_loss: 0.4888 - val_root_mean_squared_error: 0.6992 Epoch 9/12 1875/1875 [==============================] - 5s 3ms/step - loss: 0.6496 - root_mean_squared_error: 0.8060 - val_loss: 0.4909 - val_root_mean_squared_error: 0.7006 Epoch 10/12 1875/1875 [==============================] - 6s 3ms/step - loss: 0.6462 - root_mean_squared_error: 0.8039 - val_loss: 0.5000 - val_root_mean_squared_error: 0.7071 Epoch 11/12 1869/1875 [============================&gt;.] - ETA: 0s - loss: 0.6429 - root_mean_squared_error: 0.8018 . WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: model assets . INFO:tensorflow:Assets written to: model assets WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x00000253ED78C0D0&gt; has the same name &#39;LSTMCell&#39; as a built-in Keras object. Consider renaming &lt;class &#39;keras.layers.recurrent.LSTMCell&#39;&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function. . 1875/1875 [==============================] - 9s 5ms/step - loss: 0.6434 - root_mean_squared_error: 0.8021 - val_loss: 0.4830 - val_root_mean_squared_error: 0.6950 Epoch 12/12 1875/1875 [==============================] - 6s 3ms/step - loss: 0.6409 - root_mean_squared_error: 0.8006 - val_loss: 0.4838 - val_root_mean_squared_error: 0.6955 . &lt;keras.callbacks.History at 0x253e6764b20&gt; . . Predictions and Variance . from tensorflow.keras.models import load_model model = load_model(&#39;model&#39;) . Training . train_predictions = model.predict(X_train).flatten() train_results = pd.DataFrame(data={&#39;Train Predictions&#39;:train_predictions, &#39;Actuals&#39;:y_train}) train_results . Train Predictions Actuals . 0 -9.957811 | -9.67 | . 1 -9.741530 | -9.17 | . 2 -8.840203 | -8.10 | . 3 -7.372804 | -7.66 | . 4 -7.201422 | -7.04 | . ... ... | ... | . 59995 6.082879 | 6.07 | . 59996 7.174989 | 9.88 | . 59997 12.125348 | 13.53 | . 59998 15.739516 | 15.43 | . 59999 16.349472 | 15.54 | . 60000 rows × 2 columns . Test . test_predictions = model.predict(X_test).flatten() test_results = pd.DataFrame(data={&#39;Test Predictions&#39;:test_predictions, &#39;Actuals&#39;:y_test}) test_results . Test Predictions Actuals . 0 14.317657 | 13.99 | . 1 13.151199 | 13.46 | . 2 12.813556 | 12.93 | . 3 12.440071 | 12.43 | . 4 12.000203 | 12.17 | . ... ... | ... | . 5081 -1.141928 | -0.98 | . 5082 -1.407380 | -1.40 | . 5083 -1.605908 | -2.75 | . 5084 -3.087414 | -2.89 | . 5085 -3.146893 | -3.93 | . 5086 rows × 2 columns . Validation . val_predictions = model.predict(X_val).flatten() val_results = pd.DataFrame(data={&#39;Val Predictions&#39;:val_predictions, &#39;Actuals&#39;:y_val}) val_results . Val Predictions Actuals . 0 15.523746 | 14.02 | . 1 13.245414 | 13.67 | . 2 12.967385 | 12.27 | . 3 11.410678 | 11.19 | . 4 10.355401 | 10.85 | . ... ... | ... | . 4995 17.438112 | 18.27 | . 4996 17.329201 | 17.85 | . 4997 17.202988 | 16.65 | . 4998 15.849868 | 15.85 | . 4999 14.996454 | 15.09 | . 5000 rows × 2 columns . Results and Visualization . Training . import matplotlib.pyplot as plt plt.plot(train_results[&#39;Train Predictions&#39;][50:100]) plt.plot(train_results[&#39;Actuals&#39;][50:100]) . [&lt;matplotlib.lines.Line2D at 0x253842f9ca0&gt;] . Test . plt.plot(test_results[&#39;Test Predictions&#39;][:100]) plt.plot(test_results[&#39;Actuals&#39;][:100]) . [&lt;matplotlib.lines.Line2D at 0x253eda4e5b0&gt;] . Validation . plt.plot(val_results[&#39;Val Predictions&#39;][:100]) plt.plot(val_results[&#39;Actuals&#39;][:100]) . [&lt;matplotlib.lines.Line2D at 0x2538435bd00&gt;] .",
            "url": "https://kunal-bhar.github.io/blog/time-series/python/lstm/visualizations/2022/03/20/hourly-temp-forecasting.html",
            "relUrl": "/time-series/python/lstm/visualizations/2022/03/20/hourly-temp-forecasting.html",
            "date": " • Mar 20, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Reddit Headline Analysis",
            "content": "Project At A Glance . Objective: Discover sentiments associated with posts in the &#39;Hot&#39; section of r/worldnews and classify them as Positive, Negative and Neutral. . Data: 754x1 Dataset of the sub-reddit&#39;s headlines scraped using the Reddit API. [View Scraper Notebook] [Download] . Implementation: Reddit API, PRAW, NLTK&#39;s Sentiment Intensity Analyzer (SIA) . Results: . More than half of the headlines were classified as Neutral (~55%). | However, Negative Headlines (33%) still outweigh Positive Headlines (12%) by about 2.75x. | Dataset generated with labelled values to formulate models with better intelligence in the future. | . Deployment: View this project on GitHub. . Dependencies . import pandas as pd from pprint import pprint . Dataset Initialization . df = pd.read_csv(&#39;reddit-headlines.csv&#39;) . df.head() . Unnamed: 0 headlines . 0 0 | Mass graves dug in the besieged Ukrainian city... | . 1 1 | British aircraft carrier leading massive fleet... | . 2 2 | Spain detains $600 million yacht linked to Rus... | . 3 3 | Photo shows officials taking down the Russian ... | . 4 4 | Marina Ovsyannikova: Russian journalist tells ... | . Using NLTK&#39;s Sentiment Intensity Analyzer . import nltk from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA . import matplotlib.pyplot as plt import seaborn as sns . sia= SIA() results = [] for line in df[&#39;headlines&#39;]: pol_score = sia.polarity_scores(line) pol_score[&#39;headline&#39;] = line results.append(pol_score) pprint(results[:3], width=100) . [{&#39;compound&#39;: -0.7579, &#39;headline&#39;: &#39;Mass graves dug in the besieged Ukrainian city of Mariupol, as locals bury their &#39; &#39;dead&#39;, &#39;neg&#39;: 0.333, &#39;neu&#39;: 0.667, &#39;pos&#39;: 0.0}, {&#39;compound&#39;: 0.0, &#39;headline&#39;: &#39;British aircraft carrier leading massive fleet off Norway&#39;, &#39;neg&#39;: 0.0, &#39;neu&#39;: 1.0, &#39;pos&#39;: 0.0}, {&#39;compound&#39;: 0.0, &#39;headline&#39;: &#39;Spain detains $600 million yacht linked to Russian oligarch: Reuters&#39;, &#39;neg&#39;: 0.0, &#39;neu&#39;: 1.0, &#39;pos&#39;: 0.0}] . Generating DataFrame with Polarity Scores . df = pd.DataFrame.from_records(results) . df.sample(4) . neg neu pos compound headline . 380 0.506 | 0.494 | 0.000 | -0.9231 | Tigray war has seen up to half a million dead ... | . 22 0.289 | 0.711 | 0.000 | -0.4291 | &#39;Why? Why? Why?&#39; Ukraine&#39;s Mariupol descends i... | . 660 0.000 | 0.806 | 0.194 | 0.3400 | Ukraine&#39;s &#39;hero&#39; President Zelensky set to rec... | . 725 0.100 | 0.594 | 0.306 | 0.6705 | There is no life for Ukrainian people: Boxing ... | . Labelling and Classification . df[&#39;label&#39;] = 0 df.loc[df[&#39;compound&#39;]&gt;0.33, &#39;label&#39;] = 1 df.loc[df[&#39;compound&#39;]&lt;-0.33, &#39;label&#39;] = -1 . df.sample(4) . neg neu pos compound headline label . 461 0.273 | 0.727 | 0.000 | -0.4588 | Trudeau and almost every Canadian MP banned fr... | -1 | . 482 0.000 | 0.647 | 0.353 | 0.5994 | Help yourself by helping us, Ukraine&#39;s Zelensk... | 1 | . 420 0.000 | 1.000 | 0.000 | 0.0000 | Russia, India explore opening alternative paym... | 0 | . 748 0.239 | 0.761 | 0.000 | -0.2960 | Russia&#39;s state TV hit by stream of resignations | 0 | . df.label.value_counts() . 0 414 -1 247 1 93 Name: label, dtype: int64 . df.label.value_counts(normalize=True)*100 . 0 54.907162 -1 32.758621 1 12.334218 Name: label, dtype: float64 . Examples . print(&#39;Positive Headlines: n&#39;) pprint(list(df[df[&#39;label&#39;] == 1].headline)[:5], width=200) print(&#39; n n Negative Headlines: n&#39;) pprint(list(df[df[&#39;label&#39;] == -1].headline)[:5], width=200) print(&#39; n n Neutral Headlines: n&#39;) pprint(list(df[df[&#39;label&#39;] == 0].headline)[:5], width=200) . Positive Headlines: [&#39;Tibetans seek justice after 63 years of uprising against Chinese rule&#39;, &#34;The Ministry of Foreign Affairs on Tuesday (March 15) praised a Russian woman for her courage after she held up an anti-war sign on live Russian TV. MOFA head: &#39;It takes courage to be the voice of &#34; &#34;conscience&#39;.&#34;, &#39;Saudi Arabia considers accepting yuan for oil sales&#39;, &#39;Russia and Ukraine looking for compromise in peace talks&#39;, &#39;Turkmenistan leader’s son wins presidential election&#39;] Negative Headlines: [&#39;Mass graves dug in the besieged Ukrainian city of Mariupol, as locals bury their dead&#39;, &#34;Russia&#39;s former chief prosecutor says oligarch Roman Abramovich amassed his fortune through a &#39;fraudulent scheme&#39;&#34;, &#39;UN makes March 15 International Day to Combat Islamophobia&#39;, &#39;Not violation of sanctions but Russian oil deal could put India on wrong side of history, says US&#39;, &#34;&#39;Why? Why? Why?&#39; Ukraine&#39;s Mariupol descends into despair&#34;] Neutral Headlines: [&#39;British aircraft carrier leading massive fleet off Norway&#39;, &#39;Spain detains $600 million yacht linked to Russian oligarch: Reuters&#39;, &#39;Photo shows officials taking down the Russian flag after Putin gets the boot from Council of Europe&#39;, &#39;Marina Ovsyannikova: Russian journalist tells of 14-hour interrogation&#39;, &#39;China wary of being impacted by Russia sanctions: Foreign Minister&#39;] . Results and Visualization . fig, ax = plt.subplots(figsize=(8,8)) counts = df.label.value_counts(normalize=True)*100 sns.barplot(x=counts.index, y=counts, ax=ax) ax.set_xticklabels([&#39;Negative&#39;, &#39;Neutral&#39;, &#39;Positive&#39;]) ax.set_ylabel(&#39;Percentage&#39;) plt.show() . Exporting Labelled Dataset as (.csv) . df_export = df[[&#39;headline&#39;, &#39;label&#39;]] . df_export.sample(4) . headline label . 399 EU &#39;Concerned&#39; Over Disrupted Gas Supply, Shoo... | 0 | . 709 Woman fearing for family in Ukraine urges Cana... | -1 | . 618 New Zealand cuts fuel tax and halves public tr... | -1 | . 560 Slovakia meets NATO defence spending commitmen... | 1 | . df_export.to_csv(&#39;reddit-headlines-labelled.csv&#39;, encoding=&#39;utf-8&#39;, index=True) .",
            "url": "https://kunal-bhar.github.io/blog/nlp/python/reddit/news/2022/03/17/reddit-news-analysis.html",
            "relUrl": "/nlp/python/reddit/news/2022/03/17/reddit-news-analysis.html",
            "date": " • Mar 17, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Concurrent Number Prognosis",
            "content": "Project At A Glance . Objective: Predicting values in numeric data that sequentially lead by a given parameter: n. . Data: . Data: Group of 100 5x1 lists that contain overlapping adjacent values. Example: [[1:5], [2:6], [3:7]...] | Target: Group of 100 1x1 lists with the target values for generation. Example: [[1], [2], [3]...] | . Implementation: Long Short-Term Memory (LSTM), Recurrent Neural Networks . Results: . The LSTM computed f-score = 0.935 and loss = 0.036 on training for 400 epochs. | Scatter plots to visualize performance and loss deprecation. | . Deployment: View this project on GitHub. . Dependencies . import numpy as np import matplotlib.pyplot as plt from keras.models import Sequential from keras.layers import Dense, LSTM from sklearn.model_selection import train_test_split . Initialization . Data = [[[(i+j)/100] for i in range(5)] for j in range(100)] Target = [(i+5)/100 for i in range(100)] . Data as NumPy Arrays . data = np.array(Data, dtype=float) target = np.array(Target, dtype=float) . data.shape . (100, 5, 1) . target.shape . (100,) . Train-Test Split . x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=4) . Model Setup and Layers . model = Sequential() . model.add(LSTM((1), batch_input_shape=(None,5,1), return_sequences=True)) model.add(LSTM((1), return_sequences=False)) # the return_sequences parameter assists in enabling convolution for the sequential LSTM . model.compile(loss=&#39;mean_absolute_error&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm (LSTM) (None, 5, 1) 12 lstm_1 (LSTM) (None, 1) 12 ================================================================= Total params: 24 Trainable params: 24 Non-trainable params: 0 _________________________________________________________________ . history = model.fit(x_train, y_train, epochs=400, validation_data=(x_test, y_test)) . Epoch 1/400 3/3 [==============================] - 5s 474ms/step - loss: 0.5351 - accuracy: 0.0000e+00 - val_loss: 0.4311 - val_accuracy: 0.0000e+00 Epoch 2/400 3/3 [==============================] - 0s 27ms/step - loss: 0.5325 - accuracy: 0.0000e+00 - val_loss: 0.4286 - val_accuracy: 0.0000e+00 Epoch 3/400 3/3 [==============================] - 0s 25ms/step - loss: 0.5297 - accuracy: 0.0000e+00 - val_loss: 0.4260 - val_accuracy: 0.0000e+00 Epoch 4/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5270 - accuracy: 0.0000e+00 - val_loss: 0.4233 - val_accuracy: 0.0000e+00 Epoch 5/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5241 - accuracy: 0.0000e+00 - val_loss: 0.4207 - val_accuracy: 0.0000e+00 Epoch 6/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5213 - accuracy: 0.0000e+00 - val_loss: 0.4179 - val_accuracy: 0.0000e+00 Epoch 7/400 3/3 [==============================] - 0s 21ms/step - loss: 0.5184 - accuracy: 0.0000e+00 - val_loss: 0.4151 - val_accuracy: 0.0000e+00 Epoch 8/400 3/3 [==============================] - 0s 15ms/step - loss: 0.5154 - accuracy: 0.0000e+00 - val_loss: 0.4123 - val_accuracy: 0.0000e+00 Epoch 9/400 3/3 [==============================] - 0s 13ms/step - loss: 0.5123 - accuracy: 0.0000e+00 - val_loss: 0.4094 - val_accuracy: 0.0000e+00 Epoch 10/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5093 - accuracy: 0.0000e+00 - val_loss: 0.4064 - val_accuracy: 0.0000e+00 Epoch 11/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5061 - accuracy: 0.0000e+00 - val_loss: 0.4034 - val_accuracy: 0.0000e+00 Epoch 12/400 3/3 [==============================] - 0s 15ms/step - loss: 0.5029 - accuracy: 0.0000e+00 - val_loss: 0.4003 - val_accuracy: 0.0000e+00 Epoch 13/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4996 - accuracy: 0.0000e+00 - val_loss: 0.3972 - val_accuracy: 0.0000e+00 Epoch 14/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4962 - accuracy: 0.0000e+00 - val_loss: 0.3940 - val_accuracy: 0.0000e+00 Epoch 15/400 3/3 [==============================] - 0s 17ms/step - loss: 0.4928 - accuracy: 0.0000e+00 - val_loss: 0.3907 - val_accuracy: 0.0000e+00 Epoch 16/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4893 - accuracy: 0.0000e+00 - val_loss: 0.3873 - val_accuracy: 0.0000e+00 Epoch 17/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4857 - accuracy: 0.0000e+00 - val_loss: 0.3839 - val_accuracy: 0.0000e+00 Epoch 18/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4820 - accuracy: 0.0000e+00 - val_loss: 0.3804 - val_accuracy: 0.0000e+00 Epoch 19/400 3/3 [==============================] - 0s 13ms/step - loss: 0.4783 - accuracy: 0.0000e+00 - val_loss: 0.3768 - val_accuracy: 0.0000e+00 Epoch 20/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4745 - accuracy: 0.0000e+00 - val_loss: 0.3731 - val_accuracy: 0.0000e+00 Epoch 21/400 3/3 [==============================] - 0s 16ms/step - loss: 0.4705 - accuracy: 0.0000e+00 - val_loss: 0.3693 - val_accuracy: 0.0000e+00 Epoch 22/400 3/3 [==============================] - 0s 18ms/step - loss: 0.4665 - accuracy: 0.0000e+00 - val_loss: 0.3654 - val_accuracy: 0.0000e+00 Epoch 23/400 3/3 [==============================] - 0s 22ms/step - loss: 0.4625 - accuracy: 0.0000e+00 - val_loss: 0.3615 - val_accuracy: 0.0000e+00 Epoch 24/400 3/3 [==============================] - 0s 25ms/step - loss: 0.4584 - accuracy: 0.0000e+00 - val_loss: 0.3575 - val_accuracy: 0.0000e+00 Epoch 25/400 3/3 [==============================] - 0s 22ms/step - loss: 0.4542 - accuracy: 0.0000e+00 - val_loss: 0.3534 - val_accuracy: 0.0000e+00 Epoch 26/400 3/3 [==============================] - 0s 29ms/step - loss: 0.4500 - accuracy: 0.0000e+00 - val_loss: 0.3492 - val_accuracy: 0.0000e+00 Epoch 27/400 3/3 [==============================] - 0s 30ms/step - loss: 0.4457 - accuracy: 0.0000e+00 - val_loss: 0.3450 - val_accuracy: 0.0000e+00 Epoch 28/400 3/3 [==============================] - 0s 29ms/step - loss: 0.4413 - accuracy: 0.0000e+00 - val_loss: 0.3408 - val_accuracy: 0.0000e+00 Epoch 29/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4369 - accuracy: 0.0000e+00 - val_loss: 0.3367 - val_accuracy: 0.0000e+00 Epoch 30/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4324 - accuracy: 0.0000e+00 - val_loss: 0.3325 - val_accuracy: 0.0000e+00 Epoch 31/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4278 - accuracy: 0.0000e+00 - val_loss: 0.3282 - val_accuracy: 0.0000e+00 Epoch 32/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4232 - accuracy: 0.0000e+00 - val_loss: 0.3238 - val_accuracy: 0.0000e+00 Epoch 33/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4185 - accuracy: 0.0000e+00 - val_loss: 0.3194 - val_accuracy: 0.0000e+00 Epoch 34/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4138 - accuracy: 0.0000e+00 - val_loss: 0.3149 - val_accuracy: 0.0000e+00 Epoch 35/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4091 - accuracy: 0.0000e+00 - val_loss: 0.3104 - val_accuracy: 0.0000e+00 Epoch 36/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4043 - accuracy: 0.0000e+00 - val_loss: 0.3057 - val_accuracy: 0.0000e+00 Epoch 37/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3994 - accuracy: 0.0000e+00 - val_loss: 0.3010 - val_accuracy: 0.0000e+00 Epoch 38/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3946 - accuracy: 0.0000e+00 - val_loss: 0.2962 - val_accuracy: 0.0000e+00 Epoch 39/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3897 - accuracy: 0.0000e+00 - val_loss: 0.2913 - val_accuracy: 0.0000e+00 Epoch 40/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3850 - accuracy: 0.0000e+00 - val_loss: 0.2864 - val_accuracy: 0.0000e+00 Epoch 41/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3802 - accuracy: 0.0000e+00 - val_loss: 0.2815 - val_accuracy: 0.0000e+00 Epoch 42/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3753 - accuracy: 0.0000e+00 - val_loss: 0.2765 - val_accuracy: 0.0000e+00 Epoch 43/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3705 - accuracy: 0.0000e+00 - val_loss: 0.2715 - val_accuracy: 0.0000e+00 Epoch 44/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3658 - accuracy: 0.0000e+00 - val_loss: 0.2664 - val_accuracy: 0.0000e+00 Epoch 45/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3612 - accuracy: 0.0000e+00 - val_loss: 0.2614 - val_accuracy: 0.0000e+00 Epoch 46/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3562 - accuracy: 0.0000e+00 - val_loss: 0.2568 - val_accuracy: 0.0000e+00 Epoch 47/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3515 - accuracy: 0.0000e+00 - val_loss: 0.2521 - val_accuracy: 0.0000e+00 Epoch 48/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3467 - accuracy: 0.0000e+00 - val_loss: 0.2474 - val_accuracy: 0.0000e+00 Epoch 49/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3420 - accuracy: 0.0000e+00 - val_loss: 0.2426 - val_accuracy: 0.0000e+00 Epoch 50/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3373 - accuracy: 0.0000e+00 - val_loss: 0.2379 - val_accuracy: 0.0000e+00 Epoch 51/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3326 - accuracy: 0.0000e+00 - val_loss: 0.2335 - val_accuracy: 0.0000e+00 Epoch 52/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3279 - accuracy: 0.0000e+00 - val_loss: 0.2294 - val_accuracy: 0.0000e+00 Epoch 53/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3231 - accuracy: 0.0000e+00 - val_loss: 0.2254 - val_accuracy: 0.0000e+00 Epoch 54/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3182 - accuracy: 0.0000e+00 - val_loss: 0.2214 - val_accuracy: 0.0000e+00 Epoch 55/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3136 - accuracy: 0.0000e+00 - val_loss: 0.2180 - val_accuracy: 0.0000e+00 Epoch 56/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3088 - accuracy: 0.0000e+00 - val_loss: 0.2145 - val_accuracy: 0.0000e+00 Epoch 57/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3043 - accuracy: 0.0000e+00 - val_loss: 0.2111 - val_accuracy: 0.0000e+00 Epoch 58/400 3/3 [==============================] - 0s 15ms/step - loss: 0.2999 - accuracy: 0.0000e+00 - val_loss: 0.2077 - val_accuracy: 0.0000e+00 Epoch 59/400 3/3 [==============================] - 0s 14ms/step - loss: 0.2952 - accuracy: 0.0000e+00 - val_loss: 0.2043 - val_accuracy: 0.0000e+00 Epoch 60/400 3/3 [==============================] - 0s 16ms/step - loss: 0.2914 - accuracy: 0.0000e+00 - val_loss: 0.2013 - val_accuracy: 0.0000e+00 Epoch 61/400 3/3 [==============================] - 0s 24ms/step - loss: 0.2867 - accuracy: 0.0000e+00 - val_loss: 0.1985 - val_accuracy: 0.0000e+00 Epoch 62/400 3/3 [==============================] - 0s 40ms/step - loss: 0.2828 - accuracy: 0.0000e+00 - val_loss: 0.1957 - val_accuracy: 0.0000e+00 Epoch 63/400 3/3 [==============================] - 0s 35ms/step - loss: 0.2788 - accuracy: 0.0000e+00 - val_loss: 0.1929 - val_accuracy: 0.0000e+00 Epoch 64/400 3/3 [==============================] - 0s 26ms/step - loss: 0.2750 - accuracy: 0.0000e+00 - val_loss: 0.1901 - val_accuracy: 0.0000e+00 Epoch 65/400 3/3 [==============================] - 0s 27ms/step - loss: 0.2710 - accuracy: 0.0000e+00 - val_loss: 0.1877 - val_accuracy: 0.0000e+00 Epoch 66/400 3/3 [==============================] - 0s 33ms/step - loss: 0.2674 - accuracy: 0.0000e+00 - val_loss: 0.1855 - val_accuracy: 0.0000e+00 Epoch 67/400 3/3 [==============================] - 0s 45ms/step - loss: 0.2639 - accuracy: 0.0000e+00 - val_loss: 0.1833 - val_accuracy: 0.0000e+00 Epoch 68/400 3/3 [==============================] - 0s 42ms/step - loss: 0.2604 - accuracy: 0.0000e+00 - val_loss: 0.1816 - val_accuracy: 0.0000e+00 Epoch 69/400 3/3 [==============================] - 0s 36ms/step - loss: 0.2568 - accuracy: 0.0000e+00 - val_loss: 0.1800 - val_accuracy: 0.0000e+00 Epoch 70/400 3/3 [==============================] - 0s 37ms/step - loss: 0.2536 - accuracy: 0.0000e+00 - val_loss: 0.1785 - val_accuracy: 0.0000e+00 Epoch 71/400 3/3 [==============================] - 0s 48ms/step - loss: 0.2504 - accuracy: 0.0000e+00 - val_loss: 0.1769 - val_accuracy: 0.0000e+00 Epoch 72/400 3/3 [==============================] - 0s 51ms/step - loss: 0.2474 - accuracy: 0.0000e+00 - val_loss: 0.1757 - val_accuracy: 0.0000e+00 Epoch 73/400 3/3 [==============================] - 0s 50ms/step - loss: 0.2446 - accuracy: 0.0000e+00 - val_loss: 0.1747 - val_accuracy: 0.0000e+00 Epoch 74/400 3/3 [==============================] - 0s 48ms/step - loss: 0.2416 - accuracy: 0.0000e+00 - val_loss: 0.1737 - val_accuracy: 0.0000e+00 Epoch 75/400 3/3 [==============================] - 0s 55ms/step - loss: 0.2390 - accuracy: 0.0000e+00 - val_loss: 0.1727 - val_accuracy: 0.0000e+00 Epoch 76/400 3/3 [==============================] - 0s 51ms/step - loss: 0.2364 - accuracy: 0.0000e+00 - val_loss: 0.1718 - val_accuracy: 0.0000e+00 Epoch 77/400 3/3 [==============================] - 0s 59ms/step - loss: 0.2339 - accuracy: 0.0000e+00 - val_loss: 0.1708 - val_accuracy: 0.0000e+00 Epoch 78/400 3/3 [==============================] - 0s 54ms/step - loss: 0.2316 - accuracy: 0.0000e+00 - val_loss: 0.1698 - val_accuracy: 0.0000e+00 Epoch 79/400 3/3 [==============================] - 0s 28ms/step - loss: 0.2293 - accuracy: 0.0000e+00 - val_loss: 0.1688 - val_accuracy: 0.0000e+00 Epoch 80/400 3/3 [==============================] - 0s 21ms/step - loss: 0.2274 - accuracy: 0.0000e+00 - val_loss: 0.1678 - val_accuracy: 0.0000e+00 Epoch 81/400 3/3 [==============================] - 0s 23ms/step - loss: 0.2253 - accuracy: 0.0000e+00 - val_loss: 0.1668 - val_accuracy: 0.0000e+00 Epoch 82/400 3/3 [==============================] - 0s 25ms/step - loss: 0.2232 - accuracy: 0.0000e+00 - val_loss: 0.1661 - val_accuracy: 0.0000e+00 Epoch 83/400 3/3 [==============================] - 0s 23ms/step - loss: 0.2214 - accuracy: 0.0000e+00 - val_loss: 0.1654 - val_accuracy: 0.0000e+00 Epoch 84/400 3/3 [==============================] - 0s 46ms/step - loss: 0.2195 - accuracy: 0.0000e+00 - val_loss: 0.1647 - val_accuracy: 0.0500 Epoch 85/400 3/3 [==============================] - 0s 46ms/step - loss: 0.2176 - accuracy: 0.0000e+00 - val_loss: 0.1640 - val_accuracy: 0.0500 Epoch 86/400 3/3 [==============================] - 0s 43ms/step - loss: 0.2158 - accuracy: 0.0000e+00 - val_loss: 0.1633 - val_accuracy: 0.0500 Epoch 87/400 3/3 [==============================] - 0s 35ms/step - loss: 0.2140 - accuracy: 0.0000e+00 - val_loss: 0.1629 - val_accuracy: 0.0500 Epoch 88/400 3/3 [==============================] - 0s 25ms/step - loss: 0.2122 - accuracy: 0.0000e+00 - val_loss: 0.1625 - val_accuracy: 0.0500 Epoch 89/400 3/3 [==============================] - 0s 23ms/step - loss: 0.2104 - accuracy: 0.0000e+00 - val_loss: 0.1621 - val_accuracy: 0.0500 Epoch 90/400 3/3 [==============================] - 0s 24ms/step - loss: 0.2085 - accuracy: 0.0000e+00 - val_loss: 0.1617 - val_accuracy: 0.0500 Epoch 91/400 3/3 [==============================] - 0s 30ms/step - loss: 0.2070 - accuracy: 0.0000e+00 - val_loss: 0.1612 - val_accuracy: 0.0500 Epoch 92/400 3/3 [==============================] - 0s 25ms/step - loss: 0.2052 - accuracy: 0.0000e+00 - val_loss: 0.1607 - val_accuracy: 0.0500 Epoch 93/400 3/3 [==============================] - 0s 25ms/step - loss: 0.2036 - accuracy: 0.0000e+00 - val_loss: 0.1601 - val_accuracy: 0.0500 Epoch 94/400 3/3 [==============================] - 0s 30ms/step - loss: 0.2020 - accuracy: 0.0000e+00 - val_loss: 0.1595 - val_accuracy: 0.0500 Epoch 95/400 3/3 [==============================] - 0s 28ms/step - loss: 0.2005 - accuracy: 0.0000e+00 - val_loss: 0.1589 - val_accuracy: 0.0500 Epoch 96/400 3/3 [==============================] - 0s 29ms/step - loss: 0.1989 - accuracy: 0.0000e+00 - val_loss: 0.1583 - val_accuracy: 0.0500 Epoch 97/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1974 - accuracy: 0.0000e+00 - val_loss: 0.1577 - val_accuracy: 0.0500 Epoch 98/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1959 - accuracy: 0.0000e+00 - val_loss: 0.1571 - val_accuracy: 0.0500 Epoch 99/400 3/3 [==============================] - 0s 22ms/step - loss: 0.1943 - accuracy: 0.0000e+00 - val_loss: 0.1565 - val_accuracy: 0.0500 Epoch 100/400 3/3 [==============================] - 0s 24ms/step - loss: 0.1928 - accuracy: 0.0000e+00 - val_loss: 0.1558 - val_accuracy: 0.0500 Epoch 101/400 3/3 [==============================] - 0s 36ms/step - loss: 0.1913 - accuracy: 0.0000e+00 - val_loss: 0.1552 - val_accuracy: 0.0500 Epoch 102/400 3/3 [==============================] - 0s 49ms/step - loss: 0.1899 - accuracy: 0.0000e+00 - val_loss: 0.1544 - val_accuracy: 0.0500 Epoch 103/400 3/3 [==============================] - 0s 56ms/step - loss: 0.1883 - accuracy: 0.0000e+00 - val_loss: 0.1537 - val_accuracy: 0.0500 Epoch 104/400 3/3 [==============================] - 0s 52ms/step - loss: 0.1867 - accuracy: 0.0000e+00 - val_loss: 0.1529 - val_accuracy: 0.0500 Epoch 105/400 3/3 [==============================] - 0s 46ms/step - loss: 0.1852 - accuracy: 0.0000e+00 - val_loss: 0.1521 - val_accuracy: 0.0500 Epoch 106/400 3/3 [==============================] - 0s 37ms/step - loss: 0.1837 - accuracy: 0.0000e+00 - val_loss: 0.1512 - val_accuracy: 0.0500 Epoch 107/400 3/3 [==============================] - 0s 40ms/step - loss: 0.1822 - accuracy: 0.0000e+00 - val_loss: 0.1503 - val_accuracy: 0.0500 Epoch 108/400 3/3 [==============================] - 0s 47ms/step - loss: 0.1806 - accuracy: 0.0000e+00 - val_loss: 0.1494 - val_accuracy: 0.0500 Epoch 109/400 3/3 [==============================] - 0s 37ms/step - loss: 0.1790 - accuracy: 0.0000e+00 - val_loss: 0.1485 - val_accuracy: 0.0500 Epoch 110/400 3/3 [==============================] - 0s 29ms/step - loss: 0.1775 - accuracy: 0.0000e+00 - val_loss: 0.1476 - val_accuracy: 0.0500 Epoch 111/400 3/3 [==============================] - 0s 25ms/step - loss: 0.1759 - accuracy: 0.0000e+00 - val_loss: 0.1466 - val_accuracy: 0.0500 Epoch 112/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1744 - accuracy: 0.0000e+00 - val_loss: 0.1457 - val_accuracy: 0.0500 Epoch 113/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1727 - accuracy: 0.0000e+00 - val_loss: 0.1447 - val_accuracy: 0.0500 Epoch 114/400 3/3 [==============================] - 0s 28ms/step - loss: 0.1711 - accuracy: 0.0000e+00 - val_loss: 0.1434 - val_accuracy: 0.0500 Epoch 115/400 3/3 [==============================] - 0s 47ms/step - loss: 0.1695 - accuracy: 0.0000e+00 - val_loss: 0.1421 - val_accuracy: 0.0500 Epoch 116/400 3/3 [==============================] - 0s 44ms/step - loss: 0.1679 - accuracy: 0.0000e+00 - val_loss: 0.1406 - val_accuracy: 0.0500 Epoch 117/400 3/3 [==============================] - 0s 46ms/step - loss: 0.1662 - accuracy: 0.0000e+00 - val_loss: 0.1391 - val_accuracy: 0.0500 Epoch 118/400 3/3 [==============================] - 0s 45ms/step - loss: 0.1646 - accuracy: 0.0000e+00 - val_loss: 0.1373 - val_accuracy: 0.0500 Epoch 119/400 3/3 [==============================] - 0s 49ms/step - loss: 0.1628 - accuracy: 0.0000e+00 - val_loss: 0.1358 - val_accuracy: 0.0500 Epoch 120/400 3/3 [==============================] - 0s 51ms/step - loss: 0.1612 - accuracy: 0.0000e+00 - val_loss: 0.1339 - val_accuracy: 0.0500 Epoch 121/400 3/3 [==============================] - 0s 52ms/step - loss: 0.1595 - accuracy: 0.0000e+00 - val_loss: 0.1321 - val_accuracy: 0.0500 Epoch 122/400 3/3 [==============================] - 0s 52ms/step - loss: 0.1577 - accuracy: 0.0000e+00 - val_loss: 0.1304 - val_accuracy: 0.0500 Epoch 123/400 3/3 [==============================] - 0s 33ms/step - loss: 0.1560 - accuracy: 0.0000e+00 - val_loss: 0.1285 - val_accuracy: 0.0500 Epoch 124/400 3/3 [==============================] - 0s 22ms/step - loss: 0.1542 - accuracy: 0.0000e+00 - val_loss: 0.1266 - val_accuracy: 0.0500 Epoch 125/400 3/3 [==============================] - 0s 21ms/step - loss: 0.1525 - accuracy: 0.0000e+00 - val_loss: 0.1245 - val_accuracy: 0.0500 Epoch 126/400 3/3 [==============================] - 0s 45ms/step - loss: 0.1507 - accuracy: 0.0000e+00 - val_loss: 0.1227 - val_accuracy: 0.0500 Epoch 127/400 3/3 [==============================] - 0s 48ms/step - loss: 0.1489 - accuracy: 0.0000e+00 - val_loss: 0.1205 - val_accuracy: 0.0500 Epoch 128/400 3/3 [==============================] - 0s 42ms/step - loss: 0.1470 - accuracy: 0.0000e+00 - val_loss: 0.1186 - val_accuracy: 0.0500 Epoch 129/400 3/3 [==============================] - 0s 18ms/step - loss: 0.1452 - accuracy: 0.0000e+00 - val_loss: 0.1165 - val_accuracy: 0.0500 Epoch 130/400 3/3 [==============================] - 0s 14ms/step - loss: 0.1434 - accuracy: 0.0000e+00 - val_loss: 0.1144 - val_accuracy: 0.0500 Epoch 131/400 3/3 [==============================] - 0s 16ms/step - loss: 0.1416 - accuracy: 0.0000e+00 - val_loss: 0.1124 - val_accuracy: 0.0500 Epoch 132/400 3/3 [==============================] - 0s 14ms/step - loss: 0.1396 - accuracy: 0.0000e+00 - val_loss: 0.1106 - val_accuracy: 0.0500 Epoch 133/400 3/3 [==============================] - 0s 14ms/step - loss: 0.1378 - accuracy: 0.0000e+00 - val_loss: 0.1085 - val_accuracy: 0.0500 Epoch 134/400 3/3 [==============================] - 0s 15ms/step - loss: 0.1358 - accuracy: 0.0000e+00 - val_loss: 0.1066 - val_accuracy: 0.0500 Epoch 135/400 3/3 [==============================] - 0s 15ms/step - loss: 0.1339 - accuracy: 0.0000e+00 - val_loss: 0.1044 - val_accuracy: 0.0500 Epoch 136/400 3/3 [==============================] - 0s 15ms/step - loss: 0.1319 - accuracy: 0.0000e+00 - val_loss: 0.1023 - val_accuracy: 0.0500 Epoch 137/400 3/3 [==============================] - 0s 17ms/step - loss: 0.1300 - accuracy: 0.0000e+00 - val_loss: 0.1002 - val_accuracy: 0.0500 Epoch 138/400 3/3 [==============================] - 0s 35ms/step - loss: 0.1279 - accuracy: 0.0000e+00 - val_loss: 0.0982 - val_accuracy: 0.0500 Epoch 139/400 3/3 [==============================] - 0s 52ms/step - loss: 0.1259 - accuracy: 0.0000e+00 - val_loss: 0.0961 - val_accuracy: 0.0500 Epoch 140/400 3/3 [==============================] - 0s 46ms/step - loss: 0.1238 - accuracy: 0.0000e+00 - val_loss: 0.0941 - val_accuracy: 0.0500 Epoch 141/400 3/3 [==============================] - 0s 34ms/step - loss: 0.1217 - accuracy: 0.0000e+00 - val_loss: 0.0921 - val_accuracy: 0.0500 Epoch 142/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1196 - accuracy: 0.0000e+00 - val_loss: 0.0899 - val_accuracy: 0.0500 Epoch 143/400 3/3 [==============================] - 0s 25ms/step - loss: 0.1174 - accuracy: 0.0000e+00 - val_loss: 0.0875 - val_accuracy: 0.0500 Epoch 144/400 3/3 [==============================] - 0s 25ms/step - loss: 0.1153 - accuracy: 0.0000e+00 - val_loss: 0.0851 - val_accuracy: 0.0500 Epoch 145/400 3/3 [==============================] - 0s 31ms/step - loss: 0.1130 - accuracy: 0.0000e+00 - val_loss: 0.0829 - val_accuracy: 0.0500 Epoch 146/400 3/3 [==============================] - 0s 50ms/step - loss: 0.1108 - accuracy: 0.0000e+00 - val_loss: 0.0808 - val_accuracy: 0.0500 Epoch 147/400 3/3 [==============================] - 0s 51ms/step - loss: 0.1085 - accuracy: 0.0000e+00 - val_loss: 0.0786 - val_accuracy: 0.0500 Epoch 148/400 3/3 [==============================] - 0s 46ms/step - loss: 0.1062 - accuracy: 0.0000e+00 - val_loss: 0.0763 - val_accuracy: 0.0500 Epoch 149/400 3/3 [==============================] - 0s 28ms/step - loss: 0.1038 - accuracy: 0.0000e+00 - val_loss: 0.0739 - val_accuracy: 0.0500 Epoch 150/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1014 - accuracy: 0.0000e+00 - val_loss: 0.0712 - val_accuracy: 0.0500 Epoch 151/400 3/3 [==============================] - 0s 51ms/step - loss: 0.0990 - accuracy: 0.0000e+00 - val_loss: 0.0686 - val_accuracy: 0.0500 Epoch 152/400 3/3 [==============================] - 0s 67ms/step - loss: 0.0966 - accuracy: 0.0000e+00 - val_loss: 0.0660 - val_accuracy: 0.0500 Epoch 153/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0941 - accuracy: 0.0000e+00 - val_loss: 0.0635 - val_accuracy: 0.0500 Epoch 154/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0916 - accuracy: 0.0000e+00 - val_loss: 0.0610 - val_accuracy: 0.0500 Epoch 155/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0892 - accuracy: 0.0000e+00 - val_loss: 0.0595 - val_accuracy: 0.0500 Epoch 156/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0870 - accuracy: 0.0000e+00 - val_loss: 0.0585 - val_accuracy: 0.0500 Epoch 157/400 3/3 [==============================] - 0s 51ms/step - loss: 0.0849 - accuracy: 0.0000e+00 - val_loss: 0.0578 - val_accuracy: 0.0500 Epoch 158/400 3/3 [==============================] - 0s 62ms/step - loss: 0.0829 - accuracy: 0.0000e+00 - val_loss: 0.0570 - val_accuracy: 0.0500 Epoch 159/400 3/3 [==============================] - 0s 56ms/step - loss: 0.0814 - accuracy: 0.0000e+00 - val_loss: 0.0563 - val_accuracy: 0.0500 Epoch 160/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0799 - accuracy: 0.0000e+00 - val_loss: 0.0556 - val_accuracy: 0.0500 Epoch 161/400 3/3 [==============================] - 0s 52ms/step - loss: 0.0787 - accuracy: 0.0000e+00 - val_loss: 0.0553 - val_accuracy: 0.0500 Epoch 162/400 3/3 [==============================] - 0s 65ms/step - loss: 0.0774 - accuracy: 0.0000e+00 - val_loss: 0.0551 - val_accuracy: 0.0500 Epoch 163/400 3/3 [==============================] - 0s 58ms/step - loss: 0.0762 - accuracy: 0.0000e+00 - val_loss: 0.0550 - val_accuracy: 0.0500 Epoch 164/400 3/3 [==============================] - 0s 60ms/step - loss: 0.0752 - accuracy: 0.0000e+00 - val_loss: 0.0550 - val_accuracy: 0.0500 Epoch 165/400 3/3 [==============================] - 0s 37ms/step - loss: 0.0742 - accuracy: 0.0000e+00 - val_loss: 0.0552 - val_accuracy: 0.0500 Epoch 166/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0733 - accuracy: 0.0000e+00 - val_loss: 0.0554 - val_accuracy: 0.0500 Epoch 167/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0723 - accuracy: 0.0000e+00 - val_loss: 0.0555 - val_accuracy: 0.0500 Epoch 168/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0715 - accuracy: 0.0000e+00 - val_loss: 0.0557 - val_accuracy: 0.0500 Epoch 169/400 3/3 [==============================] - 0s 34ms/step - loss: 0.0708 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 170/400 3/3 [==============================] - 0s 31ms/step - loss: 0.0701 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 171/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0694 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 172/400 3/3 [==============================] - 0s 39ms/step - loss: 0.0688 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 173/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0682 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 174/400 3/3 [==============================] - 0s 45ms/step - loss: 0.0676 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 175/400 3/3 [==============================] - 0s 39ms/step - loss: 0.0671 - accuracy: 0.0000e+00 - val_loss: 0.0557 - val_accuracy: 0.0500 Epoch 176/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0666 - accuracy: 0.0000e+00 - val_loss: 0.0557 - val_accuracy: 0.0500 Epoch 177/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0661 - accuracy: 0.0000e+00 - val_loss: 0.0556 - val_accuracy: 0.0500 Epoch 178/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0657 - accuracy: 0.0000e+00 - val_loss: 0.0556 - val_accuracy: 0.0500 Epoch 179/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0652 - accuracy: 0.0000e+00 - val_loss: 0.0555 - val_accuracy: 0.0500 Epoch 180/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0648 - accuracy: 0.0000e+00 - val_loss: 0.0554 - val_accuracy: 0.0500 Epoch 181/400 3/3 [==============================] - 0s 23ms/step - loss: 0.0644 - accuracy: 0.0000e+00 - val_loss: 0.0554 - val_accuracy: 0.0500 Epoch 182/400 3/3 [==============================] - 0s 21ms/step - loss: 0.0640 - accuracy: 0.0000e+00 - val_loss: 0.0553 - val_accuracy: 0.0500 Epoch 183/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0637 - accuracy: 0.0000e+00 - val_loss: 0.0551 - val_accuracy: 0.0500 Epoch 184/400 3/3 [==============================] - 0s 51ms/step - loss: 0.0633 - accuracy: 0.0000e+00 - val_loss: 0.0549 - val_accuracy: 0.0500 Epoch 185/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0629 - accuracy: 0.0000e+00 - val_loss: 0.0547 - val_accuracy: 0.0500 Epoch 186/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0625 - accuracy: 0.0000e+00 - val_loss: 0.0545 - val_accuracy: 0.0500 Epoch 187/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0622 - accuracy: 0.0000e+00 - val_loss: 0.0543 - val_accuracy: 0.0500 Epoch 188/400 3/3 [==============================] - 0s 41ms/step - loss: 0.0618 - accuracy: 0.0000e+00 - val_loss: 0.0543 - val_accuracy: 0.0500 Epoch 189/400 3/3 [==============================] - 0s 41ms/step - loss: 0.0615 - accuracy: 0.0000e+00 - val_loss: 0.0542 - val_accuracy: 0.0500 Epoch 190/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0611 - accuracy: 0.0000e+00 - val_loss: 0.0542 - val_accuracy: 0.0500 Epoch 191/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0609 - accuracy: 0.0000e+00 - val_loss: 0.0541 - val_accuracy: 0.0500 Epoch 192/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0605 - accuracy: 0.0000e+00 - val_loss: 0.0538 - val_accuracy: 0.0500 Epoch 193/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0601 - accuracy: 0.0000e+00 - val_loss: 0.0536 - val_accuracy: 0.0500 Epoch 194/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0598 - accuracy: 0.0000e+00 - val_loss: 0.0536 - val_accuracy: 0.0500 Epoch 195/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0595 - accuracy: 0.0000e+00 - val_loss: 0.0535 - val_accuracy: 0.0500 Epoch 196/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0592 - accuracy: 0.0000e+00 - val_loss: 0.0534 - val_accuracy: 0.0500 Epoch 197/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0589 - accuracy: 0.0000e+00 - val_loss: 0.0533 - val_accuracy: 0.0500 Epoch 198/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0586 - accuracy: 0.0000e+00 - val_loss: 0.0532 - val_accuracy: 0.0500 Epoch 199/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0583 - accuracy: 0.0000e+00 - val_loss: 0.0531 - val_accuracy: 0.0500 Epoch 200/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0581 - accuracy: 0.0000e+00 - val_loss: 0.0530 - val_accuracy: 0.0500 Epoch 201/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0579 - accuracy: 0.0000e+00 - val_loss: 0.0528 - val_accuracy: 0.0500 Epoch 202/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0575 - accuracy: 0.0000e+00 - val_loss: 0.0526 - val_accuracy: 0.0500 Epoch 203/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0572 - accuracy: 0.0000e+00 - val_loss: 0.0525 - val_accuracy: 0.0500 Epoch 204/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0570 - accuracy: 0.0000e+00 - val_loss: 0.0525 - val_accuracy: 0.0500 Epoch 205/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0567 - accuracy: 0.0000e+00 - val_loss: 0.0524 - val_accuracy: 0.0500 Epoch 206/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0565 - accuracy: 0.0000e+00 - val_loss: 0.0523 - val_accuracy: 0.0500 Epoch 207/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0562 - accuracy: 0.0000e+00 - val_loss: 0.0522 - val_accuracy: 0.0500 Epoch 208/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0561 - accuracy: 0.0000e+00 - val_loss: 0.0522 - val_accuracy: 0.0500 Epoch 209/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0558 - accuracy: 0.0000e+00 - val_loss: 0.0520 - val_accuracy: 0.0500 Epoch 210/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0555 - accuracy: 0.0000e+00 - val_loss: 0.0518 - val_accuracy: 0.0500 Epoch 211/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0553 - accuracy: 0.0000e+00 - val_loss: 0.0517 - val_accuracy: 0.0500 Epoch 212/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0551 - accuracy: 0.0000e+00 - val_loss: 0.0514 - val_accuracy: 0.0500 Epoch 213/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0549 - accuracy: 0.0000e+00 - val_loss: 0.0512 - val_accuracy: 0.0500 Epoch 214/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0547 - accuracy: 0.0000e+00 - val_loss: 0.0509 - val_accuracy: 0.0500 Epoch 215/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0545 - accuracy: 0.0000e+00 - val_loss: 0.0507 - val_accuracy: 0.0500 Epoch 216/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0541 - accuracy: 0.0000e+00 - val_loss: 0.0506 - val_accuracy: 0.0500 Epoch 217/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0540 - accuracy: 0.0000e+00 - val_loss: 0.0505 - val_accuracy: 0.0500 Epoch 218/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0538 - accuracy: 0.0000e+00 - val_loss: 0.0503 - val_accuracy: 0.0500 Epoch 219/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0536 - accuracy: 0.0000e+00 - val_loss: 0.0502 - val_accuracy: 0.0500 Epoch 220/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0533 - accuracy: 0.0000e+00 - val_loss: 0.0500 - val_accuracy: 0.0500 Epoch 221/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0531 - accuracy: 0.0000e+00 - val_loss: 0.0498 - val_accuracy: 0.0500 Epoch 222/400 3/3 [==============================] - 0s 22ms/step - loss: 0.0530 - accuracy: 0.0000e+00 - val_loss: 0.0496 - val_accuracy: 0.0500 Epoch 223/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0527 - accuracy: 0.0000e+00 - val_loss: 0.0495 - val_accuracy: 0.0500 Epoch 224/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0526 - accuracy: 0.0000e+00 - val_loss: 0.0494 - val_accuracy: 0.0500 Epoch 225/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0523 - accuracy: 0.0000e+00 - val_loss: 0.0494 - val_accuracy: 0.0500 Epoch 226/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0521 - accuracy: 0.0000e+00 - val_loss: 0.0494 - val_accuracy: 0.0500 Epoch 227/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0519 - accuracy: 0.0000e+00 - val_loss: 0.0493 - val_accuracy: 0.0500 Epoch 228/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0518 - accuracy: 0.0000e+00 - val_loss: 0.0492 - val_accuracy: 0.0500 Epoch 229/400 3/3 [==============================] - 0s 21ms/step - loss: 0.0516 - accuracy: 0.0000e+00 - val_loss: 0.0490 - val_accuracy: 0.0500 Epoch 230/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0514 - accuracy: 0.0000e+00 - val_loss: 0.0488 - val_accuracy: 0.0500 Epoch 231/400 3/3 [==============================] - 0s 37ms/step - loss: 0.0512 - accuracy: 0.0000e+00 - val_loss: 0.0486 - val_accuracy: 0.0500 Epoch 232/400 3/3 [==============================] - 0s 36ms/step - loss: 0.0510 - accuracy: 0.0000e+00 - val_loss: 0.0484 - val_accuracy: 0.0500 Epoch 233/400 3/3 [==============================] - 0s 19ms/step - loss: 0.0508 - accuracy: 0.0000e+00 - val_loss: 0.0483 - val_accuracy: 0.0500 Epoch 234/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0507 - accuracy: 0.0000e+00 - val_loss: 0.0481 - val_accuracy: 0.0500 Epoch 235/400 3/3 [==============================] - 0s 41ms/step - loss: 0.0505 - accuracy: 0.0000e+00 - val_loss: 0.0480 - val_accuracy: 0.0500 Epoch 236/400 3/3 [==============================] - 0s 43ms/step - loss: 0.0504 - accuracy: 0.0000e+00 - val_loss: 0.0480 - val_accuracy: 0.0500 Epoch 237/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0502 - accuracy: 0.0000e+00 - val_loss: 0.0478 - val_accuracy: 0.0500 Epoch 238/400 3/3 [==============================] - 0s 36ms/step - loss: 0.0499 - accuracy: 0.0000e+00 - val_loss: 0.0477 - val_accuracy: 0.0500 Epoch 239/400 3/3 [==============================] - 0s 36ms/step - loss: 0.0498 - accuracy: 0.0000e+00 - val_loss: 0.0477 - val_accuracy: 0.0500 Epoch 240/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0496 - accuracy: 0.0000e+00 - val_loss: 0.0475 - val_accuracy: 0.0500 Epoch 241/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0494 - accuracy: 0.0000e+00 - val_loss: 0.0474 - val_accuracy: 0.0500 Epoch 242/400 3/3 [==============================] - 0s 22ms/step - loss: 0.0493 - accuracy: 0.0000e+00 - val_loss: 0.0473 - val_accuracy: 0.0500 Epoch 243/400 3/3 [==============================] - 0s 21ms/step - loss: 0.0492 - accuracy: 0.0000e+00 - val_loss: 0.0472 - val_accuracy: 0.0500 Epoch 244/400 3/3 [==============================] - 0s 23ms/step - loss: 0.0489 - accuracy: 0.0000e+00 - val_loss: 0.0472 - val_accuracy: 0.0500 Epoch 245/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0488 - accuracy: 0.0000e+00 - val_loss: 0.0472 - val_accuracy: 0.0500 Epoch 246/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0487 - accuracy: 0.0000e+00 - val_loss: 0.0473 - val_accuracy: 0.0500 Epoch 247/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0485 - accuracy: 0.0000e+00 - val_loss: 0.0472 - val_accuracy: 0.0500 Epoch 248/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0483 - accuracy: 0.0000e+00 - val_loss: 0.0471 - val_accuracy: 0.0500 Epoch 249/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0482 - accuracy: 0.0000e+00 - val_loss: 0.0470 - val_accuracy: 0.0500 Epoch 250/400 3/3 [==============================] - 0s 13ms/step - loss: 0.0481 - accuracy: 0.0000e+00 - val_loss: 0.0470 - val_accuracy: 0.0500 Epoch 251/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0479 - accuracy: 0.0000e+00 - val_loss: 0.0469 - val_accuracy: 0.0500 Epoch 252/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0478 - accuracy: 0.0000e+00 - val_loss: 0.0468 - val_accuracy: 0.0500 Epoch 253/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0477 - accuracy: 0.0000e+00 - val_loss: 0.0467 - val_accuracy: 0.0500 Epoch 254/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0475 - accuracy: 0.0000e+00 - val_loss: 0.0465 - val_accuracy: 0.0500 Epoch 255/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0474 - accuracy: 0.0000e+00 - val_loss: 0.0463 - val_accuracy: 0.0500 Epoch 256/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0472 - accuracy: 0.0000e+00 - val_loss: 0.0462 - val_accuracy: 0.0500 Epoch 257/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0471 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 258/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0470 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 259/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0468 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 260/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0467 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 261/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0466 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 262/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0466 - accuracy: 0.0000e+00 - val_loss: 0.0459 - val_accuracy: 0.0500 Epoch 263/400 3/3 [==============================] - 0s 22ms/step - loss: 0.0465 - accuracy: 0.0000e+00 - val_loss: 0.0457 - val_accuracy: 0.0500 Epoch 264/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0463 - accuracy: 0.0000e+00 - val_loss: 0.0455 - val_accuracy: 0.0500 Epoch 265/400 3/3 [==============================] - 0s 39ms/step - loss: 0.0462 - accuracy: 0.0000e+00 - val_loss: 0.0453 - val_accuracy: 0.0500 Epoch 266/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0461 - accuracy: 0.0000e+00 - val_loss: 0.0451 - val_accuracy: 0.0500 Epoch 267/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0460 - accuracy: 0.0000e+00 - val_loss: 0.0449 - val_accuracy: 0.0500 Epoch 268/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0458 - accuracy: 0.0000e+00 - val_loss: 0.0449 - val_accuracy: 0.0500 Epoch 269/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0457 - accuracy: 0.0000e+00 - val_loss: 0.0449 - val_accuracy: 0.0500 Epoch 270/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0455 - accuracy: 0.0000e+00 - val_loss: 0.0447 - val_accuracy: 0.0500 Epoch 271/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0454 - accuracy: 0.0000e+00 - val_loss: 0.0445 - val_accuracy: 0.0500 Epoch 272/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0454 - accuracy: 0.0000e+00 - val_loss: 0.0444 - val_accuracy: 0.0500 Epoch 273/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0453 - accuracy: 0.0000e+00 - val_loss: 0.0443 - val_accuracy: 0.0500 Epoch 274/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0452 - accuracy: 0.0000e+00 - val_loss: 0.0442 - val_accuracy: 0.0500 Epoch 275/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0450 - accuracy: 0.0000e+00 - val_loss: 0.0441 - val_accuracy: 0.0500 Epoch 276/400 3/3 [==============================] - 0s 19ms/step - loss: 0.0449 - accuracy: 0.0000e+00 - val_loss: 0.0440 - val_accuracy: 0.0500 Epoch 277/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0449 - accuracy: 0.0000e+00 - val_loss: 0.0439 - val_accuracy: 0.0500 Epoch 278/400 3/3 [==============================] - 0s 21ms/step - loss: 0.0447 - accuracy: 0.0000e+00 - val_loss: 0.0438 - val_accuracy: 0.0500 Epoch 279/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0446 - accuracy: 0.0000e+00 - val_loss: 0.0437 - val_accuracy: 0.0500 Epoch 280/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0445 - accuracy: 0.0000e+00 - val_loss: 0.0436 - val_accuracy: 0.0500 Epoch 281/400 3/3 [==============================] - 0s 19ms/step - loss: 0.0444 - accuracy: 0.0000e+00 - val_loss: 0.0436 - val_accuracy: 0.0500 Epoch 282/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0443 - accuracy: 0.0000e+00 - val_loss: 0.0436 - val_accuracy: 0.0500 Epoch 283/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0442 - accuracy: 0.0000e+00 - val_loss: 0.0435 - val_accuracy: 0.0500 Epoch 284/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0441 - accuracy: 0.0000e+00 - val_loss: 0.0434 - val_accuracy: 0.0500 Epoch 285/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0440 - accuracy: 0.0000e+00 - val_loss: 0.0434 - val_accuracy: 0.0500 Epoch 286/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0440 - accuracy: 0.0000e+00 - val_loss: 0.0433 - val_accuracy: 0.0500 Epoch 287/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0438 - accuracy: 0.0000e+00 - val_loss: 0.0433 - val_accuracy: 0.0500 Epoch 288/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0438 - accuracy: 0.0000e+00 - val_loss: 0.0432 - val_accuracy: 0.0500 Epoch 289/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0436 - accuracy: 0.0000e+00 - val_loss: 0.0431 - val_accuracy: 0.0500 Epoch 290/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0435 - accuracy: 0.0000e+00 - val_loss: 0.0430 - val_accuracy: 0.0500 Epoch 291/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0434 - accuracy: 0.0000e+00 - val_loss: 0.0429 - val_accuracy: 0.0500 Epoch 292/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0433 - accuracy: 0.0000e+00 - val_loss: 0.0429 - val_accuracy: 0.0500 Epoch 293/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0433 - accuracy: 0.0000e+00 - val_loss: 0.0429 - val_accuracy: 0.0500 Epoch 294/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0432 - accuracy: 0.0000e+00 - val_loss: 0.0429 - val_accuracy: 0.0500 Epoch 295/400 3/3 [==============================] - 0s 19ms/step - loss: 0.0431 - accuracy: 0.0000e+00 - val_loss: 0.0426 - val_accuracy: 0.0500 Epoch 296/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0430 - accuracy: 0.0000e+00 - val_loss: 0.0424 - val_accuracy: 0.0500 Epoch 297/400 3/3 [==============================] - 0s 23ms/step - loss: 0.0429 - accuracy: 0.0000e+00 - val_loss: 0.0424 - val_accuracy: 0.0500 Epoch 298/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0428 - accuracy: 0.0000e+00 - val_loss: 0.0423 - val_accuracy: 0.0500 Epoch 299/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0427 - accuracy: 0.0000e+00 - val_loss: 0.0423 - val_accuracy: 0.0500 Epoch 300/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0426 - accuracy: 0.0000e+00 - val_loss: 0.0422 - val_accuracy: 0.0500 Epoch 301/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0425 - accuracy: 0.0000e+00 - val_loss: 0.0421 - val_accuracy: 0.0500 Epoch 302/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0424 - accuracy: 0.0000e+00 - val_loss: 0.0422 - val_accuracy: 0.0500 Epoch 303/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0424 - accuracy: 0.0000e+00 - val_loss: 0.0423 - val_accuracy: 0.0500 Epoch 304/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0423 - accuracy: 0.0000e+00 - val_loss: 0.0422 - val_accuracy: 0.0500 Epoch 305/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0422 - accuracy: 0.0000e+00 - val_loss: 0.0421 - val_accuracy: 0.0500 Epoch 306/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0422 - accuracy: 0.0000e+00 - val_loss: 0.0418 - val_accuracy: 0.0500 Epoch 307/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0421 - accuracy: 0.0000e+00 - val_loss: 0.0417 - val_accuracy: 0.0500 Epoch 308/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0419 - accuracy: 0.0000e+00 - val_loss: 0.0417 - val_accuracy: 0.0500 Epoch 309/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0419 - accuracy: 0.0000e+00 - val_loss: 0.0417 - val_accuracy: 0.0500 Epoch 310/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0418 - accuracy: 0.0000e+00 - val_loss: 0.0416 - val_accuracy: 0.0500 Epoch 311/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0417 - accuracy: 0.0000e+00 - val_loss: 0.0414 - val_accuracy: 0.0500 Epoch 312/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0417 - accuracy: 0.0000e+00 - val_loss: 0.0412 - val_accuracy: 0.0500 Epoch 313/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0416 - accuracy: 0.0000e+00 - val_loss: 0.0412 - val_accuracy: 0.0500 Epoch 314/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0416 - accuracy: 0.0000e+00 - val_loss: 0.0412 - val_accuracy: 0.0500 Epoch 315/400 3/3 [==============================] - 0s 22ms/step - loss: 0.0414 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 316/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0413 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 317/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0413 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 318/400 3/3 [==============================] - 0s 92ms/step - loss: 0.0411 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 319/400 3/3 [==============================] - 0s 31ms/step - loss: 0.0411 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 320/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0410 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 321/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0410 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 322/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0409 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 323/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0408 - accuracy: 0.0000e+00 - val_loss: 0.0406 - val_accuracy: 0.0500 Epoch 324/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0407 - accuracy: 0.0000e+00 - val_loss: 0.0405 - val_accuracy: 0.0500 Epoch 325/400 3/3 [==============================] - 0s 23ms/step - loss: 0.0407 - accuracy: 0.0000e+00 - val_loss: 0.0404 - val_accuracy: 0.0500 Epoch 326/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0406 - accuracy: 0.0000e+00 - val_loss: 0.0404 - val_accuracy: 0.0500 Epoch 327/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0405 - accuracy: 0.0000e+00 - val_loss: 0.0404 - val_accuracy: 0.0500 Epoch 328/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0406 - accuracy: 0.0000e+00 - val_loss: 0.0404 - val_accuracy: 0.0500 Epoch 329/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0404 - accuracy: 0.0000e+00 - val_loss: 0.0402 - val_accuracy: 0.0500 Epoch 330/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0403 - accuracy: 0.0000e+00 - val_loss: 0.0401 - val_accuracy: 0.0500 Epoch 331/400 3/3 [==============================] - 0s 31ms/step - loss: 0.0402 - accuracy: 0.0000e+00 - val_loss: 0.0402 - val_accuracy: 0.0500 Epoch 332/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0402 - accuracy: 0.0000e+00 - val_loss: 0.0402 - val_accuracy: 0.0500 Epoch 333/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0401 - accuracy: 0.0000e+00 - val_loss: 0.0405 - val_accuracy: 0.0500 Epoch 334/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0401 - accuracy: 0.0000e+00 - val_loss: 0.0408 - val_accuracy: 0.0500 Epoch 335/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0401 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 336/400 3/3 [==============================] - 0s 37ms/step - loss: 0.0400 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 337/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0399 - accuracy: 0.0000e+00 - val_loss: 0.0405 - val_accuracy: 0.0500 Epoch 338/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0398 - accuracy: 0.0000e+00 - val_loss: 0.0401 - val_accuracy: 0.0500 Epoch 339/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0397 - accuracy: 0.0000e+00 - val_loss: 0.0400 - val_accuracy: 0.0500 Epoch 340/400 3/3 [==============================] - 0s 40ms/step - loss: 0.0396 - accuracy: 0.0000e+00 - val_loss: 0.0400 - val_accuracy: 0.0500 Epoch 341/400 3/3 [==============================] - 0s 53ms/step - loss: 0.0395 - accuracy: 0.0000e+00 - val_loss: 0.0399 - val_accuracy: 0.0500 Epoch 342/400 3/3 [==============================] - 0s 54ms/step - loss: 0.0395 - accuracy: 0.0000e+00 - val_loss: 0.0399 - val_accuracy: 0.0500 Epoch 343/400 3/3 [==============================] - 0s 45ms/step - loss: 0.0394 - accuracy: 0.0000e+00 - val_loss: 0.0398 - val_accuracy: 0.0500 Epoch 344/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0393 - accuracy: 0.0000e+00 - val_loss: 0.0398 - val_accuracy: 0.0500 Epoch 345/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0393 - accuracy: 0.0000e+00 - val_loss: 0.0397 - val_accuracy: 0.0500 Epoch 346/400 3/3 [==============================] - 0s 53ms/step - loss: 0.0392 - accuracy: 0.0000e+00 - val_loss: 0.0396 - val_accuracy: 0.0500 Epoch 347/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0391 - accuracy: 0.0000e+00 - val_loss: 0.0395 - val_accuracy: 0.0500 Epoch 348/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0391 - accuracy: 0.0000e+00 - val_loss: 0.0394 - val_accuracy: 0.0500 Epoch 349/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0390 - accuracy: 0.0000e+00 - val_loss: 0.0394 - val_accuracy: 0.0500 Epoch 350/400 3/3 [==============================] - 0s 34ms/step - loss: 0.0390 - accuracy: 0.0000e+00 - val_loss: 0.0394 - val_accuracy: 0.0500 Epoch 351/400 3/3 [==============================] - 0s 90ms/step - loss: 0.0389 - accuracy: 0.0000e+00 - val_loss: 0.0393 - val_accuracy: 0.0500 Epoch 352/400 3/3 [==============================] - 0s 41ms/step - loss: 0.0388 - accuracy: 0.0000e+00 - val_loss: 0.0392 - val_accuracy: 0.0500 Epoch 353/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0389 - accuracy: 0.0000e+00 - val_loss: 0.0392 - val_accuracy: 0.0500 Epoch 354/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0387 - accuracy: 0.0000e+00 - val_loss: 0.0391 - val_accuracy: 0.0500 Epoch 355/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0386 - accuracy: 0.0000e+00 - val_loss: 0.0390 - val_accuracy: 0.0500 Epoch 356/400 3/3 [==============================] - 0s 61ms/step - loss: 0.0386 - accuracy: 0.0000e+00 - val_loss: 0.0389 - val_accuracy: 0.0500 Epoch 357/400 3/3 [==============================] - 0s 58ms/step - loss: 0.0386 - accuracy: 0.0000e+00 - val_loss: 0.0390 - val_accuracy: 0.0500 Epoch 358/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0386 - accuracy: 0.0000e+00 - val_loss: 0.0389 - val_accuracy: 0.0500 Epoch 359/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0385 - accuracy: 0.0000e+00 - val_loss: 0.0388 - val_accuracy: 0.0500 Epoch 360/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0384 - accuracy: 0.0000e+00 - val_loss: 0.0387 - val_accuracy: 0.0500 Epoch 361/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0384 - accuracy: 0.0000e+00 - val_loss: 0.0388 - val_accuracy: 0.0500 Epoch 362/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0382 - accuracy: 0.0000e+00 - val_loss: 0.0389 - val_accuracy: 0.0500 Epoch 363/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0382 - accuracy: 0.0000e+00 - val_loss: 0.0391 - val_accuracy: 0.0500 Epoch 364/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0381 - accuracy: 0.0000e+00 - val_loss: 0.0391 - val_accuracy: 0.0500 Epoch 365/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0381 - accuracy: 0.0000e+00 - val_loss: 0.0390 - val_accuracy: 0.0500 Epoch 366/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0380 - accuracy: 0.0000e+00 - val_loss: 0.0390 - val_accuracy: 0.0500 Epoch 367/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0380 - accuracy: 0.0000e+00 - val_loss: 0.0388 - val_accuracy: 0.0500 Epoch 368/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0379 - accuracy: 0.0000e+00 - val_loss: 0.0386 - val_accuracy: 0.0500 Epoch 369/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0379 - accuracy: 0.0000e+00 - val_loss: 0.0386 - val_accuracy: 0.0500 Epoch 370/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0379 - accuracy: 0.0000e+00 - val_loss: 0.0384 - val_accuracy: 0.0500 Epoch 371/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0377 - accuracy: 0.0000e+00 - val_loss: 0.0384 - val_accuracy: 0.0500 Epoch 372/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0377 - accuracy: 0.0000e+00 - val_loss: 0.0383 - val_accuracy: 0.0500 Epoch 373/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0376 - accuracy: 0.0000e+00 - val_loss: 0.0382 - val_accuracy: 0.0500 Epoch 374/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0376 - accuracy: 0.0000e+00 - val_loss: 0.0381 - val_accuracy: 0.0500 Epoch 375/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0375 - accuracy: 0.0000e+00 - val_loss: 0.0380 - val_accuracy: 0.0500 Epoch 376/400 3/3 [==============================] - 0s 42ms/step - loss: 0.0375 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 377/400 3/3 [==============================] - 0s 34ms/step - loss: 0.0375 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 378/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0374 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 379/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0373 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 380/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0372 - accuracy: 0.0000e+00 - val_loss: 0.0381 - val_accuracy: 0.0500 Epoch 381/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0372 - accuracy: 0.0000e+00 - val_loss: 0.0383 - val_accuracy: 0.0500 Epoch 382/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0372 - accuracy: 0.0000e+00 - val_loss: 0.0382 - val_accuracy: 0.0500 Epoch 383/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0371 - accuracy: 0.0000e+00 - val_loss: 0.0381 - val_accuracy: 0.0500 Epoch 384/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0371 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 385/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0370 - accuracy: 0.0000e+00 - val_loss: 0.0377 - val_accuracy: 0.0500 Epoch 386/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0371 - accuracy: 0.0000e+00 - val_loss: 0.0376 - val_accuracy: 0.0500 Epoch 387/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0369 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 388/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0369 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 389/400 3/3 [==============================] - 0s 31ms/step - loss: 0.0368 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 390/400 3/3 [==============================] - 0s 47ms/step - loss: 0.0368 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 391/400 3/3 [==============================] - 0s 63ms/step - loss: 0.0367 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 392/400 3/3 [==============================] - 0s 57ms/step - loss: 0.0367 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 393/400 3/3 [==============================] - 0s 54ms/step - loss: 0.0366 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 394/400 3/3 [==============================] - 0s 54ms/step - loss: 0.0366 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 395/400 3/3 [==============================] - 0s 54ms/step - loss: 0.0365 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 396/400 3/3 [==============================] - 0s 49ms/step - loss: 0.0365 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 397/400 3/3 [==============================] - 0s 51ms/step - loss: 0.0364 - accuracy: 0.0000e+00 - val_loss: 0.0373 - val_accuracy: 0.0500 Epoch 398/400 3/3 [==============================] - 0s 52ms/step - loss: 0.0364 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 399/400 3/3 [==============================] - 0s 48ms/step - loss: 0.0364 - accuracy: 0.0000e+00 - val_loss: 0.0373 - val_accuracy: 0.0500 Epoch 400/400 3/3 [==============================] - 0s 56ms/step - loss: 0.0363 - accuracy: 0.0000e+00 - val_loss: 0.0371 - val_accuracy: 0.0500 . . Results and Visualization . results = model.predict(x_test) . plt.scatter(range(20), results, c=&#39;r&#39;) plt.scatter(range(20), y_test,c=&#39;g&#39;) plt.show() . plt.plot(history.history[&#39;loss&#39;]) plt.show() #### The loss nearly stagnates during the later stages - therefore the model would still produce a satisfactory output at epochs~300 .",
            "url": "https://kunal-bhar.github.io/blog/python/math/lstm/visualizations/2022/03/11/concurrent-num-prognosis.html",
            "relUrl": "/python/math/lstm/visualizations/2022/03/11/concurrent-num-prognosis.html",
            "date": " • Mar 11, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Shakespearean English Semantics",
            "content": "Project At A Glance . Objective: Evaluate and visualize relationships between annotations of the era&#39;s literature. . Data: Compiled Shakespeare Dataset [Download] . Implementation: Word2Vec, Word Embeddings, Principal Component Analysis . Results: . 100-parameter vectorized representation of every word in the corpus. | Computed similarility scores for distinct words of the time period relative to today&#39;s English language. | Scatter plots to visualize Word-Embeddings and their Principal Components. | . Deployment: View this project on GitHub. . Dependencies . import pandas as pd import nltk import string import matplotlib.pyplot as plt from nltk.corpus import stopwords from nltk import word_tokenize from gensim.models import Word2Vec as w2v from sklearn.decomposition import PCA . Initialization . PATH = &#39;ShakespeareDataset.txt&#39; sw = stopwords.words(&#39;english&#39;) plt.style.use(&#39;ggplot&#39;) . %%time lines = [] with open(PATH, &#39;r&#39;) as f: for l in f: lines.append(l) . Wall time: 70.5 ms . Text Pre-Processing . lines = [line.rstrip(&#39; n&#39;) for line in lines] lines = [line.lower() for line in lines] lines = [line.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation)) for line in lines] . %time lines = [word_tokenize(line) for line in lines] . Wall time: 15.3 s . def remove_stopwords(lines, sw = sw): res = [] for line in lines: original = line line = [w for w in line if w not in sw] if len(line) &lt; 1: line = original res.append(line) return res . %time filtered_lines = remove_stopwords(lines = lines, sw = sw) . Wall time: 2.03 s . Custom Word2Vec and Word-Similarity . %%time w = w2v( filtered_lines, min_count=3, sg = 1, window=7 ) . Wall time: 3.22 s . w.wv.most_similar(&#39;thou&#39;) . [(&#39;art&#39;, 0.8374333381652832), (&#39;thyself&#39;, 0.8162680864334106), (&#39;dost&#39;, 0.7874499559402466), (&#39;villain&#39;, 0.7856082320213318), (&#39;kill&#39;, 0.733100950717926), (&#39;hast&#39;, 0.7226855158805847), (&#39;wilt&#39;, 0.7046181559562683), (&#39;didst&#39;, 0.6970406770706177), (&#39;fellow&#39;, 0.696016788482666), (&#39;traitor&#39;, 0.6928953528404236)] . w.wv.most_similar(&#39;shall&#39;) . [(&#39;may&#39;, 0.8649993538856506), (&#39;could&#39;, 0.8336020708084106), (&#39;youll&#39;, 0.8054620623588562), (&#39;doth&#39;, 0.8045750856399536), (&#39;till&#39;, 0.7994358539581299), (&#39;business&#39;, 0.7948688864707947), (&#39;ill&#39;, 0.7912845611572266), (&#39;dare&#39;, 0.7817249298095703), (&#39;let&#39;, 0.7762875556945801), (&#39;might&#39;, 0.776180624961853)] . w.wv.most_similar(&#39;abhor&#39;) . [(&#39;revenue&#39;, 0.9962968826293945), (&#39;exercise&#39;, 0.9959706664085388), (&#39;wedlock&#39;, 0.9957810640335083), (&#39;fever&#39;, 0.995772659778595), (&#39;painting&#39;, 0.9955847263336182), (&#39;arthurs&#39;, 0.9955565929412842), (&#39;touched&#39;, 0.9955466985702515), (&#39;purgation&#39;, 0.9953007698059082), (&#39;devotion&#39;, 0.9951791763305664), (&#39;havior&#39;, 0.9951609373092651)] . w.wv.most_similar(&#39;vile&#39;) . [(&#39;form&#39;, 0.9728872776031494), (&#39;monstrous&#39;, 0.9698807597160339), (&#39;smiling&#39;, 0.9687707424163818), (&#39;merit&#39;, 0.9668200016021729), (&#39;savage&#39;, 0.9668026566505432), (&#39;blown&#39;, 0.9667961597442627), (&#39;tremble&#39;, 0.9661442041397095), (&#39;worm&#39;, 0.9657843112945557), (&#39;quite&#39;, 0.9650156497955322), (&#39;giddy&#39;, 0.9649045467376709)] . Generate Embedding DataFrame . %%time emb_df = ( pd.DataFrame( [w.wv.get_vector(str(n)) for n in w.wv.key_to_index], index = w.wv.key_to_index ) ) . Wall time: 797 ms . emb_df.shape . (11628, 100) . emb_df.head() . 0 1 2 3 4 5 6 7 8 9 ... 90 91 92 93 94 95 96 97 98 99 . thou 0.048444 | -0.179371 | 0.656354 | 0.553357 | -0.390886 | -0.399313 | 0.408025 | 0.459145 | -0.405728 | -0.201803 | ... | 0.168611 | 0.248811 | 0.342443 | -0.067844 | 1.087360 | 0.751683 | -0.544332 | -0.557544 | 0.064042 | 0.441012 | . thy 0.025636 | 0.257252 | 0.512183 | 0.255503 | 0.298671 | -0.194974 | 0.258179 | 0.727472 | -0.141695 | -0.345477 | ... | 0.183475 | -0.138357 | 0.696960 | -0.215607 | 0.402519 | 0.359478 | 0.072248 | -0.589917 | 0.021417 | 0.011141 | . shall -0.059704 | 0.162047 | -0.045949 | -0.057693 | 0.579027 | -0.023031 | 0.043291 | 0.445429 | -0.326463 | 0.111084 | ... | 0.486898 | 0.051414 | 0.056035 | -0.147839 | 0.489713 | 0.178388 | 0.121662 | -0.007791 | 0.305106 | 0.014491 | . thee -0.437681 | 0.170327 | 0.516488 | 0.250964 | 0.051163 | -0.085569 | 0.174727 | 0.692341 | -0.266182 | -0.164628 | ... | 0.455216 | 0.022709 | 0.256135 | -0.054368 | 0.635989 | 0.482457 | -0.008304 | -0.079286 | 0.276732 | 0.008572 | . good 0.068176 | 0.297002 | 0.267275 | -0.144337 | 0.127091 | 0.131793 | 0.484557 | 0.709339 | -0.335214 | 0.117829 | ... | 0.368580 | -0.192079 | 0.248281 | -0.377771 | 0.294786 | -0.042967 | 0.284816 | -0.195110 | 0.345721 | -0.013074 | . 5 rows × 100 columns . Word-Embedding Visualization . plt.clf() fig=plt.figure(figsize=(6,4)) plt.scatter( x = emb_df.iloc[:,0], y = emb_df.iloc[:,1], s = 0.2, color = &#39;maroon&#39;, alpha = 0.5 ) plt.title(&#39;Embedding Visualizations&#39;) plt.show() . &lt;Figure size 432x288 with 0 Axes&gt; . Principal Component Analysis . Processing . pca = PCA(n_components=2, random_state=7) pca_model = pca.fit_transform(emb_df) . emb_df_PCA = ( pd.DataFrame( pca_model, columns=[&#39;x&#39;,&#39;y&#39;], index = emb_df.index ) ) . Visualization . plt.clf() fig = plt.figure(figsize=(6,4)) plt.scatter( x = emb_df_PCA[&#39;x&#39;], y = emb_df_PCA[&#39;y&#39;], s = 0.4, color = &#39;maroon&#39;, alpha = 0.5 ) plt.xlabel(&#39;PCA-1&#39;) plt.ylabel(&#39;PCA-2&#39;) plt.title(&#39;PCA Visualization&#39;) plt.plot() plt.show() . &lt;Figure size 432x288 with 0 Axes&gt; .",
            "url": "https://kunal-bhar.github.io/blog/nlp/python/english/visualizations/2022/03/10/shakespeare-semantics.html",
            "relUrl": "/nlp/python/english/visualizations/2022/03/10/shakespeare-semantics.html",
            "date": " • Mar 10, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "probplotlib- Open Source Python Library",
            "content": "Project At A Glance . Objective: Higher-order mathematical operations using Python3. . Implementation: A lightwight library capable of parsing data directly as Probability Distributions - available as a Python installation through pip. . Results: . Concise calculations that eliminate double-precision. | Compatibility with external datasets (.txt). | Faster than NumPy operatives by ~13%. | . Deployment: View this project on the Python Package Index or GitHub. . Probability Distributions for Python . . The Statistical Void . Stats can get tricky in the transition from plotting fun graphs to advanced algebraic equations. A classic example is the given sum: . 1.0e14 + 1.0 - 1.0e14 . The actual result is 1.0 but in double precision, this will result in 0.0. While in this example the failure is quite obvious, it can get a lot trickier than that. Instances like these hinder the community from exploring the inferential potential of complex entities. . p=Gaussian(a,b) q=Gaussian(x,y) p+q . This snippet would be close to useless as python addition doesn’t isn’t attributed for higher-level declarables such as Gaussian variables. probplotlib provides simple solutions for probability distributions; posing a highly-optimized alternative to numpy and math, in a niche that is scarce in options. . Usage . probplotlib has the following operative methods: . + : uses Dunder Methods for facilitating dist-additions. . | calculate_mean(): returns the mean of a distribution. . | . gaussianex = Gaussian() calculate_mean(gaussianx) . calculate_stdev(): returns the standard deviation of a distribution. | . binomialex = Binomial() calculate_stdev(binomialex) . read_dataset(): reads an external .txt dataset directly as a distribution. | . gaussianex.read_dataset(&#39;values.txt&#39;) binomialex.read_dataset(&#39;values.txt&#39;) . params(): retrieves the identity parameters of an imported dataset. | . gaussianex.params() binomialex.params() . pdf(): returns the probability density function at a given point. | . pdf(gaussianex, 2) . functions unique to Gaussian Distributions: . plot_histogram(): uses matplotlib to display a histogram of the Gaussian Distribution. | . gaussianex.plot_histogram() . plot_histogram_pdf(): uses matplotlib to display a co-relative plot along with the Gaussian probability density function. | . gaussianex.plot_histogram_pdf() . functions unique to Binomial Distributions: . plot_bar(): uses matplotlib to display a bar graph of the Binomial Distribution. | . binomialex.plot_bar() . plot_bar_pdf(): uses matplotlib to display a co-relative plot along with the Binomial probability density function. | . binomialex.plot_bar_pdf() . Data Visualization . probplotlib therefore allows you to analyze raw numerical data graphically in minimial lines of code. The example below makes for better understanding. . . a bag of numbers in a .txt file corresponds to the following plots: . histogram plot: . . bar plot: . . histogram plot with pdf: . . References . Stanford Archives: CS109- The Normal(Gaussian) Distribution . A Practical Overview on Probability Distributions: Andrea Viti, Alberto Terzi, Luca Bertolaccini . Awesome Scientific Computing: Nico Schlömer, GitHub Repository . math.statistics: Python 3.10 Source Code . Stack Overflow . Dependencies . probplotlib depends on the matplotlib library on top of your regular python installation. . pip install matplotlib . or . conda install matplotlib . Installation . probplotlib is available on the Python Package Index. You can install it directly using pip. . pip install probplotlib . Testing . To run the tests, simply check to this directory and run the code below. . python -m unittest test_probplotlib .",
            "url": "https://kunal-bhar.github.io/blog/python/math/probability/visualizations/2021/10/28/probplotlib.html",
            "relUrl": "/python/math/probability/visualizations/2021/10/28/probplotlib.html",
            "date": " • Oct 28, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://kunal-bhar.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://kunal-bhar.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://kunal-bhar.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kunal-bhar.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}