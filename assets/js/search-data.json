{
  
    
        "post0": {
            "title": "Reddit Headline Analysis",
            "content": "Project At A Glance . Objective: Discover sentiments associated with posts in the &#39;Hot&#39; section of r/worldnews and classify them as Positive, Negative and Neutral. . Data: 754x1 Dataset of the sub-reddit&#39;s headlines scraped using the Reddit API. [View Scraper Notebook] [Download] . Implementation: Reddit API, PRAW, NLTK&#39;s Sentiment Intensity Analyzer (SIA) . Results: . More than half of the headlines were classified as Neutral (~55%). | However, Negative Headlines (33%) still outweigh Positive Headlines (12%) by about 2.75x. | Dataset generated with labelled values to formulate models with better intelligence in the future. | . Deployment: View this project on GitHub. . Dependencies . import pandas as pd from pprint import pprint . Dataset Initialization . df = pd.read_csv(&#39;reddit-headlines.csv&#39;) . df.head() . Unnamed: 0 headlines . 0 0 | Mass graves dug in the besieged Ukrainian city... | . 1 1 | British aircraft carrier leading massive fleet... | . 2 2 | Spain detains $600 million yacht linked to Rus... | . 3 3 | Photo shows officials taking down the Russian ... | . 4 4 | Marina Ovsyannikova: Russian journalist tells ... | . Using NLTK&#39;s Sentiment Intensity Analyzer . import nltk from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA . import matplotlib.pyplot as plt import seaborn as sns . sia= SIA() results = [] for line in df[&#39;headlines&#39;]: pol_score = sia.polarity_scores(line) pol_score[&#39;headline&#39;] = line results.append(pol_score) pprint(results[:3], width=100) . [{&#39;compound&#39;: -0.7579, &#39;headline&#39;: &#39;Mass graves dug in the besieged Ukrainian city of Mariupol, as locals bury their &#39; &#39;dead&#39;, &#39;neg&#39;: 0.333, &#39;neu&#39;: 0.667, &#39;pos&#39;: 0.0}, {&#39;compound&#39;: 0.0, &#39;headline&#39;: &#39;British aircraft carrier leading massive fleet off Norway&#39;, &#39;neg&#39;: 0.0, &#39;neu&#39;: 1.0, &#39;pos&#39;: 0.0}, {&#39;compound&#39;: 0.0, &#39;headline&#39;: &#39;Spain detains $600 million yacht linked to Russian oligarch: Reuters&#39;, &#39;neg&#39;: 0.0, &#39;neu&#39;: 1.0, &#39;pos&#39;: 0.0}] . Generating DataFrame with Polarity Scores . df = pd.DataFrame.from_records(results) . df.sample(4) . neg neu pos compound headline . 380 0.506 | 0.494 | 0.000 | -0.9231 | Tigray war has seen up to half a million dead ... | . 22 0.289 | 0.711 | 0.000 | -0.4291 | &#39;Why? Why? Why?&#39; Ukraine&#39;s Mariupol descends i... | . 660 0.000 | 0.806 | 0.194 | 0.3400 | Ukraine&#39;s &#39;hero&#39; President Zelensky set to rec... | . 725 0.100 | 0.594 | 0.306 | 0.6705 | There is no life for Ukrainian people: Boxing ... | . Labelling and Classification . df[&#39;label&#39;] = 0 df.loc[df[&#39;compound&#39;]&gt;0.33, &#39;label&#39;] = 1 df.loc[df[&#39;compound&#39;]&lt;-0.33, &#39;label&#39;] = -1 . df.sample(4) . neg neu pos compound headline label . 461 0.273 | 0.727 | 0.000 | -0.4588 | Trudeau and almost every Canadian MP banned fr... | -1 | . 482 0.000 | 0.647 | 0.353 | 0.5994 | Help yourself by helping us, Ukraine&#39;s Zelensk... | 1 | . 420 0.000 | 1.000 | 0.000 | 0.0000 | Russia, India explore opening alternative paym... | 0 | . 748 0.239 | 0.761 | 0.000 | -0.2960 | Russia&#39;s state TV hit by stream of resignations | 0 | . df.label.value_counts() . 0 414 -1 247 1 93 Name: label, dtype: int64 . df.label.value_counts(normalize=True)*100 . 0 54.907162 -1 32.758621 1 12.334218 Name: label, dtype: float64 . Examples . print(&#39;Positive Headlines: n&#39;) pprint(list(df[df[&#39;label&#39;] == 1].headline)[:5], width=200) print(&#39; n n Negative Headlines: n&#39;) pprint(list(df[df[&#39;label&#39;] == -1].headline)[:5], width=200) print(&#39; n n Neutral Headlines: n&#39;) pprint(list(df[df[&#39;label&#39;] == 0].headline)[:5], width=200) . Positive Headlines: [&#39;Tibetans seek justice after 63 years of uprising against Chinese rule&#39;, &#34;The Ministry of Foreign Affairs on Tuesday (March 15) praised a Russian woman for her courage after she held up an anti-war sign on live Russian TV. MOFA head: &#39;It takes courage to be the voice of &#34; &#34;conscience&#39;.&#34;, &#39;Saudi Arabia considers accepting yuan for oil sales&#39;, &#39;Russia and Ukraine looking for compromise in peace talks&#39;, &#39;Turkmenistan leader’s son wins presidential election&#39;] Negative Headlines: [&#39;Mass graves dug in the besieged Ukrainian city of Mariupol, as locals bury their dead&#39;, &#34;Russia&#39;s former chief prosecutor says oligarch Roman Abramovich amassed his fortune through a &#39;fraudulent scheme&#39;&#34;, &#39;UN makes March 15 International Day to Combat Islamophobia&#39;, &#39;Not violation of sanctions but Russian oil deal could put India on wrong side of history, says US&#39;, &#34;&#39;Why? Why? Why?&#39; Ukraine&#39;s Mariupol descends into despair&#34;] Neutral Headlines: [&#39;British aircraft carrier leading massive fleet off Norway&#39;, &#39;Spain detains $600 million yacht linked to Russian oligarch: Reuters&#39;, &#39;Photo shows officials taking down the Russian flag after Putin gets the boot from Council of Europe&#39;, &#39;Marina Ovsyannikova: Russian journalist tells of 14-hour interrogation&#39;, &#39;China wary of being impacted by Russia sanctions: Foreign Minister&#39;] . Results and Visualization . fig, ax = plt.subplots(figsize=(8,8)) counts = df.label.value_counts(normalize=True)*100 sns.barplot(x=counts.index, y=counts, ax=ax) ax.set_xticklabels([&#39;Negative&#39;, &#39;Neutral&#39;, &#39;Positive&#39;]) ax.set_ylabel(&#39;Percentage&#39;) plt.show() . Exporting Labelled Dataset as (.csv) . df_export = df[[&#39;headline&#39;, &#39;label&#39;]] . df_export.sample(4) . headline label . 399 EU &#39;Concerned&#39; Over Disrupted Gas Supply, Shoo... | 0 | . 709 Woman fearing for family in Ukraine urges Cana... | -1 | . 618 New Zealand cuts fuel tax and halves public tr... | -1 | . 560 Slovakia meets NATO defence spending commitmen... | 1 | . df_export.to_csv(&#39;reddit-headlines-labelled.csv&#39;, encoding=&#39;utf-8&#39;, index=True) .",
            "url": "https://kunal-bhar.github.io/blog/nlp/python/reddit/news/2022/03/17/reddit-news-analysis.html",
            "relUrl": "/nlp/python/reddit/news/2022/03/17/reddit-news-analysis.html",
            "date": " • Mar 17, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Concurrent Number Prognosis",
            "content": "Project At A Glance . Objective: Predicting values in numeric data that sequentially lead by a given parameter: n. . Data: . Data: Group of 100 5x1 lists that contain overlapping adjacent values. Example: [[1:5], [2:6], [3:7]...] | Target: Group of 100 1x1 lists with the target values for generation. Example: [[1], [2], [3]...] | . Implementation: Long Short-Term Memory (LSTM), Recurrent Neural Networks . Results: . The LSTM computed f-score = 0.935 and loss = 0.036 on training for 400 epochs. | Scatter plots to visualize performance and loss deprecation. | . Deployment: View this project on GitHub. . Dependencies . import numpy as np import matplotlib.pyplot as plt from keras.models import Sequential from keras.layers import Dense, LSTM from sklearn.model_selection import train_test_split . Initialization . Data = [[[(i+j)/100] for i in range(5)] for j in range(100)] Target = [(i+5)/100 for i in range(100)] . Data as NumPy Arrays . data = np.array(Data, dtype=float) target = np.array(Target, dtype=float) . data.shape . (100, 5, 1) . target.shape . (100,) . Train-Test Split . x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=4) . Model Setup and Layers . model = Sequential() . model.add(LSTM((1), batch_input_shape=(None,5,1), return_sequences=True)) model.add(LSTM((1), return_sequences=False)) # the return_sequences parameter assists in enabling convolution for the sequential LSTM . model.compile(loss=&#39;mean_absolute_error&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm (LSTM) (None, 5, 1) 12 lstm_1 (LSTM) (None, 1) 12 ================================================================= Total params: 24 Trainable params: 24 Non-trainable params: 0 _________________________________________________________________ . history = model.fit(x_train, y_train, epochs=400, validation_data=(x_test, y_test)) . Epoch 1/400 3/3 [==============================] - 5s 474ms/step - loss: 0.5351 - accuracy: 0.0000e+00 - val_loss: 0.4311 - val_accuracy: 0.0000e+00 Epoch 2/400 3/3 [==============================] - 0s 27ms/step - loss: 0.5325 - accuracy: 0.0000e+00 - val_loss: 0.4286 - val_accuracy: 0.0000e+00 Epoch 3/400 3/3 [==============================] - 0s 25ms/step - loss: 0.5297 - accuracy: 0.0000e+00 - val_loss: 0.4260 - val_accuracy: 0.0000e+00 Epoch 4/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5270 - accuracy: 0.0000e+00 - val_loss: 0.4233 - val_accuracy: 0.0000e+00 Epoch 5/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5241 - accuracy: 0.0000e+00 - val_loss: 0.4207 - val_accuracy: 0.0000e+00 Epoch 6/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5213 - accuracy: 0.0000e+00 - val_loss: 0.4179 - val_accuracy: 0.0000e+00 Epoch 7/400 3/3 [==============================] - 0s 21ms/step - loss: 0.5184 - accuracy: 0.0000e+00 - val_loss: 0.4151 - val_accuracy: 0.0000e+00 Epoch 8/400 3/3 [==============================] - 0s 15ms/step - loss: 0.5154 - accuracy: 0.0000e+00 - val_loss: 0.4123 - val_accuracy: 0.0000e+00 Epoch 9/400 3/3 [==============================] - 0s 13ms/step - loss: 0.5123 - accuracy: 0.0000e+00 - val_loss: 0.4094 - val_accuracy: 0.0000e+00 Epoch 10/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5093 - accuracy: 0.0000e+00 - val_loss: 0.4064 - val_accuracy: 0.0000e+00 Epoch 11/400 3/3 [==============================] - 0s 14ms/step - loss: 0.5061 - accuracy: 0.0000e+00 - val_loss: 0.4034 - val_accuracy: 0.0000e+00 Epoch 12/400 3/3 [==============================] - 0s 15ms/step - loss: 0.5029 - accuracy: 0.0000e+00 - val_loss: 0.4003 - val_accuracy: 0.0000e+00 Epoch 13/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4996 - accuracy: 0.0000e+00 - val_loss: 0.3972 - val_accuracy: 0.0000e+00 Epoch 14/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4962 - accuracy: 0.0000e+00 - val_loss: 0.3940 - val_accuracy: 0.0000e+00 Epoch 15/400 3/3 [==============================] - 0s 17ms/step - loss: 0.4928 - accuracy: 0.0000e+00 - val_loss: 0.3907 - val_accuracy: 0.0000e+00 Epoch 16/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4893 - accuracy: 0.0000e+00 - val_loss: 0.3873 - val_accuracy: 0.0000e+00 Epoch 17/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4857 - accuracy: 0.0000e+00 - val_loss: 0.3839 - val_accuracy: 0.0000e+00 Epoch 18/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4820 - accuracy: 0.0000e+00 - val_loss: 0.3804 - val_accuracy: 0.0000e+00 Epoch 19/400 3/3 [==============================] - 0s 13ms/step - loss: 0.4783 - accuracy: 0.0000e+00 - val_loss: 0.3768 - val_accuracy: 0.0000e+00 Epoch 20/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4745 - accuracy: 0.0000e+00 - val_loss: 0.3731 - val_accuracy: 0.0000e+00 Epoch 21/400 3/3 [==============================] - 0s 16ms/step - loss: 0.4705 - accuracy: 0.0000e+00 - val_loss: 0.3693 - val_accuracy: 0.0000e+00 Epoch 22/400 3/3 [==============================] - 0s 18ms/step - loss: 0.4665 - accuracy: 0.0000e+00 - val_loss: 0.3654 - val_accuracy: 0.0000e+00 Epoch 23/400 3/3 [==============================] - 0s 22ms/step - loss: 0.4625 - accuracy: 0.0000e+00 - val_loss: 0.3615 - val_accuracy: 0.0000e+00 Epoch 24/400 3/3 [==============================] - 0s 25ms/step - loss: 0.4584 - accuracy: 0.0000e+00 - val_loss: 0.3575 - val_accuracy: 0.0000e+00 Epoch 25/400 3/3 [==============================] - 0s 22ms/step - loss: 0.4542 - accuracy: 0.0000e+00 - val_loss: 0.3534 - val_accuracy: 0.0000e+00 Epoch 26/400 3/3 [==============================] - 0s 29ms/step - loss: 0.4500 - accuracy: 0.0000e+00 - val_loss: 0.3492 - val_accuracy: 0.0000e+00 Epoch 27/400 3/3 [==============================] - 0s 30ms/step - loss: 0.4457 - accuracy: 0.0000e+00 - val_loss: 0.3450 - val_accuracy: 0.0000e+00 Epoch 28/400 3/3 [==============================] - 0s 29ms/step - loss: 0.4413 - accuracy: 0.0000e+00 - val_loss: 0.3408 - val_accuracy: 0.0000e+00 Epoch 29/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4369 - accuracy: 0.0000e+00 - val_loss: 0.3367 - val_accuracy: 0.0000e+00 Epoch 30/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4324 - accuracy: 0.0000e+00 - val_loss: 0.3325 - val_accuracy: 0.0000e+00 Epoch 31/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4278 - accuracy: 0.0000e+00 - val_loss: 0.3282 - val_accuracy: 0.0000e+00 Epoch 32/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4232 - accuracy: 0.0000e+00 - val_loss: 0.3238 - val_accuracy: 0.0000e+00 Epoch 33/400 3/3 [==============================] - 0s 14ms/step - loss: 0.4185 - accuracy: 0.0000e+00 - val_loss: 0.3194 - val_accuracy: 0.0000e+00 Epoch 34/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4138 - accuracy: 0.0000e+00 - val_loss: 0.3149 - val_accuracy: 0.0000e+00 Epoch 35/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4091 - accuracy: 0.0000e+00 - val_loss: 0.3104 - val_accuracy: 0.0000e+00 Epoch 36/400 3/3 [==============================] - 0s 15ms/step - loss: 0.4043 - accuracy: 0.0000e+00 - val_loss: 0.3057 - val_accuracy: 0.0000e+00 Epoch 37/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3994 - accuracy: 0.0000e+00 - val_loss: 0.3010 - val_accuracy: 0.0000e+00 Epoch 38/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3946 - accuracy: 0.0000e+00 - val_loss: 0.2962 - val_accuracy: 0.0000e+00 Epoch 39/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3897 - accuracy: 0.0000e+00 - val_loss: 0.2913 - val_accuracy: 0.0000e+00 Epoch 40/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3850 - accuracy: 0.0000e+00 - val_loss: 0.2864 - val_accuracy: 0.0000e+00 Epoch 41/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3802 - accuracy: 0.0000e+00 - val_loss: 0.2815 - val_accuracy: 0.0000e+00 Epoch 42/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3753 - accuracy: 0.0000e+00 - val_loss: 0.2765 - val_accuracy: 0.0000e+00 Epoch 43/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3705 - accuracy: 0.0000e+00 - val_loss: 0.2715 - val_accuracy: 0.0000e+00 Epoch 44/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3658 - accuracy: 0.0000e+00 - val_loss: 0.2664 - val_accuracy: 0.0000e+00 Epoch 45/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3612 - accuracy: 0.0000e+00 - val_loss: 0.2614 - val_accuracy: 0.0000e+00 Epoch 46/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3562 - accuracy: 0.0000e+00 - val_loss: 0.2568 - val_accuracy: 0.0000e+00 Epoch 47/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3515 - accuracy: 0.0000e+00 - val_loss: 0.2521 - val_accuracy: 0.0000e+00 Epoch 48/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3467 - accuracy: 0.0000e+00 - val_loss: 0.2474 - val_accuracy: 0.0000e+00 Epoch 49/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3420 - accuracy: 0.0000e+00 - val_loss: 0.2426 - val_accuracy: 0.0000e+00 Epoch 50/400 3/3 [==============================] - 0s 14ms/step - loss: 0.3373 - accuracy: 0.0000e+00 - val_loss: 0.2379 - val_accuracy: 0.0000e+00 Epoch 51/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3326 - accuracy: 0.0000e+00 - val_loss: 0.2335 - val_accuracy: 0.0000e+00 Epoch 52/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3279 - accuracy: 0.0000e+00 - val_loss: 0.2294 - val_accuracy: 0.0000e+00 Epoch 53/400 3/3 [==============================] - 0s 16ms/step - loss: 0.3231 - accuracy: 0.0000e+00 - val_loss: 0.2254 - val_accuracy: 0.0000e+00 Epoch 54/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3182 - accuracy: 0.0000e+00 - val_loss: 0.2214 - val_accuracy: 0.0000e+00 Epoch 55/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3136 - accuracy: 0.0000e+00 - val_loss: 0.2180 - val_accuracy: 0.0000e+00 Epoch 56/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3088 - accuracy: 0.0000e+00 - val_loss: 0.2145 - val_accuracy: 0.0000e+00 Epoch 57/400 3/3 [==============================] - 0s 15ms/step - loss: 0.3043 - accuracy: 0.0000e+00 - val_loss: 0.2111 - val_accuracy: 0.0000e+00 Epoch 58/400 3/3 [==============================] - 0s 15ms/step - loss: 0.2999 - accuracy: 0.0000e+00 - val_loss: 0.2077 - val_accuracy: 0.0000e+00 Epoch 59/400 3/3 [==============================] - 0s 14ms/step - loss: 0.2952 - accuracy: 0.0000e+00 - val_loss: 0.2043 - val_accuracy: 0.0000e+00 Epoch 60/400 3/3 [==============================] - 0s 16ms/step - loss: 0.2914 - accuracy: 0.0000e+00 - val_loss: 0.2013 - val_accuracy: 0.0000e+00 Epoch 61/400 3/3 [==============================] - 0s 24ms/step - loss: 0.2867 - accuracy: 0.0000e+00 - val_loss: 0.1985 - val_accuracy: 0.0000e+00 Epoch 62/400 3/3 [==============================] - 0s 40ms/step - loss: 0.2828 - accuracy: 0.0000e+00 - val_loss: 0.1957 - val_accuracy: 0.0000e+00 Epoch 63/400 3/3 [==============================] - 0s 35ms/step - loss: 0.2788 - accuracy: 0.0000e+00 - val_loss: 0.1929 - val_accuracy: 0.0000e+00 Epoch 64/400 3/3 [==============================] - 0s 26ms/step - loss: 0.2750 - accuracy: 0.0000e+00 - val_loss: 0.1901 - val_accuracy: 0.0000e+00 Epoch 65/400 3/3 [==============================] - 0s 27ms/step - loss: 0.2710 - accuracy: 0.0000e+00 - val_loss: 0.1877 - val_accuracy: 0.0000e+00 Epoch 66/400 3/3 [==============================] - 0s 33ms/step - loss: 0.2674 - accuracy: 0.0000e+00 - val_loss: 0.1855 - val_accuracy: 0.0000e+00 Epoch 67/400 3/3 [==============================] - 0s 45ms/step - loss: 0.2639 - accuracy: 0.0000e+00 - val_loss: 0.1833 - val_accuracy: 0.0000e+00 Epoch 68/400 3/3 [==============================] - 0s 42ms/step - loss: 0.2604 - accuracy: 0.0000e+00 - val_loss: 0.1816 - val_accuracy: 0.0000e+00 Epoch 69/400 3/3 [==============================] - 0s 36ms/step - loss: 0.2568 - accuracy: 0.0000e+00 - val_loss: 0.1800 - val_accuracy: 0.0000e+00 Epoch 70/400 3/3 [==============================] - 0s 37ms/step - loss: 0.2536 - accuracy: 0.0000e+00 - val_loss: 0.1785 - val_accuracy: 0.0000e+00 Epoch 71/400 3/3 [==============================] - 0s 48ms/step - loss: 0.2504 - accuracy: 0.0000e+00 - val_loss: 0.1769 - val_accuracy: 0.0000e+00 Epoch 72/400 3/3 [==============================] - 0s 51ms/step - loss: 0.2474 - accuracy: 0.0000e+00 - val_loss: 0.1757 - val_accuracy: 0.0000e+00 Epoch 73/400 3/3 [==============================] - 0s 50ms/step - loss: 0.2446 - accuracy: 0.0000e+00 - val_loss: 0.1747 - val_accuracy: 0.0000e+00 Epoch 74/400 3/3 [==============================] - 0s 48ms/step - loss: 0.2416 - accuracy: 0.0000e+00 - val_loss: 0.1737 - val_accuracy: 0.0000e+00 Epoch 75/400 3/3 [==============================] - 0s 55ms/step - loss: 0.2390 - accuracy: 0.0000e+00 - val_loss: 0.1727 - val_accuracy: 0.0000e+00 Epoch 76/400 3/3 [==============================] - 0s 51ms/step - loss: 0.2364 - accuracy: 0.0000e+00 - val_loss: 0.1718 - val_accuracy: 0.0000e+00 Epoch 77/400 3/3 [==============================] - 0s 59ms/step - loss: 0.2339 - accuracy: 0.0000e+00 - val_loss: 0.1708 - val_accuracy: 0.0000e+00 Epoch 78/400 3/3 [==============================] - 0s 54ms/step - loss: 0.2316 - accuracy: 0.0000e+00 - val_loss: 0.1698 - val_accuracy: 0.0000e+00 Epoch 79/400 3/3 [==============================] - 0s 28ms/step - loss: 0.2293 - accuracy: 0.0000e+00 - val_loss: 0.1688 - val_accuracy: 0.0000e+00 Epoch 80/400 3/3 [==============================] - 0s 21ms/step - loss: 0.2274 - accuracy: 0.0000e+00 - val_loss: 0.1678 - val_accuracy: 0.0000e+00 Epoch 81/400 3/3 [==============================] - 0s 23ms/step - loss: 0.2253 - accuracy: 0.0000e+00 - val_loss: 0.1668 - val_accuracy: 0.0000e+00 Epoch 82/400 3/3 [==============================] - 0s 25ms/step - loss: 0.2232 - accuracy: 0.0000e+00 - val_loss: 0.1661 - val_accuracy: 0.0000e+00 Epoch 83/400 3/3 [==============================] - 0s 23ms/step - loss: 0.2214 - accuracy: 0.0000e+00 - val_loss: 0.1654 - val_accuracy: 0.0000e+00 Epoch 84/400 3/3 [==============================] - 0s 46ms/step - loss: 0.2195 - accuracy: 0.0000e+00 - val_loss: 0.1647 - val_accuracy: 0.0500 Epoch 85/400 3/3 [==============================] - 0s 46ms/step - loss: 0.2176 - accuracy: 0.0000e+00 - val_loss: 0.1640 - val_accuracy: 0.0500 Epoch 86/400 3/3 [==============================] - 0s 43ms/step - loss: 0.2158 - accuracy: 0.0000e+00 - val_loss: 0.1633 - val_accuracy: 0.0500 Epoch 87/400 3/3 [==============================] - 0s 35ms/step - loss: 0.2140 - accuracy: 0.0000e+00 - val_loss: 0.1629 - val_accuracy: 0.0500 Epoch 88/400 3/3 [==============================] - 0s 25ms/step - loss: 0.2122 - accuracy: 0.0000e+00 - val_loss: 0.1625 - val_accuracy: 0.0500 Epoch 89/400 3/3 [==============================] - 0s 23ms/step - loss: 0.2104 - accuracy: 0.0000e+00 - val_loss: 0.1621 - val_accuracy: 0.0500 Epoch 90/400 3/3 [==============================] - 0s 24ms/step - loss: 0.2085 - accuracy: 0.0000e+00 - val_loss: 0.1617 - val_accuracy: 0.0500 Epoch 91/400 3/3 [==============================] - 0s 30ms/step - loss: 0.2070 - accuracy: 0.0000e+00 - val_loss: 0.1612 - val_accuracy: 0.0500 Epoch 92/400 3/3 [==============================] - 0s 25ms/step - loss: 0.2052 - accuracy: 0.0000e+00 - val_loss: 0.1607 - val_accuracy: 0.0500 Epoch 93/400 3/3 [==============================] - 0s 25ms/step - loss: 0.2036 - accuracy: 0.0000e+00 - val_loss: 0.1601 - val_accuracy: 0.0500 Epoch 94/400 3/3 [==============================] - 0s 30ms/step - loss: 0.2020 - accuracy: 0.0000e+00 - val_loss: 0.1595 - val_accuracy: 0.0500 Epoch 95/400 3/3 [==============================] - 0s 28ms/step - loss: 0.2005 - accuracy: 0.0000e+00 - val_loss: 0.1589 - val_accuracy: 0.0500 Epoch 96/400 3/3 [==============================] - 0s 29ms/step - loss: 0.1989 - accuracy: 0.0000e+00 - val_loss: 0.1583 - val_accuracy: 0.0500 Epoch 97/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1974 - accuracy: 0.0000e+00 - val_loss: 0.1577 - val_accuracy: 0.0500 Epoch 98/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1959 - accuracy: 0.0000e+00 - val_loss: 0.1571 - val_accuracy: 0.0500 Epoch 99/400 3/3 [==============================] - 0s 22ms/step - loss: 0.1943 - accuracy: 0.0000e+00 - val_loss: 0.1565 - val_accuracy: 0.0500 Epoch 100/400 3/3 [==============================] - 0s 24ms/step - loss: 0.1928 - accuracy: 0.0000e+00 - val_loss: 0.1558 - val_accuracy: 0.0500 Epoch 101/400 3/3 [==============================] - 0s 36ms/step - loss: 0.1913 - accuracy: 0.0000e+00 - val_loss: 0.1552 - val_accuracy: 0.0500 Epoch 102/400 3/3 [==============================] - 0s 49ms/step - loss: 0.1899 - accuracy: 0.0000e+00 - val_loss: 0.1544 - val_accuracy: 0.0500 Epoch 103/400 3/3 [==============================] - 0s 56ms/step - loss: 0.1883 - accuracy: 0.0000e+00 - val_loss: 0.1537 - val_accuracy: 0.0500 Epoch 104/400 3/3 [==============================] - 0s 52ms/step - loss: 0.1867 - accuracy: 0.0000e+00 - val_loss: 0.1529 - val_accuracy: 0.0500 Epoch 105/400 3/3 [==============================] - 0s 46ms/step - loss: 0.1852 - accuracy: 0.0000e+00 - val_loss: 0.1521 - val_accuracy: 0.0500 Epoch 106/400 3/3 [==============================] - 0s 37ms/step - loss: 0.1837 - accuracy: 0.0000e+00 - val_loss: 0.1512 - val_accuracy: 0.0500 Epoch 107/400 3/3 [==============================] - 0s 40ms/step - loss: 0.1822 - accuracy: 0.0000e+00 - val_loss: 0.1503 - val_accuracy: 0.0500 Epoch 108/400 3/3 [==============================] - 0s 47ms/step - loss: 0.1806 - accuracy: 0.0000e+00 - val_loss: 0.1494 - val_accuracy: 0.0500 Epoch 109/400 3/3 [==============================] - 0s 37ms/step - loss: 0.1790 - accuracy: 0.0000e+00 - val_loss: 0.1485 - val_accuracy: 0.0500 Epoch 110/400 3/3 [==============================] - 0s 29ms/step - loss: 0.1775 - accuracy: 0.0000e+00 - val_loss: 0.1476 - val_accuracy: 0.0500 Epoch 111/400 3/3 [==============================] - 0s 25ms/step - loss: 0.1759 - accuracy: 0.0000e+00 - val_loss: 0.1466 - val_accuracy: 0.0500 Epoch 112/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1744 - accuracy: 0.0000e+00 - val_loss: 0.1457 - val_accuracy: 0.0500 Epoch 113/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1727 - accuracy: 0.0000e+00 - val_loss: 0.1447 - val_accuracy: 0.0500 Epoch 114/400 3/3 [==============================] - 0s 28ms/step - loss: 0.1711 - accuracy: 0.0000e+00 - val_loss: 0.1434 - val_accuracy: 0.0500 Epoch 115/400 3/3 [==============================] - 0s 47ms/step - loss: 0.1695 - accuracy: 0.0000e+00 - val_loss: 0.1421 - val_accuracy: 0.0500 Epoch 116/400 3/3 [==============================] - 0s 44ms/step - loss: 0.1679 - accuracy: 0.0000e+00 - val_loss: 0.1406 - val_accuracy: 0.0500 Epoch 117/400 3/3 [==============================] - 0s 46ms/step - loss: 0.1662 - accuracy: 0.0000e+00 - val_loss: 0.1391 - val_accuracy: 0.0500 Epoch 118/400 3/3 [==============================] - 0s 45ms/step - loss: 0.1646 - accuracy: 0.0000e+00 - val_loss: 0.1373 - val_accuracy: 0.0500 Epoch 119/400 3/3 [==============================] - 0s 49ms/step - loss: 0.1628 - accuracy: 0.0000e+00 - val_loss: 0.1358 - val_accuracy: 0.0500 Epoch 120/400 3/3 [==============================] - 0s 51ms/step - loss: 0.1612 - accuracy: 0.0000e+00 - val_loss: 0.1339 - val_accuracy: 0.0500 Epoch 121/400 3/3 [==============================] - 0s 52ms/step - loss: 0.1595 - accuracy: 0.0000e+00 - val_loss: 0.1321 - val_accuracy: 0.0500 Epoch 122/400 3/3 [==============================] - 0s 52ms/step - loss: 0.1577 - accuracy: 0.0000e+00 - val_loss: 0.1304 - val_accuracy: 0.0500 Epoch 123/400 3/3 [==============================] - 0s 33ms/step - loss: 0.1560 - accuracy: 0.0000e+00 - val_loss: 0.1285 - val_accuracy: 0.0500 Epoch 124/400 3/3 [==============================] - 0s 22ms/step - loss: 0.1542 - accuracy: 0.0000e+00 - val_loss: 0.1266 - val_accuracy: 0.0500 Epoch 125/400 3/3 [==============================] - 0s 21ms/step - loss: 0.1525 - accuracy: 0.0000e+00 - val_loss: 0.1245 - val_accuracy: 0.0500 Epoch 126/400 3/3 [==============================] - 0s 45ms/step - loss: 0.1507 - accuracy: 0.0000e+00 - val_loss: 0.1227 - val_accuracy: 0.0500 Epoch 127/400 3/3 [==============================] - 0s 48ms/step - loss: 0.1489 - accuracy: 0.0000e+00 - val_loss: 0.1205 - val_accuracy: 0.0500 Epoch 128/400 3/3 [==============================] - 0s 42ms/step - loss: 0.1470 - accuracy: 0.0000e+00 - val_loss: 0.1186 - val_accuracy: 0.0500 Epoch 129/400 3/3 [==============================] - 0s 18ms/step - loss: 0.1452 - accuracy: 0.0000e+00 - val_loss: 0.1165 - val_accuracy: 0.0500 Epoch 130/400 3/3 [==============================] - 0s 14ms/step - loss: 0.1434 - accuracy: 0.0000e+00 - val_loss: 0.1144 - val_accuracy: 0.0500 Epoch 131/400 3/3 [==============================] - 0s 16ms/step - loss: 0.1416 - accuracy: 0.0000e+00 - val_loss: 0.1124 - val_accuracy: 0.0500 Epoch 132/400 3/3 [==============================] - 0s 14ms/step - loss: 0.1396 - accuracy: 0.0000e+00 - val_loss: 0.1106 - val_accuracy: 0.0500 Epoch 133/400 3/3 [==============================] - 0s 14ms/step - loss: 0.1378 - accuracy: 0.0000e+00 - val_loss: 0.1085 - val_accuracy: 0.0500 Epoch 134/400 3/3 [==============================] - 0s 15ms/step - loss: 0.1358 - accuracy: 0.0000e+00 - val_loss: 0.1066 - val_accuracy: 0.0500 Epoch 135/400 3/3 [==============================] - 0s 15ms/step - loss: 0.1339 - accuracy: 0.0000e+00 - val_loss: 0.1044 - val_accuracy: 0.0500 Epoch 136/400 3/3 [==============================] - 0s 15ms/step - loss: 0.1319 - accuracy: 0.0000e+00 - val_loss: 0.1023 - val_accuracy: 0.0500 Epoch 137/400 3/3 [==============================] - 0s 17ms/step - loss: 0.1300 - accuracy: 0.0000e+00 - val_loss: 0.1002 - val_accuracy: 0.0500 Epoch 138/400 3/3 [==============================] - 0s 35ms/step - loss: 0.1279 - accuracy: 0.0000e+00 - val_loss: 0.0982 - val_accuracy: 0.0500 Epoch 139/400 3/3 [==============================] - 0s 52ms/step - loss: 0.1259 - accuracy: 0.0000e+00 - val_loss: 0.0961 - val_accuracy: 0.0500 Epoch 140/400 3/3 [==============================] - 0s 46ms/step - loss: 0.1238 - accuracy: 0.0000e+00 - val_loss: 0.0941 - val_accuracy: 0.0500 Epoch 141/400 3/3 [==============================] - 0s 34ms/step - loss: 0.1217 - accuracy: 0.0000e+00 - val_loss: 0.0921 - val_accuracy: 0.0500 Epoch 142/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1196 - accuracy: 0.0000e+00 - val_loss: 0.0899 - val_accuracy: 0.0500 Epoch 143/400 3/3 [==============================] - 0s 25ms/step - loss: 0.1174 - accuracy: 0.0000e+00 - val_loss: 0.0875 - val_accuracy: 0.0500 Epoch 144/400 3/3 [==============================] - 0s 25ms/step - loss: 0.1153 - accuracy: 0.0000e+00 - val_loss: 0.0851 - val_accuracy: 0.0500 Epoch 145/400 3/3 [==============================] - 0s 31ms/step - loss: 0.1130 - accuracy: 0.0000e+00 - val_loss: 0.0829 - val_accuracy: 0.0500 Epoch 146/400 3/3 [==============================] - 0s 50ms/step - loss: 0.1108 - accuracy: 0.0000e+00 - val_loss: 0.0808 - val_accuracy: 0.0500 Epoch 147/400 3/3 [==============================] - 0s 51ms/step - loss: 0.1085 - accuracy: 0.0000e+00 - val_loss: 0.0786 - val_accuracy: 0.0500 Epoch 148/400 3/3 [==============================] - 0s 46ms/step - loss: 0.1062 - accuracy: 0.0000e+00 - val_loss: 0.0763 - val_accuracy: 0.0500 Epoch 149/400 3/3 [==============================] - 0s 28ms/step - loss: 0.1038 - accuracy: 0.0000e+00 - val_loss: 0.0739 - val_accuracy: 0.0500 Epoch 150/400 3/3 [==============================] - 0s 27ms/step - loss: 0.1014 - accuracy: 0.0000e+00 - val_loss: 0.0712 - val_accuracy: 0.0500 Epoch 151/400 3/3 [==============================] - 0s 51ms/step - loss: 0.0990 - accuracy: 0.0000e+00 - val_loss: 0.0686 - val_accuracy: 0.0500 Epoch 152/400 3/3 [==============================] - 0s 67ms/step - loss: 0.0966 - accuracy: 0.0000e+00 - val_loss: 0.0660 - val_accuracy: 0.0500 Epoch 153/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0941 - accuracy: 0.0000e+00 - val_loss: 0.0635 - val_accuracy: 0.0500 Epoch 154/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0916 - accuracy: 0.0000e+00 - val_loss: 0.0610 - val_accuracy: 0.0500 Epoch 155/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0892 - accuracy: 0.0000e+00 - val_loss: 0.0595 - val_accuracy: 0.0500 Epoch 156/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0870 - accuracy: 0.0000e+00 - val_loss: 0.0585 - val_accuracy: 0.0500 Epoch 157/400 3/3 [==============================] - 0s 51ms/step - loss: 0.0849 - accuracy: 0.0000e+00 - val_loss: 0.0578 - val_accuracy: 0.0500 Epoch 158/400 3/3 [==============================] - 0s 62ms/step - loss: 0.0829 - accuracy: 0.0000e+00 - val_loss: 0.0570 - val_accuracy: 0.0500 Epoch 159/400 3/3 [==============================] - 0s 56ms/step - loss: 0.0814 - accuracy: 0.0000e+00 - val_loss: 0.0563 - val_accuracy: 0.0500 Epoch 160/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0799 - accuracy: 0.0000e+00 - val_loss: 0.0556 - val_accuracy: 0.0500 Epoch 161/400 3/3 [==============================] - 0s 52ms/step - loss: 0.0787 - accuracy: 0.0000e+00 - val_loss: 0.0553 - val_accuracy: 0.0500 Epoch 162/400 3/3 [==============================] - 0s 65ms/step - loss: 0.0774 - accuracy: 0.0000e+00 - val_loss: 0.0551 - val_accuracy: 0.0500 Epoch 163/400 3/3 [==============================] - 0s 58ms/step - loss: 0.0762 - accuracy: 0.0000e+00 - val_loss: 0.0550 - val_accuracy: 0.0500 Epoch 164/400 3/3 [==============================] - 0s 60ms/step - loss: 0.0752 - accuracy: 0.0000e+00 - val_loss: 0.0550 - val_accuracy: 0.0500 Epoch 165/400 3/3 [==============================] - 0s 37ms/step - loss: 0.0742 - accuracy: 0.0000e+00 - val_loss: 0.0552 - val_accuracy: 0.0500 Epoch 166/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0733 - accuracy: 0.0000e+00 - val_loss: 0.0554 - val_accuracy: 0.0500 Epoch 167/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0723 - accuracy: 0.0000e+00 - val_loss: 0.0555 - val_accuracy: 0.0500 Epoch 168/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0715 - accuracy: 0.0000e+00 - val_loss: 0.0557 - val_accuracy: 0.0500 Epoch 169/400 3/3 [==============================] - 0s 34ms/step - loss: 0.0708 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 170/400 3/3 [==============================] - 0s 31ms/step - loss: 0.0701 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 171/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0694 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 172/400 3/3 [==============================] - 0s 39ms/step - loss: 0.0688 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 173/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0682 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 174/400 3/3 [==============================] - 0s 45ms/step - loss: 0.0676 - accuracy: 0.0000e+00 - val_loss: 0.0558 - val_accuracy: 0.0500 Epoch 175/400 3/3 [==============================] - 0s 39ms/step - loss: 0.0671 - accuracy: 0.0000e+00 - val_loss: 0.0557 - val_accuracy: 0.0500 Epoch 176/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0666 - accuracy: 0.0000e+00 - val_loss: 0.0557 - val_accuracy: 0.0500 Epoch 177/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0661 - accuracy: 0.0000e+00 - val_loss: 0.0556 - val_accuracy: 0.0500 Epoch 178/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0657 - accuracy: 0.0000e+00 - val_loss: 0.0556 - val_accuracy: 0.0500 Epoch 179/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0652 - accuracy: 0.0000e+00 - val_loss: 0.0555 - val_accuracy: 0.0500 Epoch 180/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0648 - accuracy: 0.0000e+00 - val_loss: 0.0554 - val_accuracy: 0.0500 Epoch 181/400 3/3 [==============================] - 0s 23ms/step - loss: 0.0644 - accuracy: 0.0000e+00 - val_loss: 0.0554 - val_accuracy: 0.0500 Epoch 182/400 3/3 [==============================] - 0s 21ms/step - loss: 0.0640 - accuracy: 0.0000e+00 - val_loss: 0.0553 - val_accuracy: 0.0500 Epoch 183/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0637 - accuracy: 0.0000e+00 - val_loss: 0.0551 - val_accuracy: 0.0500 Epoch 184/400 3/3 [==============================] - 0s 51ms/step - loss: 0.0633 - accuracy: 0.0000e+00 - val_loss: 0.0549 - val_accuracy: 0.0500 Epoch 185/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0629 - accuracy: 0.0000e+00 - val_loss: 0.0547 - val_accuracy: 0.0500 Epoch 186/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0625 - accuracy: 0.0000e+00 - val_loss: 0.0545 - val_accuracy: 0.0500 Epoch 187/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0622 - accuracy: 0.0000e+00 - val_loss: 0.0543 - val_accuracy: 0.0500 Epoch 188/400 3/3 [==============================] - 0s 41ms/step - loss: 0.0618 - accuracy: 0.0000e+00 - val_loss: 0.0543 - val_accuracy: 0.0500 Epoch 189/400 3/3 [==============================] - 0s 41ms/step - loss: 0.0615 - accuracy: 0.0000e+00 - val_loss: 0.0542 - val_accuracy: 0.0500 Epoch 190/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0611 - accuracy: 0.0000e+00 - val_loss: 0.0542 - val_accuracy: 0.0500 Epoch 191/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0609 - accuracy: 0.0000e+00 - val_loss: 0.0541 - val_accuracy: 0.0500 Epoch 192/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0605 - accuracy: 0.0000e+00 - val_loss: 0.0538 - val_accuracy: 0.0500 Epoch 193/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0601 - accuracy: 0.0000e+00 - val_loss: 0.0536 - val_accuracy: 0.0500 Epoch 194/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0598 - accuracy: 0.0000e+00 - val_loss: 0.0536 - val_accuracy: 0.0500 Epoch 195/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0595 - accuracy: 0.0000e+00 - val_loss: 0.0535 - val_accuracy: 0.0500 Epoch 196/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0592 - accuracy: 0.0000e+00 - val_loss: 0.0534 - val_accuracy: 0.0500 Epoch 197/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0589 - accuracy: 0.0000e+00 - val_loss: 0.0533 - val_accuracy: 0.0500 Epoch 198/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0586 - accuracy: 0.0000e+00 - val_loss: 0.0532 - val_accuracy: 0.0500 Epoch 199/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0583 - accuracy: 0.0000e+00 - val_loss: 0.0531 - val_accuracy: 0.0500 Epoch 200/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0581 - accuracy: 0.0000e+00 - val_loss: 0.0530 - val_accuracy: 0.0500 Epoch 201/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0579 - accuracy: 0.0000e+00 - val_loss: 0.0528 - val_accuracy: 0.0500 Epoch 202/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0575 - accuracy: 0.0000e+00 - val_loss: 0.0526 - val_accuracy: 0.0500 Epoch 203/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0572 - accuracy: 0.0000e+00 - val_loss: 0.0525 - val_accuracy: 0.0500 Epoch 204/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0570 - accuracy: 0.0000e+00 - val_loss: 0.0525 - val_accuracy: 0.0500 Epoch 205/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0567 - accuracy: 0.0000e+00 - val_loss: 0.0524 - val_accuracy: 0.0500 Epoch 206/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0565 - accuracy: 0.0000e+00 - val_loss: 0.0523 - val_accuracy: 0.0500 Epoch 207/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0562 - accuracy: 0.0000e+00 - val_loss: 0.0522 - val_accuracy: 0.0500 Epoch 208/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0561 - accuracy: 0.0000e+00 - val_loss: 0.0522 - val_accuracy: 0.0500 Epoch 209/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0558 - accuracy: 0.0000e+00 - val_loss: 0.0520 - val_accuracy: 0.0500 Epoch 210/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0555 - accuracy: 0.0000e+00 - val_loss: 0.0518 - val_accuracy: 0.0500 Epoch 211/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0553 - accuracy: 0.0000e+00 - val_loss: 0.0517 - val_accuracy: 0.0500 Epoch 212/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0551 - accuracy: 0.0000e+00 - val_loss: 0.0514 - val_accuracy: 0.0500 Epoch 213/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0549 - accuracy: 0.0000e+00 - val_loss: 0.0512 - val_accuracy: 0.0500 Epoch 214/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0547 - accuracy: 0.0000e+00 - val_loss: 0.0509 - val_accuracy: 0.0500 Epoch 215/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0545 - accuracy: 0.0000e+00 - val_loss: 0.0507 - val_accuracy: 0.0500 Epoch 216/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0541 - accuracy: 0.0000e+00 - val_loss: 0.0506 - val_accuracy: 0.0500 Epoch 217/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0540 - accuracy: 0.0000e+00 - val_loss: 0.0505 - val_accuracy: 0.0500 Epoch 218/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0538 - accuracy: 0.0000e+00 - val_loss: 0.0503 - val_accuracy: 0.0500 Epoch 219/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0536 - accuracy: 0.0000e+00 - val_loss: 0.0502 - val_accuracy: 0.0500 Epoch 220/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0533 - accuracy: 0.0000e+00 - val_loss: 0.0500 - val_accuracy: 0.0500 Epoch 221/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0531 - accuracy: 0.0000e+00 - val_loss: 0.0498 - val_accuracy: 0.0500 Epoch 222/400 3/3 [==============================] - 0s 22ms/step - loss: 0.0530 - accuracy: 0.0000e+00 - val_loss: 0.0496 - val_accuracy: 0.0500 Epoch 223/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0527 - accuracy: 0.0000e+00 - val_loss: 0.0495 - val_accuracy: 0.0500 Epoch 224/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0526 - accuracy: 0.0000e+00 - val_loss: 0.0494 - val_accuracy: 0.0500 Epoch 225/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0523 - accuracy: 0.0000e+00 - val_loss: 0.0494 - val_accuracy: 0.0500 Epoch 226/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0521 - accuracy: 0.0000e+00 - val_loss: 0.0494 - val_accuracy: 0.0500 Epoch 227/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0519 - accuracy: 0.0000e+00 - val_loss: 0.0493 - val_accuracy: 0.0500 Epoch 228/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0518 - accuracy: 0.0000e+00 - val_loss: 0.0492 - val_accuracy: 0.0500 Epoch 229/400 3/3 [==============================] - 0s 21ms/step - loss: 0.0516 - accuracy: 0.0000e+00 - val_loss: 0.0490 - val_accuracy: 0.0500 Epoch 230/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0514 - accuracy: 0.0000e+00 - val_loss: 0.0488 - val_accuracy: 0.0500 Epoch 231/400 3/3 [==============================] - 0s 37ms/step - loss: 0.0512 - accuracy: 0.0000e+00 - val_loss: 0.0486 - val_accuracy: 0.0500 Epoch 232/400 3/3 [==============================] - 0s 36ms/step - loss: 0.0510 - accuracy: 0.0000e+00 - val_loss: 0.0484 - val_accuracy: 0.0500 Epoch 233/400 3/3 [==============================] - 0s 19ms/step - loss: 0.0508 - accuracy: 0.0000e+00 - val_loss: 0.0483 - val_accuracy: 0.0500 Epoch 234/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0507 - accuracy: 0.0000e+00 - val_loss: 0.0481 - val_accuracy: 0.0500 Epoch 235/400 3/3 [==============================] - 0s 41ms/step - loss: 0.0505 - accuracy: 0.0000e+00 - val_loss: 0.0480 - val_accuracy: 0.0500 Epoch 236/400 3/3 [==============================] - 0s 43ms/step - loss: 0.0504 - accuracy: 0.0000e+00 - val_loss: 0.0480 - val_accuracy: 0.0500 Epoch 237/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0502 - accuracy: 0.0000e+00 - val_loss: 0.0478 - val_accuracy: 0.0500 Epoch 238/400 3/3 [==============================] - 0s 36ms/step - loss: 0.0499 - accuracy: 0.0000e+00 - val_loss: 0.0477 - val_accuracy: 0.0500 Epoch 239/400 3/3 [==============================] - 0s 36ms/step - loss: 0.0498 - accuracy: 0.0000e+00 - val_loss: 0.0477 - val_accuracy: 0.0500 Epoch 240/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0496 - accuracy: 0.0000e+00 - val_loss: 0.0475 - val_accuracy: 0.0500 Epoch 241/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0494 - accuracy: 0.0000e+00 - val_loss: 0.0474 - val_accuracy: 0.0500 Epoch 242/400 3/3 [==============================] - 0s 22ms/step - loss: 0.0493 - accuracy: 0.0000e+00 - val_loss: 0.0473 - val_accuracy: 0.0500 Epoch 243/400 3/3 [==============================] - 0s 21ms/step - loss: 0.0492 - accuracy: 0.0000e+00 - val_loss: 0.0472 - val_accuracy: 0.0500 Epoch 244/400 3/3 [==============================] - 0s 23ms/step - loss: 0.0489 - accuracy: 0.0000e+00 - val_loss: 0.0472 - val_accuracy: 0.0500 Epoch 245/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0488 - accuracy: 0.0000e+00 - val_loss: 0.0472 - val_accuracy: 0.0500 Epoch 246/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0487 - accuracy: 0.0000e+00 - val_loss: 0.0473 - val_accuracy: 0.0500 Epoch 247/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0485 - accuracy: 0.0000e+00 - val_loss: 0.0472 - val_accuracy: 0.0500 Epoch 248/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0483 - accuracy: 0.0000e+00 - val_loss: 0.0471 - val_accuracy: 0.0500 Epoch 249/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0482 - accuracy: 0.0000e+00 - val_loss: 0.0470 - val_accuracy: 0.0500 Epoch 250/400 3/3 [==============================] - 0s 13ms/step - loss: 0.0481 - accuracy: 0.0000e+00 - val_loss: 0.0470 - val_accuracy: 0.0500 Epoch 251/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0479 - accuracy: 0.0000e+00 - val_loss: 0.0469 - val_accuracy: 0.0500 Epoch 252/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0478 - accuracy: 0.0000e+00 - val_loss: 0.0468 - val_accuracy: 0.0500 Epoch 253/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0477 - accuracy: 0.0000e+00 - val_loss: 0.0467 - val_accuracy: 0.0500 Epoch 254/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0475 - accuracy: 0.0000e+00 - val_loss: 0.0465 - val_accuracy: 0.0500 Epoch 255/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0474 - accuracy: 0.0000e+00 - val_loss: 0.0463 - val_accuracy: 0.0500 Epoch 256/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0472 - accuracy: 0.0000e+00 - val_loss: 0.0462 - val_accuracy: 0.0500 Epoch 257/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0471 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 258/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0470 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 259/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0468 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 260/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0467 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 261/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0466 - accuracy: 0.0000e+00 - val_loss: 0.0460 - val_accuracy: 0.0500 Epoch 262/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0466 - accuracy: 0.0000e+00 - val_loss: 0.0459 - val_accuracy: 0.0500 Epoch 263/400 3/3 [==============================] - 0s 22ms/step - loss: 0.0465 - accuracy: 0.0000e+00 - val_loss: 0.0457 - val_accuracy: 0.0500 Epoch 264/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0463 - accuracy: 0.0000e+00 - val_loss: 0.0455 - val_accuracy: 0.0500 Epoch 265/400 3/3 [==============================] - 0s 39ms/step - loss: 0.0462 - accuracy: 0.0000e+00 - val_loss: 0.0453 - val_accuracy: 0.0500 Epoch 266/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0461 - accuracy: 0.0000e+00 - val_loss: 0.0451 - val_accuracy: 0.0500 Epoch 267/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0460 - accuracy: 0.0000e+00 - val_loss: 0.0449 - val_accuracy: 0.0500 Epoch 268/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0458 - accuracy: 0.0000e+00 - val_loss: 0.0449 - val_accuracy: 0.0500 Epoch 269/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0457 - accuracy: 0.0000e+00 - val_loss: 0.0449 - val_accuracy: 0.0500 Epoch 270/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0455 - accuracy: 0.0000e+00 - val_loss: 0.0447 - val_accuracy: 0.0500 Epoch 271/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0454 - accuracy: 0.0000e+00 - val_loss: 0.0445 - val_accuracy: 0.0500 Epoch 272/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0454 - accuracy: 0.0000e+00 - val_loss: 0.0444 - val_accuracy: 0.0500 Epoch 273/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0453 - accuracy: 0.0000e+00 - val_loss: 0.0443 - val_accuracy: 0.0500 Epoch 274/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0452 - accuracy: 0.0000e+00 - val_loss: 0.0442 - val_accuracy: 0.0500 Epoch 275/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0450 - accuracy: 0.0000e+00 - val_loss: 0.0441 - val_accuracy: 0.0500 Epoch 276/400 3/3 [==============================] - 0s 19ms/step - loss: 0.0449 - accuracy: 0.0000e+00 - val_loss: 0.0440 - val_accuracy: 0.0500 Epoch 277/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0449 - accuracy: 0.0000e+00 - val_loss: 0.0439 - val_accuracy: 0.0500 Epoch 278/400 3/3 [==============================] - 0s 21ms/step - loss: 0.0447 - accuracy: 0.0000e+00 - val_loss: 0.0438 - val_accuracy: 0.0500 Epoch 279/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0446 - accuracy: 0.0000e+00 - val_loss: 0.0437 - val_accuracy: 0.0500 Epoch 280/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0445 - accuracy: 0.0000e+00 - val_loss: 0.0436 - val_accuracy: 0.0500 Epoch 281/400 3/3 [==============================] - 0s 19ms/step - loss: 0.0444 - accuracy: 0.0000e+00 - val_loss: 0.0436 - val_accuracy: 0.0500 Epoch 282/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0443 - accuracy: 0.0000e+00 - val_loss: 0.0436 - val_accuracy: 0.0500 Epoch 283/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0442 - accuracy: 0.0000e+00 - val_loss: 0.0435 - val_accuracy: 0.0500 Epoch 284/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0441 - accuracy: 0.0000e+00 - val_loss: 0.0434 - val_accuracy: 0.0500 Epoch 285/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0440 - accuracy: 0.0000e+00 - val_loss: 0.0434 - val_accuracy: 0.0500 Epoch 286/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0440 - accuracy: 0.0000e+00 - val_loss: 0.0433 - val_accuracy: 0.0500 Epoch 287/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0438 - accuracy: 0.0000e+00 - val_loss: 0.0433 - val_accuracy: 0.0500 Epoch 288/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0438 - accuracy: 0.0000e+00 - val_loss: 0.0432 - val_accuracy: 0.0500 Epoch 289/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0436 - accuracy: 0.0000e+00 - val_loss: 0.0431 - val_accuracy: 0.0500 Epoch 290/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0435 - accuracy: 0.0000e+00 - val_loss: 0.0430 - val_accuracy: 0.0500 Epoch 291/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0434 - accuracy: 0.0000e+00 - val_loss: 0.0429 - val_accuracy: 0.0500 Epoch 292/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0433 - accuracy: 0.0000e+00 - val_loss: 0.0429 - val_accuracy: 0.0500 Epoch 293/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0433 - accuracy: 0.0000e+00 - val_loss: 0.0429 - val_accuracy: 0.0500 Epoch 294/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0432 - accuracy: 0.0000e+00 - val_loss: 0.0429 - val_accuracy: 0.0500 Epoch 295/400 3/3 [==============================] - 0s 19ms/step - loss: 0.0431 - accuracy: 0.0000e+00 - val_loss: 0.0426 - val_accuracy: 0.0500 Epoch 296/400 3/3 [==============================] - 0s 20ms/step - loss: 0.0430 - accuracy: 0.0000e+00 - val_loss: 0.0424 - val_accuracy: 0.0500 Epoch 297/400 3/3 [==============================] - 0s 23ms/step - loss: 0.0429 - accuracy: 0.0000e+00 - val_loss: 0.0424 - val_accuracy: 0.0500 Epoch 298/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0428 - accuracy: 0.0000e+00 - val_loss: 0.0423 - val_accuracy: 0.0500 Epoch 299/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0427 - accuracy: 0.0000e+00 - val_loss: 0.0423 - val_accuracy: 0.0500 Epoch 300/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0426 - accuracy: 0.0000e+00 - val_loss: 0.0422 - val_accuracy: 0.0500 Epoch 301/400 3/3 [==============================] - 0s 16ms/step - loss: 0.0425 - accuracy: 0.0000e+00 - val_loss: 0.0421 - val_accuracy: 0.0500 Epoch 302/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0424 - accuracy: 0.0000e+00 - val_loss: 0.0422 - val_accuracy: 0.0500 Epoch 303/400 3/3 [==============================] - 0s 14ms/step - loss: 0.0424 - accuracy: 0.0000e+00 - val_loss: 0.0423 - val_accuracy: 0.0500 Epoch 304/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0423 - accuracy: 0.0000e+00 - val_loss: 0.0422 - val_accuracy: 0.0500 Epoch 305/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0422 - accuracy: 0.0000e+00 - val_loss: 0.0421 - val_accuracy: 0.0500 Epoch 306/400 3/3 [==============================] - 0s 18ms/step - loss: 0.0422 - accuracy: 0.0000e+00 - val_loss: 0.0418 - val_accuracy: 0.0500 Epoch 307/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0421 - accuracy: 0.0000e+00 - val_loss: 0.0417 - val_accuracy: 0.0500 Epoch 308/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0419 - accuracy: 0.0000e+00 - val_loss: 0.0417 - val_accuracy: 0.0500 Epoch 309/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0419 - accuracy: 0.0000e+00 - val_loss: 0.0417 - val_accuracy: 0.0500 Epoch 310/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0418 - accuracy: 0.0000e+00 - val_loss: 0.0416 - val_accuracy: 0.0500 Epoch 311/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0417 - accuracy: 0.0000e+00 - val_loss: 0.0414 - val_accuracy: 0.0500 Epoch 312/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0417 - accuracy: 0.0000e+00 - val_loss: 0.0412 - val_accuracy: 0.0500 Epoch 313/400 3/3 [==============================] - 0s 15ms/step - loss: 0.0416 - accuracy: 0.0000e+00 - val_loss: 0.0412 - val_accuracy: 0.0500 Epoch 314/400 3/3 [==============================] - 0s 17ms/step - loss: 0.0416 - accuracy: 0.0000e+00 - val_loss: 0.0412 - val_accuracy: 0.0500 Epoch 315/400 3/3 [==============================] - 0s 22ms/step - loss: 0.0414 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 316/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0413 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 317/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0413 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 318/400 3/3 [==============================] - 0s 92ms/step - loss: 0.0411 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 319/400 3/3 [==============================] - 0s 31ms/step - loss: 0.0411 - accuracy: 0.0000e+00 - val_loss: 0.0410 - val_accuracy: 0.0500 Epoch 320/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0410 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 321/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0410 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 322/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0409 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 323/400 3/3 [==============================] - 0s 35ms/step - loss: 0.0408 - accuracy: 0.0000e+00 - val_loss: 0.0406 - val_accuracy: 0.0500 Epoch 324/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0407 - accuracy: 0.0000e+00 - val_loss: 0.0405 - val_accuracy: 0.0500 Epoch 325/400 3/3 [==============================] - 0s 23ms/step - loss: 0.0407 - accuracy: 0.0000e+00 - val_loss: 0.0404 - val_accuracy: 0.0500 Epoch 326/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0406 - accuracy: 0.0000e+00 - val_loss: 0.0404 - val_accuracy: 0.0500 Epoch 327/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0405 - accuracy: 0.0000e+00 - val_loss: 0.0404 - val_accuracy: 0.0500 Epoch 328/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0406 - accuracy: 0.0000e+00 - val_loss: 0.0404 - val_accuracy: 0.0500 Epoch 329/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0404 - accuracy: 0.0000e+00 - val_loss: 0.0402 - val_accuracy: 0.0500 Epoch 330/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0403 - accuracy: 0.0000e+00 - val_loss: 0.0401 - val_accuracy: 0.0500 Epoch 331/400 3/3 [==============================] - 0s 31ms/step - loss: 0.0402 - accuracy: 0.0000e+00 - val_loss: 0.0402 - val_accuracy: 0.0500 Epoch 332/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0402 - accuracy: 0.0000e+00 - val_loss: 0.0402 - val_accuracy: 0.0500 Epoch 333/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0401 - accuracy: 0.0000e+00 - val_loss: 0.0405 - val_accuracy: 0.0500 Epoch 334/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0401 - accuracy: 0.0000e+00 - val_loss: 0.0408 - val_accuracy: 0.0500 Epoch 335/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0401 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 336/400 3/3 [==============================] - 0s 37ms/step - loss: 0.0400 - accuracy: 0.0000e+00 - val_loss: 0.0407 - val_accuracy: 0.0500 Epoch 337/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0399 - accuracy: 0.0000e+00 - val_loss: 0.0405 - val_accuracy: 0.0500 Epoch 338/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0398 - accuracy: 0.0000e+00 - val_loss: 0.0401 - val_accuracy: 0.0500 Epoch 339/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0397 - accuracy: 0.0000e+00 - val_loss: 0.0400 - val_accuracy: 0.0500 Epoch 340/400 3/3 [==============================] - 0s 40ms/step - loss: 0.0396 - accuracy: 0.0000e+00 - val_loss: 0.0400 - val_accuracy: 0.0500 Epoch 341/400 3/3 [==============================] - 0s 53ms/step - loss: 0.0395 - accuracy: 0.0000e+00 - val_loss: 0.0399 - val_accuracy: 0.0500 Epoch 342/400 3/3 [==============================] - 0s 54ms/step - loss: 0.0395 - accuracy: 0.0000e+00 - val_loss: 0.0399 - val_accuracy: 0.0500 Epoch 343/400 3/3 [==============================] - 0s 45ms/step - loss: 0.0394 - accuracy: 0.0000e+00 - val_loss: 0.0398 - val_accuracy: 0.0500 Epoch 344/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0393 - accuracy: 0.0000e+00 - val_loss: 0.0398 - val_accuracy: 0.0500 Epoch 345/400 3/3 [==============================] - 0s 46ms/step - loss: 0.0393 - accuracy: 0.0000e+00 - val_loss: 0.0397 - val_accuracy: 0.0500 Epoch 346/400 3/3 [==============================] - 0s 53ms/step - loss: 0.0392 - accuracy: 0.0000e+00 - val_loss: 0.0396 - val_accuracy: 0.0500 Epoch 347/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0391 - accuracy: 0.0000e+00 - val_loss: 0.0395 - val_accuracy: 0.0500 Epoch 348/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0391 - accuracy: 0.0000e+00 - val_loss: 0.0394 - val_accuracy: 0.0500 Epoch 349/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0390 - accuracy: 0.0000e+00 - val_loss: 0.0394 - val_accuracy: 0.0500 Epoch 350/400 3/3 [==============================] - 0s 34ms/step - loss: 0.0390 - accuracy: 0.0000e+00 - val_loss: 0.0394 - val_accuracy: 0.0500 Epoch 351/400 3/3 [==============================] - 0s 90ms/step - loss: 0.0389 - accuracy: 0.0000e+00 - val_loss: 0.0393 - val_accuracy: 0.0500 Epoch 352/400 3/3 [==============================] - 0s 41ms/step - loss: 0.0388 - accuracy: 0.0000e+00 - val_loss: 0.0392 - val_accuracy: 0.0500 Epoch 353/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0389 - accuracy: 0.0000e+00 - val_loss: 0.0392 - val_accuracy: 0.0500 Epoch 354/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0387 - accuracy: 0.0000e+00 - val_loss: 0.0391 - val_accuracy: 0.0500 Epoch 355/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0386 - accuracy: 0.0000e+00 - val_loss: 0.0390 - val_accuracy: 0.0500 Epoch 356/400 3/3 [==============================] - 0s 61ms/step - loss: 0.0386 - accuracy: 0.0000e+00 - val_loss: 0.0389 - val_accuracy: 0.0500 Epoch 357/400 3/3 [==============================] - 0s 58ms/step - loss: 0.0386 - accuracy: 0.0000e+00 - val_loss: 0.0390 - val_accuracy: 0.0500 Epoch 358/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0386 - accuracy: 0.0000e+00 - val_loss: 0.0389 - val_accuracy: 0.0500 Epoch 359/400 3/3 [==============================] - 0s 32ms/step - loss: 0.0385 - accuracy: 0.0000e+00 - val_loss: 0.0388 - val_accuracy: 0.0500 Epoch 360/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0384 - accuracy: 0.0000e+00 - val_loss: 0.0387 - val_accuracy: 0.0500 Epoch 361/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0384 - accuracy: 0.0000e+00 - val_loss: 0.0388 - val_accuracy: 0.0500 Epoch 362/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0382 - accuracy: 0.0000e+00 - val_loss: 0.0389 - val_accuracy: 0.0500 Epoch 363/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0382 - accuracy: 0.0000e+00 - val_loss: 0.0391 - val_accuracy: 0.0500 Epoch 364/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0381 - accuracy: 0.0000e+00 - val_loss: 0.0391 - val_accuracy: 0.0500 Epoch 365/400 3/3 [==============================] - 0s 29ms/step - loss: 0.0381 - accuracy: 0.0000e+00 - val_loss: 0.0390 - val_accuracy: 0.0500 Epoch 366/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0380 - accuracy: 0.0000e+00 - val_loss: 0.0390 - val_accuracy: 0.0500 Epoch 367/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0380 - accuracy: 0.0000e+00 - val_loss: 0.0388 - val_accuracy: 0.0500 Epoch 368/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0379 - accuracy: 0.0000e+00 - val_loss: 0.0386 - val_accuracy: 0.0500 Epoch 369/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0379 - accuracy: 0.0000e+00 - val_loss: 0.0386 - val_accuracy: 0.0500 Epoch 370/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0379 - accuracy: 0.0000e+00 - val_loss: 0.0384 - val_accuracy: 0.0500 Epoch 371/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0377 - accuracy: 0.0000e+00 - val_loss: 0.0384 - val_accuracy: 0.0500 Epoch 372/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0377 - accuracy: 0.0000e+00 - val_loss: 0.0383 - val_accuracy: 0.0500 Epoch 373/400 3/3 [==============================] - 0s 26ms/step - loss: 0.0376 - accuracy: 0.0000e+00 - val_loss: 0.0382 - val_accuracy: 0.0500 Epoch 374/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0376 - accuracy: 0.0000e+00 - val_loss: 0.0381 - val_accuracy: 0.0500 Epoch 375/400 3/3 [==============================] - 0s 33ms/step - loss: 0.0375 - accuracy: 0.0000e+00 - val_loss: 0.0380 - val_accuracy: 0.0500 Epoch 376/400 3/3 [==============================] - 0s 42ms/step - loss: 0.0375 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 377/400 3/3 [==============================] - 0s 34ms/step - loss: 0.0375 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 378/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0374 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 379/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0373 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 380/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0372 - accuracy: 0.0000e+00 - val_loss: 0.0381 - val_accuracy: 0.0500 Epoch 381/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0372 - accuracy: 0.0000e+00 - val_loss: 0.0383 - val_accuracy: 0.0500 Epoch 382/400 3/3 [==============================] - 0s 27ms/step - loss: 0.0372 - accuracy: 0.0000e+00 - val_loss: 0.0382 - val_accuracy: 0.0500 Epoch 383/400 3/3 [==============================] - 0s 28ms/step - loss: 0.0371 - accuracy: 0.0000e+00 - val_loss: 0.0381 - val_accuracy: 0.0500 Epoch 384/400 3/3 [==============================] - 0s 30ms/step - loss: 0.0371 - accuracy: 0.0000e+00 - val_loss: 0.0379 - val_accuracy: 0.0500 Epoch 385/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0370 - accuracy: 0.0000e+00 - val_loss: 0.0377 - val_accuracy: 0.0500 Epoch 386/400 3/3 [==============================] - 0s 24ms/step - loss: 0.0371 - accuracy: 0.0000e+00 - val_loss: 0.0376 - val_accuracy: 0.0500 Epoch 387/400 3/3 [==============================] - 0s 38ms/step - loss: 0.0369 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 388/400 3/3 [==============================] - 0s 25ms/step - loss: 0.0369 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 389/400 3/3 [==============================] - 0s 31ms/step - loss: 0.0368 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 390/400 3/3 [==============================] - 0s 47ms/step - loss: 0.0368 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 391/400 3/3 [==============================] - 0s 63ms/step - loss: 0.0367 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 392/400 3/3 [==============================] - 0s 57ms/step - loss: 0.0367 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 393/400 3/3 [==============================] - 0s 54ms/step - loss: 0.0366 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 394/400 3/3 [==============================] - 0s 54ms/step - loss: 0.0366 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 395/400 3/3 [==============================] - 0s 54ms/step - loss: 0.0365 - accuracy: 0.0000e+00 - val_loss: 0.0375 - val_accuracy: 0.0500 Epoch 396/400 3/3 [==============================] - 0s 49ms/step - loss: 0.0365 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 397/400 3/3 [==============================] - 0s 51ms/step - loss: 0.0364 - accuracy: 0.0000e+00 - val_loss: 0.0373 - val_accuracy: 0.0500 Epoch 398/400 3/3 [==============================] - 0s 52ms/step - loss: 0.0364 - accuracy: 0.0000e+00 - val_loss: 0.0374 - val_accuracy: 0.0500 Epoch 399/400 3/3 [==============================] - 0s 48ms/step - loss: 0.0364 - accuracy: 0.0000e+00 - val_loss: 0.0373 - val_accuracy: 0.0500 Epoch 400/400 3/3 [==============================] - 0s 56ms/step - loss: 0.0363 - accuracy: 0.0000e+00 - val_loss: 0.0371 - val_accuracy: 0.0500 . . Results and Visualization . results = model.predict(x_test) . plt.scatter(range(20), results, c=&#39;r&#39;) plt.scatter(range(20), y_test,c=&#39;g&#39;) plt.show() . plt.plot(history.history[&#39;loss&#39;]) plt.show() #### The loss nearly stagnates during the later stages - therefore the model would still produce a satisfactory output at epochs~300 .",
            "url": "https://kunal-bhar.github.io/blog/python/math/lstm/visualizations/2022/03/11/concurrent-num-prognosis.html",
            "relUrl": "/python/math/lstm/visualizations/2022/03/11/concurrent-num-prognosis.html",
            "date": " • Mar 11, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Shakespearean English Semantics",
            "content": "Project At A Glance . Objective: Evaluate and visualize relationships between annotations of the era&#39;s literature. . Data: Compiled Shakespeare Dataset [Download] . Implementation: Word2Vec, Word Embeddings, Principal Component Analysis . Results: . 100-parameter vectorized representation of every word in the corpus. | Computed similarility scores for distinct words of the time period relative to today&#39;s English language. | Scatter plots to visualize Word-Embeddings and their Principal Components. | . Deployment: View this project on GitHub. . Dependencies . import pandas as pd import nltk import string import matplotlib.pyplot as plt from nltk.corpus import stopwords from nltk import word_tokenize from gensim.models import Word2Vec as w2v from sklearn.decomposition import PCA . Initialization . PATH = &#39;ShakespeareDataset.txt&#39; sw = stopwords.words(&#39;english&#39;) plt.style.use(&#39;ggplot&#39;) . %%time lines = [] with open(PATH, &#39;r&#39;) as f: for l in f: lines.append(l) . Wall time: 70.5 ms . Text Pre-Processing . lines = [line.rstrip(&#39; n&#39;) for line in lines] lines = [line.lower() for line in lines] lines = [line.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation)) for line in lines] . %time lines = [word_tokenize(line) for line in lines] . Wall time: 15.3 s . def remove_stopwords(lines, sw = sw): res = [] for line in lines: original = line line = [w for w in line if w not in sw] if len(line) &lt; 1: line = original res.append(line) return res . %time filtered_lines = remove_stopwords(lines = lines, sw = sw) . Wall time: 2.03 s . Custom Word2Vec and Word-Similarity . %%time w = w2v( filtered_lines, min_count=3, sg = 1, window=7 ) . Wall time: 3.22 s . w.wv.most_similar(&#39;thou&#39;) . [(&#39;art&#39;, 0.8374333381652832), (&#39;thyself&#39;, 0.8162680864334106), (&#39;dost&#39;, 0.7874499559402466), (&#39;villain&#39;, 0.7856082320213318), (&#39;kill&#39;, 0.733100950717926), (&#39;hast&#39;, 0.7226855158805847), (&#39;wilt&#39;, 0.7046181559562683), (&#39;didst&#39;, 0.6970406770706177), (&#39;fellow&#39;, 0.696016788482666), (&#39;traitor&#39;, 0.6928953528404236)] . w.wv.most_similar(&#39;shall&#39;) . [(&#39;may&#39;, 0.8649993538856506), (&#39;could&#39;, 0.8336020708084106), (&#39;youll&#39;, 0.8054620623588562), (&#39;doth&#39;, 0.8045750856399536), (&#39;till&#39;, 0.7994358539581299), (&#39;business&#39;, 0.7948688864707947), (&#39;ill&#39;, 0.7912845611572266), (&#39;dare&#39;, 0.7817249298095703), (&#39;let&#39;, 0.7762875556945801), (&#39;might&#39;, 0.776180624961853)] . w.wv.most_similar(&#39;abhor&#39;) . [(&#39;revenue&#39;, 0.9962968826293945), (&#39;exercise&#39;, 0.9959706664085388), (&#39;wedlock&#39;, 0.9957810640335083), (&#39;fever&#39;, 0.995772659778595), (&#39;painting&#39;, 0.9955847263336182), (&#39;arthurs&#39;, 0.9955565929412842), (&#39;touched&#39;, 0.9955466985702515), (&#39;purgation&#39;, 0.9953007698059082), (&#39;devotion&#39;, 0.9951791763305664), (&#39;havior&#39;, 0.9951609373092651)] . w.wv.most_similar(&#39;vile&#39;) . [(&#39;form&#39;, 0.9728872776031494), (&#39;monstrous&#39;, 0.9698807597160339), (&#39;smiling&#39;, 0.9687707424163818), (&#39;merit&#39;, 0.9668200016021729), (&#39;savage&#39;, 0.9668026566505432), (&#39;blown&#39;, 0.9667961597442627), (&#39;tremble&#39;, 0.9661442041397095), (&#39;worm&#39;, 0.9657843112945557), (&#39;quite&#39;, 0.9650156497955322), (&#39;giddy&#39;, 0.9649045467376709)] . Generate Embedding DataFrame . %%time emb_df = ( pd.DataFrame( [w.wv.get_vector(str(n)) for n in w.wv.key_to_index], index = w.wv.key_to_index ) ) . Wall time: 797 ms . emb_df.shape . (11628, 100) . emb_df.head() . 0 1 2 3 4 5 6 7 8 9 ... 90 91 92 93 94 95 96 97 98 99 . thou 0.048444 | -0.179371 | 0.656354 | 0.553357 | -0.390886 | -0.399313 | 0.408025 | 0.459145 | -0.405728 | -0.201803 | ... | 0.168611 | 0.248811 | 0.342443 | -0.067844 | 1.087360 | 0.751683 | -0.544332 | -0.557544 | 0.064042 | 0.441012 | . thy 0.025636 | 0.257252 | 0.512183 | 0.255503 | 0.298671 | -0.194974 | 0.258179 | 0.727472 | -0.141695 | -0.345477 | ... | 0.183475 | -0.138357 | 0.696960 | -0.215607 | 0.402519 | 0.359478 | 0.072248 | -0.589917 | 0.021417 | 0.011141 | . shall -0.059704 | 0.162047 | -0.045949 | -0.057693 | 0.579027 | -0.023031 | 0.043291 | 0.445429 | -0.326463 | 0.111084 | ... | 0.486898 | 0.051414 | 0.056035 | -0.147839 | 0.489713 | 0.178388 | 0.121662 | -0.007791 | 0.305106 | 0.014491 | . thee -0.437681 | 0.170327 | 0.516488 | 0.250964 | 0.051163 | -0.085569 | 0.174727 | 0.692341 | -0.266182 | -0.164628 | ... | 0.455216 | 0.022709 | 0.256135 | -0.054368 | 0.635989 | 0.482457 | -0.008304 | -0.079286 | 0.276732 | 0.008572 | . good 0.068176 | 0.297002 | 0.267275 | -0.144337 | 0.127091 | 0.131793 | 0.484557 | 0.709339 | -0.335214 | 0.117829 | ... | 0.368580 | -0.192079 | 0.248281 | -0.377771 | 0.294786 | -0.042967 | 0.284816 | -0.195110 | 0.345721 | -0.013074 | . 5 rows × 100 columns . Word-Embedding Visualization . plt.clf() fig=plt.figure(figsize=(6,4)) plt.scatter( x = emb_df.iloc[:,0], y = emb_df.iloc[:,1], s = 0.2, color = &#39;maroon&#39;, alpha = 0.5 ) plt.title(&#39;Embedding Visualizations&#39;) plt.show() . &lt;Figure size 432x288 with 0 Axes&gt; . Principal Component Analysis . Processing . pca = PCA(n_components=2, random_state=7) pca_model = pca.fit_transform(emb_df) . emb_df_PCA = ( pd.DataFrame( pca_model, columns=[&#39;x&#39;,&#39;y&#39;], index = emb_df.index ) ) . Visualization . plt.clf() fig = plt.figure(figsize=(6,4)) plt.scatter( x = emb_df_PCA[&#39;x&#39;], y = emb_df_PCA[&#39;y&#39;], s = 0.4, color = &#39;maroon&#39;, alpha = 0.5 ) plt.xlabel(&#39;PCA-1&#39;) plt.ylabel(&#39;PCA-2&#39;) plt.title(&#39;PCA Visualization&#39;) plt.plot() plt.show() . &lt;Figure size 432x288 with 0 Axes&gt; .",
            "url": "https://kunal-bhar.github.io/blog/nlp/python/english/visualizations/2022/03/10/shakespeare-semantics.html",
            "relUrl": "/nlp/python/english/visualizations/2022/03/10/shakespeare-semantics.html",
            "date": " • Mar 10, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "probplotlib- Open Source Python Library",
            "content": "Project At A Glance . Objective: Higher-order mathematical operations using Python3. . Implementation: A lightwight library capable of parsing data directly as Probability Distributions - available as a Python installation through pip. . Results: . Concise calculations that eliminate double-precision. | Compatibility with external datasets (.txt). | Faster than NumPy operatives by ~13%. | . Deployment: View this project on the Python Package Index or GitHub. . Probability Distributions for Python . . The Statistical Void . Stats can get tricky in the transition from plotting fun graphs to advanced algebraic equations. A classic example is the given sum: . 1.0e14 + 1.0 - 1.0e14 . The actual result is 1.0 but in double precision, this will result in 0.0. While in this example the failure is quite obvious, it can get a lot trickier than that. Instances like these hinder the community from exploring the inferential potential of complex entities. . p=Gaussian(a,b) q=Gaussian(x,y) p+q . This snippet would be close to useless as python addition doesn’t isn’t attributed for higher-level declarables such as Gaussian variables. probplotlib provides simple solutions for probability distributions; posing a highly-optimized alternative to numpy and math, in a niche that is scarce in options. . Usage . probplotlib has the following operative methods: . + : uses Dunder Methods for facilitating dist-additions. . | calculate_mean(): returns the mean of a distribution. . | . gaussianex = Gaussian() calculate_mean(gaussianx) . calculate_stdev(): returns the standard deviation of a distribution. | . binomialex = Binomial() calculate_stdev(binomialex) . read_dataset(): reads an external .txt dataset directly as a distribution. | . gaussianex.read_dataset(&#39;values.txt&#39;) binomialex.read_dataset(&#39;values.txt&#39;) . params(): retrieves the identity parameters of an imported dataset. | . gaussianex.params() binomialex.params() . pdf(): returns the probability density function at a given point. | . pdf(gaussianex, 2) . functions unique to Gaussian Distributions: . plot_histogram(): uses matplotlib to display a histogram of the Gaussian Distribution. | . gaussianex.plot_histogram() . plot_histogram_pdf(): uses matplotlib to display a co-relative plot along with the Gaussian probability density function. | . gaussianex.plot_histogram_pdf() . functions unique to Binomial Distributions: . plot_bar(): uses matplotlib to display a bar graph of the Binomial Distribution. | . binomialex.plot_bar() . plot_bar_pdf(): uses matplotlib to display a co-relative plot along with the Binomial probability density function. | . binomialex.plot_bar_pdf() . Data Visualization . probplotlib therefore allows you to analyze raw numerical data graphically in minimial lines of code. The example below makes for better understanding. . . a bag of numbers in a .txt file corresponds to the following plots: . histogram plot: . . bar plot: . . histogram plot with pdf: . . References . Stanford Archives: CS109- The Normal(Gaussian) Distribution . A Practical Overview on Probability Distributions: Andrea Viti, Alberto Terzi, Luca Bertolaccini . Awesome Scientific Computing: Nico Schlömer, GitHub Repository . math.statistics: Python 3.10 Source Code . Stack Overflow . Dependencies . probplotlib depends on the matplotlib library on top of your regular python installation. . pip install matplotlib . or . conda install matplotlib . Installation . probplotlib is available on the Python Package Index. You can install it directly using pip. . pip install probplotlib . Testing . To run the tests, simply check to this directory and run the code below. . python -m unittest test_probplotlib .",
            "url": "https://kunal-bhar.github.io/blog/python/math/probability/visualizations/2021/10/28/probplotlib.html",
            "relUrl": "/python/math/probability/visualizations/2021/10/28/probplotlib.html",
            "date": " • Oct 28, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://kunal-bhar.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://kunal-bhar.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://kunal-bhar.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kunal-bhar.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}